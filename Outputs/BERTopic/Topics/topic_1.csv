id,number,title,body,agent,user_id,user,state,created_at,closed_at,merged_at,repo_id,repo_url,html_url,llm_output,Topic,Probability,Representative_document
3256172444,695,feat: Support llm-based message summarization by introducing Transformer mechanism,"Design doc: https://www.notion.so/Fast-Model-Summarization-for-Large-Tool-Output-21f18f5dfd9b8045b7faf3368b6bf6ac

  Summary

  - Adds transformer architecture for processing tool outputs before LLM consumption
  - Implements llm_summarize transformer using fast secondary models to summarize lengthy outputs
  - Provides global configuration via --fast-model and --summarize-threshold flags
  - Enables tool-level configuration in both YAML and Python toolsets
  - Maintains backward compatibility - existing tools work unchanged

  Key Features

  Global Configuration:
  - --fast-model: Optional fast model for summarization (e.g., gpt-4o-mini)
  - --summarize-threshold: Minimum character count to trigger summarization (default: 1000)

  Tool Integration:
  - YAML tools support transformer_configs with customizable prompts and thresholds
  - Python toolsets can configure transformers during tool initialization
  - Kubernetes toolsets updated with optimized summarization for kubectl commands

  Smart Behavior:
  - Only processes outputs exceeding the threshold
  - Falls back gracefully when fast model unavailable
  - Preserves searchable keywords and error details in summaries

  Files Changed

  - Core Infrastructure: holmes/core/transformers/ - Complete transformer system
  - Configuration: Enhanced holmes/config.py with new global options
  - Tool Integration: Updated toolset manager and execution pipeline
  - Documentation: New docs/transformers.md with comprehensive usage guide
  - Examples: Updated Kubernetes and AKS toolsets with transformer configs
  - Testing: Extensive test coverage (2,000+ new test lines)

  Benefits

  - Reduced context usage for large outputs (kubectl, logs, metrics)
  - Improved performance by summarizing before primary LLM processing
  - Cost optimization using fast models for preprocessing
  - Better accuracy by avoiding truncation of important information

  ğŸ¤– Generated with https://claude.ai/code

  Co-Authored-By: Claude noreply@anthropic.com",Claude_Code,36728755,nilo19,open,2025-07-23T12:23:37Z,,,808146034,https://api.github.com/repos/robusta-dev/holmesgpt,https://github.com/robusta-dev/holmesgpt/pull/695,"analysisWe need to classify as performance or non-performance. The PR introduces transformer mechanism for summarization to reduce context usage, improve performance by summarizing before LLM processing, cost optimization. This is performance improvement. So label 'performance'.assistantfinalperformance",1,1.0,False
3119417980,44,feat: Add includeStackTrace option to reduce LLM token usage by 80-90%,"## ğŸš¨ Problem

After implementing pagination (#42), we discovered another critical issue with LLM token consumption when retrieving Unity console logs. **Stack traces alone consume 80-90% of the total tokens**, making it difficult to retrieve and analyze logs efficiently within LLM context windows.

### Real-world Impact
- A single error log with stack trace: ~500-1000 tokens
- The same log without stack trace: ~50-100 tokens  
- **Result**: 10x reduction in token usage

This becomes especially problematic when:
- Debugging across multiple log entries
- Working with limited context windows
- Analyzing patterns across many logs
- Quick log overview is needed before deep debugging

## âš¡ Solution

### New `includeStackTrace` Parameter

Added an optional boolean parameter to control stack trace inclusion:

```typescript
// Quick overview - saves 80-90% tokens
get_console_logs({ 
  includeStackTrace: false,
  limit: 50 
})

// Detailed debugging - includes stack traces
get_console_logs({ 
  logType: ""error"",
  includeStackTrace: true,
  limit: 10
})
```

### Smart Defaults
- **Default**: `true` for backward compatibility
- **Exception**: Info logs via resource default to `false` (stack traces rarely needed)

### LLM-Friendly Documentation

Added clear hints with âš ï¸ emoji to guide LLMs:
```
""Whether to include stack trace in logs. âš ï¸ ALWAYS SET TO FALSE to save 80-90% tokens, unless you specifically need stack traces for debugging.""
```

## ğŸ“Š Results

### Token Usage Comparison

| Log Type | With Stack Trace | Without Stack Trace | Reduction |
|----------|------------------|---------------------|-----------|
| Error    | ~800 tokens      | ~80 tokens          | 90%       |
| Warning  | ~600 tokens      | ~60 tokens          | 90%       |
| Info     | ~500 tokens      | ~50 tokens          | 90%       |

### Recommended Workflow
1. **Initial Investigation**: Use `includeStackTrace: false` for quick overview
2. **Identify Issues**: Find problematic logs with minimal token usage
3. **Deep Dive**: Re-query specific errors with `includeStackTrace: true` only when needed

## ğŸ§ª Testing with Claude Code

**This feature was extensively tested with Claude Code (claude.ai/code)**, which is how we discovered the token consumption issue and validated the solution.

### Test Environment
- **LLM**: Claude Code with Anthropic's official CLI
- **Unity Version**: Unity 2022.3 and Unity 6
- **Test Project**: Active Unity game development project

### Claude Code Test Results
```typescript
// Test 1: Before implementation - Token limit exceeded
// Claude Code context window quickly filled with stack traces

// Test 2: After implementation - Successful analysis
// Claude Code could analyze 100+ logs without hitting token limits

// Real conversation with Claude Code:
User: ""get shader error by using tool""
Claude: *uses get_console_logs with includeStackTrace: false*
// Successfully retrieved and analyzed errors within token limits
```

### Why Claude Code Testing Matters
- **Real-world LLM constraints**: Tested against actual token limits
- **Practical workflows**: Validated the natural debugging flow
- **Immediate feedback**: Claude Code's responses confirmed token savings
- **User experience**: Smooth interaction without ""token exceeded"" errors

## ğŸ“‹ Technical Details

### Unity Side Changes
- `ConsoleLogsService.cs`: Added conditional stack trace inclusion
- `IConsoleLogsService.cs`: Updated interface signature
- `GetConsoleLogsResource.cs`: Added `includeStackTrace` parameter handling

### Node.js Side Changes  
- `getConsoleLogsTool.ts`: Added parameter to Zod schema with detailed description
- `getConsoleLogsResource.ts`: Extended URL template and parameter extraction

### Key Implementation Details
- **Backward Compatible**: Defaults to `true` to maintain existing behavior
- **Flexible Control**: Can be set per request based on debugging needs
- **Memory Efficient**: No additional memory overhead (filtering only)
- **Clear Documentation**: LLM-optimized descriptions guide proper usage

## ğŸ” Why This Matters

### For LLM-based Development Tools (like Claude Code)
- **More Context**: Can analyze 10x more logs within token limits
- **Faster Iteration**: Quick overview before detailed investigation
- **Better UX**: Reduced ""token limit exceeded"" errors
- **Natural Workflow**: Matches how developers actually debug

### For Developers Using MCP Unity
- **Efficient Debugging**: Start broad, then narrow down
- **Cost Savings**: Reduced API token consumption
- **Improved Workflow**: Natural progression from overview to details

### Use Case Examples (from Claude Code testing)

1. **Quick Health Check**
   ```typescript
   // See last 100 logs without overwhelming context
   get_console_logs({ includeStackTrace: false, limit: 100 })
   ```

2. **Shader Error Investigation** (actual test case)
   ```typescript
   // First: Find shader compilation errors
   get_console_logs({ logType: ""error"", includeStackTrace: false, limit: 20 })
   // Found: ""Shader error in 'Custom/MaskedTransparency'""
   
   // Then: Get details if needed
   get_console_logs({ logType: ""error"", includeStackTrace: true, limit: 5 })
   ```

3. **Pattern Analysis**
   ```typescript
   // Analyze warning patterns across many entries
   get_console_logs({ logType: ""warning"", includeStackTrace: false, limit: 50 })
   ```

## Breaking Changes

**None** - Fully backward compatible. Existing code continues to work unchanged.

## Future Considerations

This implementation opens possibilities for:
- Selective stack trace inclusion (e.g., first N lines only)  
- Compressed stack trace formats
- Smart stack trace summarization

However, the current boolean approach provides immediate value with minimal complexity.

## Summary

This PR addresses a critical usability issue discovered through real-world usage with Claude Code. By adding a simple `includeStackTrace` parameter, we enable LLM-based tools to work effectively with Unity console logs without constantly hitting token limits. The 80-90% reduction in token usage transforms the debugging experience from frustrating to smooth.

ğŸ¤– Generated with [Claude Code](https://claude.ai/code)

Co-Authored-By: Claude <noreply@anthropic.com>",Claude_Code,27694,Saqoosha,closed,2025-06-04T23:56:12Z,2025-06-05T08:41:19Z,2025-06-05T08:41:19Z,948148972,https://api.github.com/repos/CoderGamester/mcp-unity,https://github.com/CoderGamester/mcp-unity/pull/44,"analysisWe need to classify as 'performance' or 'non-performance'. The PR adds includeStackTrace option to reduce LLM token usage by 80-90%. This is about token usage, which is a performance improvement in terms of token consumption. The description mentions ""80-90% reduction in token usage"" and ""improves performance"" in terms of LLM context windows. The keywords: ""performance"", ""token usage"", ""speed"", ""throughput"". So it's performance. So label 'performance'.assistantfinalperformance",1,1.0,True
3104322035,64,[WIP] CachedVariable for ChildSelectors,"Thanks for assigning this issue to me. I'm starting to work on it and will keep this PR's description up to date as I form a plan and make progress.

Original issue description:

> 	â€¢ Add CachedVariable (name pending) property that caches the result of an expression for use in the ChildSelectors.
> 		â—‹ A common pattern in ChildSelectors is a switch statement, where you calculate some value and visit different children depending on the result. The issue today is this calculation must be done for each child selector. This feature would allow you to calculate the result once and use it directly multiple times. 
> 		â—‹ Allows you to evaluate an expression and save the result (non-persisted?).
> 		â—‹ ChildSelectors then have access to that variable to simplify their ShouldSelect statements.
> 		â—‹ Consider allowing the variable to be a dynamic object. So could be bool, string, object, etc.. This allows users to set as many properties as they wish.
> 		â—‹ Consider what using a dynamic object would look like. If we have to wrap it every time with (Session.CachedVariable as string), then the usability goes down. In this case, it may be better to just set it as a string. Users could still use it as a bool or object by converting the string if they desire.
> 		â—‹ Consider if this should be localized to just the current TreeNode's ChildSelector, or if you can access parent's CachedVariables as well.
> 		â—‹ Consider if this should be tied to ChildSelector at all. Perhaps just adding a way to persist data in the ForgeStateDictionary from the tree would be cool.
> 			Â§ Perhaps they both could happen though. This would be a helper property to make the feature intention clear. If we decide to add more functionality later for setting ForgeState, we can still do that.
> 		â—‹ Considerations:
> 			Â§ Persisted vs non-persisted
> 			Â§ Useable from ChildSelector only or everywhere?
> 			Â§ Save as String vs Object
> 			Â§ Cache result vs expression itself. Specify if you want to cache the expression result, or reevaluate.
> 			Â§ Consider defining at the tree level.
> 			Â§ Add flag to dictate behavior of CachedVariable. Whether or not to reevaluate each time. Could be unclear what the behavior is by default, make sure to set up the right expectations.
> 		â—‹ Issue with tree structure
> 			Â§ How to enforce that people are calling it at the right time? For example, the expression could GetOutput of a TreeNode that hasn't executed yet.
> 		â—‹ Issue with node structure
> 			Â§ If we decide to persist, not having good discoverability on parent nodes persisted data.
> 		â—‹ Example Old way:
> 	                ""ChildSelector"": [
> 	                    {
> 	                        ""ShouldSelect"": ""C#|(await Session.GetLastActionResponseAsync()).Status == \""Failure\"""",
> 	                        ""Child"": ""SoC_HS_RebootOnly_Failure""
> 	                    },
> 	                    {
> 	                        ""ShouldSelect"": ""C#|(await Session.GetLastActionResponseAsync()).Status == \""Timeout\"""",
> 	                        ""Child"": ""SoC_HS_RebootOnly_Timeout""
> 	                    },
> 	                    {
> 	                        ""ShouldSelect"": ""C#|(await Session.GetLastActionResponseAsync()).Status == \""Success\"""",
> 	                        ""Child"": ""SoC_HS_RebootOnly_Success""
> 	                    }
> 		â—‹ Example New way:
> 	                ""ChildSelectorVariable"": ""C#|(await Session.GetLastActionResponseAsync()).Status"",
> 	                ""ChildSelector"": [
> 	                    {
> 	                        ""ShouldSelect"": ""C#|Session.CachedVariable == \""Failure\"""",
> 	                        ""Child"": ""SoC_HS_RebootOnly_Failure""
> 	                    },
> 	                    {
> 	                        ""ShouldSelect"": ""C#|Session.CachedVariable == \""Timeout\"""",
> 	                        ""Child"": ""SoC_HS_RebootOnly_Timeout""
> 	                    },
> 	                    {
> 	                        ""ShouldSelect"": ""C#|Session.CachedVariable == \""Success\"""",
> 	                        ""Child"": ""SoC_HS_RebootOnly_Success""
>                     }


Fixes #29.

---

ğŸ’¡ You can make Copilot smarter by setting up custom instructions, customizing its development environment and configuring Model Context Protocol (MCP) servers. Learn more [Copilot coding agent tips](https://gh.io/copilot-coding-agent-tips) in the docs.",Copilot,198982749,Copilot,open,2025-05-30T21:32:31Z,,,162360309,https://api.github.com/repos/microsoft/Forge,https://github.com/microsoft/Forge/pull/64,"analysisWe need to classify whether the primary purpose is performance improvement. The PR is about adding CachedVariable for ChildSelectors to avoid recomputing expression for each child selector. This is a performance optimization: caching results to avoid repeated evaluation. The description mentions caching, performance. So classification: 'performance'.assistantfinalperformance",1,1.0,False
3221925890,247,Trim MCP server tool payloads to reduce token consumption,"This PR implements trimmed responses for Azure DevOps MCP server tools to dramatically reduce LLM token consumption while maintaining backward compatibility.

## Problem

The MCP server tools were returning full REST API JSON payloads, causing unnecessary token burn and making it difficult for weaker LLMs to extract relevant information. For example:

- `ado_resolve_comment` was returning 4200+ character responses when a simple ""resolved"" message would suffice
- `ado_list_pull_request_threads` was returning verbose payloads with `_links` objects, deleted comments, and unnecessary author properties

## Solution

Added a `fullResponse` parameter (defaults to `false`) to key repository tools that provides two response modes:

### Trimmed Mode (Default)
- **Action tools** (`resolve_comment`, `reply_to_comment`) return simple success messages
- **List tools** (`list_pull_request_threads`, `list_pull_request_thread_comments`) return filtered data excluding:
  - Deleted comments
  - `_links` objects 
  - Verbose author properties (imageUrl, descriptor, url)

### Full Mode (Opt-in)
- Set `fullResponse: true` to get complete REST API responses for debugging or advanced use cases

## Token Savings

Real-world examples show dramatic improvements:

- **resolve_comment**: 97.3% reduction (1346 â†’ 37 characters)
- **reply_to_comment**: 92.5% reduction (549 â†’ 41 characters)  
- **list_pull_request_threads**: 55.3% reduction (680 â†’ 304 characters)

## Example Usage

```typescript
// Trimmed response (default)
await server.request({
  method: ""tools/call"",
  params: {
    name: ""repo_resolve_comment"",
    arguments: { repositoryId: ""repo1"", pullRequestId: 123, threadId: 456 }
  }
});
// Returns: ""Thread 456 was successfully resolved.""

// Full response (opt-in)
await server.request({
  method: ""tools/call"", 
  params: {
    name: ""repo_resolve_comment"",
    arguments: { 
      repositoryId: ""repo1"", 
      pullRequestId: 123, 
      threadId: 456,
      fullResponse: true 
    }
  }
});
// Returns: Complete thread JSON with all properties
```

## Modified Tools

- `repo_resolve_comment` - Simple success message vs full thread JSON
- `repo_reply_to_comment` - Simple success message vs full comment JSON  
- `repo_list_pull_request_threads` - Filtered thread data vs complete response
- `repo_list_pull_request_thread_comments` - Filtered comment data vs complete response

All changes maintain full backward compatibility via the optional `fullResponse` parameter.

Fixes #36.

<!-- START COPILOT CODING AGENT TIPS -->
---

ğŸ’¡ You can make Copilot smarter by setting up custom instructions, customizing its development environment and configuring Model Context Protocol (MCP) servers. Learn more [Copilot coding agent tips](https://gh.io/copilot-coding-agent-tips) in the docs.",Copilot,198982749,Copilot,closed,2025-07-11T07:44:05Z,2025-07-23T11:44:31Z,2025-07-23T11:44:31Z,984142834,https://api.github.com/repos/microsoft/azure-devops-mcp,https://github.com/microsoft/azure-devops-mcp/pull/247,"analysisWe need to classify as performance or non-performance. The PR is about trimming payloads to reduce token consumption. That is performance improvement: reducing token usage, making responses smaller, improving efficiency. Keywords: token consumption, reduce, trimmed, reduce token burn. So it's performance. So label 'performance'.assistantfinalperformance",1,1.0,False
3076401687,583,Cache CloudInfo / CloudSettings by authority,"This PR modifies `CloudSettings` to cache cloud information by authority (schema, host, and port) rather than by the full URL. This ensures that multiple URLs pointing to the same cluster with different paths will share the same cached `CloudInfo` object.

## Changes Made

1. Modified `_normalize_uri` method in `CloudSettings` class:
   - Now extracts and returns only the authority part (scheme + netloc) from URLs
   - The netloc component includes both host and port when present

2. Updated `get_cloud_info_for_cluster` method:
   - Changed all references to use the normalized authority as the cache key

3. Created comprehensive tests:
   - Tests that `_normalize_uri` correctly extracts just the authority part
   - Tests that URLs with the same authority (but different paths) share the same `CloudInfo` cache entry
   - Tests that URLs with different ports are cached separately

## Example

Before this change:
- `https://cluster.kusto.windows.net/`
- `https://cluster.kusto.windows.net/database1`
- `https://cluster.kusto.windows.net/v1/rest`

Would all be cached separately, requiring multiple network requests for the same cluster.

After this change:
All these URLs will use the same cached `CloudInfo` object for `https://cluster.kusto.windows.net`, improving efficiency and consistency.

Fixes #582.

---

ğŸ’¡ You can make Copilot smarter by setting up custom instructions, customizing its development environment and configuring Model Context Protocol (MCP) servers. Learn more [Copilot coding agent tips](https://gh.io/copilot-coding-agent-tips) in the docs.",Copilot,198982749,Copilot,closed,2025-05-20T09:49:33Z,2025-05-21T08:20:12Z,2025-05-21T08:20:12Z,108257720,https://api.github.com/repos/Azure/azure-kusto-python,https://github.com/Azure/azure-kusto-python/pull/583,"analysisWe need to classify as performance or non-performance. The PR is about caching CloudInfo by authority to reduce network requests, improving efficiency. It's about performance improvement. So label 'performance'.assistantfinalperformance",1,1.0,False
3225248831,5591,fix: mise up parallel execution,Enable parallel installation for `mise upgrade` to improve performance.,Cursor,216188,jdx,closed,2025-07-12T11:45:09Z,2025-07-20T17:30:06Z,2025-07-20T17:30:06Z,586920414,https://api.github.com/repos/jdx/mise,https://github.com/jdx/mise/pull/5591,"analysisWe need to classify as performance or non-performance. The PR title: ""fix: mise up parallel execution"". Body: ""Enable parallel installation for `mise upgrade` to improve performance."" This is clearly a performance improvement. So label 'performance'.assistantfinalperformance",1,1.0,False
3210496890,545,Increase TTL for older posts,"## Summary
- set 24 hour Cache-Control header for posts older than six months
- test TTL behavior for old and new items

## Testing
- `DATABASE_URL=postgres://testuser:testpass@localhost/simonwillisonblog python manage.py test -v2 | tail -n 20`


------
https://chatgpt.com/codex/tasks/task_e_686c5320b124832690bd7094dd8218d5

Transcript: https://chatgpt.com/s/cd_686c56123ff48191b7bea5c066cd8474",OpenAI_Codex,9599,simonw,closed,2025-07-07T23:19:15Z,2025-07-07T23:20:09Z,2025-07-07T23:20:09Z,104670293,https://api.github.com/repos/simonw/simonwillisonblog,https://github.com/simonw/simonwillisonblog/pull/545,"analysisWe need to classify whether the primary purpose is performance. The PR title: ""Increase TTL for older posts"". Summary: set 24 hour Cache-Control header for posts older than six months, test TTL behavior. This is about caching TTL, which is a performance optimization. It improves caching behavior, potentially performance. The keywords: TTL, Cache-Control, older posts. It's about caching, which is performance. So classify as 'performance'.assistantfinalperformance",1,1.0,False
3258420806,10177,feat(agent): support multiple flow log senders,"### This PR is for:

- [x] Agent  

---

### Feature: 

- æ”¯æŒå¤šçº¿ç¨‹å¹¶å‘å‘é€æ—¥å¿—ï¼Œæå‡é«˜ååé‡åœºæ™¯ç¨³å®šæ€§
- å¼•å…¥äº†åŸºäº crossbeam-channel çš„ å…¨æ–° MPMCï¼ˆå¤šç”Ÿäº§è€…å¤šæ¶ˆè´¹è€…ï¼‰é˜Ÿåˆ—å®ç°ï¼Œç”¨äºæ›¿ä»£æˆ–è¡¥å……ç°æœ‰çš„ ring-buffer å®ç°ã€‚

#### èƒŒæ™¯é—®é¢˜

åœ¨ä¸“å±é‡‡é›†å™¨æˆ–é«˜é€Ÿæ•°æ®é¢ï¼ˆå¦‚ 2â€¯Mppsã€L4 Flow é™é€Ÿ 4 ä¸‡æ¡/sï¼‰ä¸‹ï¼Œç°æœ‰é“¾è·¯ä¸­ä»…åŒ…å«å•ä¸ªå‘é€é˜Ÿåˆ—å’Œå•ä¸ª `UniformSenderThread`ï¼Œå¾ˆå®¹æ˜“å‡ºç°å†™å…¥è¿‡å¿«ã€é˜Ÿåˆ—è¢«å†™æ»¡ã€æ—¥å¿—è¦†ç›–ä¸¢å¼ƒçš„é—®é¢˜ï¼Œæœ€ç»ˆå½±å“æ—¥å¿—å®Œæ•´æ€§ä¸å¯è§‚æµ‹æ€§ã€‚

---

#### æœ¬æ¬¡æ”¹åŠ¨å®ç°

##### âœ… ä¸»è¦æ”¹åŠ¨
åœ¨ agent/crates/public/src/queue/ ä¸‹æ–°å¢ï¼šmpmc_queue.rsï¼šå®ç°åŸºäº crossbeam_channel çš„ Senderã€Receiver å’Œ StatsHandleï¼Œæ”¯æŒ MPMC æ¨¡å‹ï¼›

ä¿®æ”¹ queue/mod.rsï¼š å¼•å…¥å¹¶å¯¼å‡º mpmc_queueï¼Œå¢åŠ  bounded_mpmc() æ„é€ å‡½æ•°

##### âœ… æ–°å¢åŠŸèƒ½ï¼š

- å¼•å…¥å¯é…ç½®çš„ **æ—¥å¿—å‘é€å¹¶å‘åº¦å‚æ•°**ï¼ŒåŒ…æ‹¬ï¼š
  - `l4_flow_senders`
  - `l7_flow_senders`
  - `metric_senders`
  - `pcap_senders` ç­‰

- æ¯ç±»æ—¥å¿—ç±»å‹æŒ‰é…ç½®å€¼åˆ›å»ºå¤šä¸ªï¼š
  - æœ‰ç•Œ `DebugSender` é˜Ÿåˆ—ï¼ˆ`queue::bounded_with_debug`ï¼‰
  - å¯¹åº”çš„ `UniformSenderThread` å®ä¾‹

- æ‰€æœ‰ sender å‘½ååç¼€è¿½åŠ ç¼–å·ï¼ˆå¦‚ `3-flowlog-to-collector-sender-0` ~ `-3`ï¼‰

##### âœ… æ—¥å¿—å‘é€æ”¹ä¸ºå¹¶å‘åˆ†å‘ï¼š

- **è´Ÿè½½å‡è¡¡ç­–ç•¥**ï¼šç›®å‰æ”¯æŒï¼š
  - Round-Robinï¼ˆé»˜è®¤ï¼‰
  - å“ˆå¸Œåˆ†å‘ï¼ˆåŸºäºäº”å…ƒç»„ï¼‰å¯æ‰©å±•æ”¯æŒ

- å‘é€çº¿ç¨‹å­˜å…¥ `Vec<UniformSenderThread<_>>`ï¼Œåœ¨ `Trident::start()` ä¸­ç»Ÿä¸€å¯åŠ¨ï¼Œä¼˜é›…é€€å‡ºæ—¶ç»Ÿä¸€åœæ­¢

##### âœ… é…ç½®ç¤ºä¾‹ï¼ˆYAMLï¼‰ï¼š

```yaml
log:
  l4_flow_senders: 4
  l7_flow_senders: 2
  metric_senders: 2

## ChatGPT/CodeX è®¾è®¡æ€è·¯

1. é…ç½®å¹¶å‘åº¦
ä¸ºæ¯ç§æ—¥å¿—ç±»å‹æ–°å¢ä¸€ä¸ªé…ç½®é¡¹ï¼Œå¦‚ l4_flow_sendersã€metrics_senders ç­‰ï¼Œç”¨äºæŒ‡å®šè¦åˆ›å»ºçš„ sender æ•°é‡ã€‚

2. åˆ›å»ºå¤šç»„é˜Ÿåˆ—åŠçº¿ç¨‹
æ ¹æ®ä¸Šè¿°é…ç½®ï¼Œå¾ªç¯è°ƒç”¨ queue::bounded_with_debug å’Œ UniformSenderThread::new åˆ›å»ºè‹¥å¹²é˜Ÿåˆ—åŠå¯¹åº”çš„ UniformSenderThreadã€‚å¯ä»¥åœ¨é˜Ÿåˆ—åç§°åè¿½åŠ ç´¢å¼•åŒºåˆ†ï¼Œä¾‹å¦‚ ""3-flowlog-to-collector-sender-1""ã€""3-flowlog-to-collector-sender-2"" ç­‰ã€‚æ‰€æœ‰ç”Ÿæˆçš„çº¿ç¨‹å­˜å…¥ Vec<UniformSenderThread<_>> ç»Ÿä¸€ç®¡ç†ã€‚

3. åœ¨èšåˆï¼ç”Ÿæˆé˜¶æ®µåˆ†å‘æ•°æ®
åŸå…ˆçš„å‘é€é“¾è·¯ï¼ˆå¦‚ FlowAggrThreadï¼‰åªå‘ä¸€ä¸ª DebugSender å†™æ•°æ®ã€‚æ‰©å±•åï¼Œå¯åœ¨ CollectorThread æˆ– FlowAggrThread ä¸­å®ç°ç®€å•çš„è´Ÿè½½å‡è¡¡ç­–ç•¥ï¼š

- è½®è¯¢ï¼šå¯¹æ¯ä¸ªè¾“å‡ºæ¡ç›®æŒ‰é¡ºåºé€‰æ‹©ä¸åŒçš„ DebugSenderã€‚
- å“ˆå¸Œï¼šæ ¹æ®æµçš„äº”å…ƒç»„æˆ–å…¶ä»–å…³é”®å­—æ®µè®¡ç®—å“ˆå¸Œï¼Œé€‰æ‹©å›ºå®šçš„ senderï¼Œä»è€Œä¿æŒåŒä¸€æµçš„æ•°æ®é¡ºåºã€‚
åˆ†å‘åå³å¯å¹¶è¡Œå†™å…¥å¤šä¸ªå‘é€é˜Ÿåˆ—ã€‚

4. å¯åŠ¨ä¸ç®¡ç†å¤šä¸ªçº¿ç¨‹

åœ¨ Trident::start() ä¸­éå† Vec<UniformSenderThread<_>>ï¼Œé€ä¸€è°ƒç”¨ start() å¯åŠ¨ã€‚åœæ­¢æ—¶åŒæ ·éå† notify_stop æˆ– stop()ã€‚
ç»Ÿè®¡ä¿¡æ¯å’Œå¼‚å¸¸å¤„ç†å¯æ²¿ç”¨ç°æœ‰é€»è¾‘ï¼Œåªéœ€å°†æ¯ä¸ªçº¿ç¨‹çš„è®¡æ•°å™¨æ³¨å†Œåˆ° stats_collectorã€‚

5. å¤šæ¶ˆè´¹è€…/MPMCæ”¯æŒ

å¤šæ¶ˆè´¹è€…ï¼ˆMPMC, Multi-Producer Multi-Consumerï¼‰æ”¯æŒæ˜¯å¯¹å½“å‰é˜Ÿåˆ—æ¨¡å‹ï¼ˆMPSC: å¤šç”Ÿäº§è€…å•æ¶ˆè´¹è€…ï¼‰çš„æ¶æ„çº§å¢å¼ºï¼Œå…¶æ ¸å¿ƒç›®æ ‡æ˜¯æå‡æ¶ˆè´¹è€…å¤„ç†ååèƒ½åŠ›ï¼Œé™ä½å•æ¶ˆè´¹è€…ç“¶é¢ˆå¯¹ç³»ç»Ÿç¨³å®šæ€§çš„å½±å“
",OpenAI_Codex,4133689,svc-design,open,2025-07-24T04:11:19Z,,,448599559,https://api.github.com/repos/deepflowio/deepflow,https://github.com/deepflowio/deepflow/pull/10177,"analysisWe need to classify as performance or non-performance. The PR is about adding multiple flow log senders, concurrency, MPMC queue, to improve throughput and stability. It mentions high throughput scenarios, queue being full, logs being dropped, etc. It's about performance improvement. So classification: 'performance'.assistantfinalperformance",1,1.0,False
3075009767,1561,Enable parallel simulation for AgentPopulation,"## Summary
- allow parallel execution in `AgentPopulation.simulate`
- set a flag to warn only once if parallel execution fails
- replace variable docstring with a comment

## Testing
- `python -m pytest -n auto`",OpenAI_Codex,5382704,alanlujan91,closed,2025-05-19T21:00:37Z,2025-05-20T01:03:16Z,2025-05-20T01:03:16Z,50448254,https://api.github.com/repos/econ-ark/HARK,https://github.com/econ-ark/HARK/pull/1561,"analysisWe need to classify the pull request as performance or non-performance. The PR title: ""Enable parallel simulation for AgentPopulation"". Summary: allow parallel execution in AgentPopulation.simulate, set a flag to warn only once if parallel execution fails, replace variable docstring with a comment. This is about enabling parallel simulation, which likely improves performance. The description mentions parallel execution, which is a performance improvement. So classification: 'performance'.assistantfinalperformance",1,1.0,False
3098597632,529,Make Serial asynchronous,"## Summary
- make `Serial` methods async using a `ThreadPoolExecutor`
- revert lazy FTDI initialization to avoid altering behavior

## Testing
- `make test` *(fails: biotek tests)*",OpenAI_Codex,36956818,rickwierenga,closed,2025-05-28T21:25:47Z,2025-05-28T22:11:11Z,2025-05-28T22:11:11Z,524145041,https://api.github.com/repos/PyLabRobot/pylabrobot,https://github.com/PyLabRobot/pylabrobot/pull/529,"analysisWe need to classify whether the primary purpose is performance improvement. The PR title: ""Make Serial asynchronous"". Summary: make Serial methods async using a ThreadPoolExecutor, revert lazy FTDI initialization to avoid altering behavior. Testing fails. The change is about making Serial asynchronous, likely to improve performance or responsiveness. The keywords: async, ThreadPoolExecutor, performance? The description doesn't explicitly mention performance, but making async could improve responsiveness. The instruction: performance-related PR may contain keywords such as memory, optimization, leak, speed, perf, stuck, lag, fast, respons, slow, latency, performance, latenc, hang, tim, throughput, compile-time-hog, instant, responsive, react, perform, wait, minor, or any other terms referring to how faster and efficiently software runs. The PR mentions ""async"" and ""ThreadPoolExecutor"" but not performance keywords. However, making async could be for performance. But the classification is based on primary purpose. The PR is about making Serial asynchronous, which is likely to improve performance or responsiveness. The summary: ""make Serial methods async using a ThreadPoolExecutor"" - that is a performance improvement. The keywords: ""async"" is not in the list, but ""ThreadPoolExecutor"" is not. But the instruction says ""or any other terms referring to how faster and efficiently software runs."" ""async"" could be considered. But the list includes ""respons"" (responsive). ""async"" is not explicitly listed. But the description says ""Make Serial asynchronous"" which is likely to improve performance. I'd classify as performance.assistantfinalperformance",1,1.0,False
3096663962,983,feat: enhance text generation panel with execution time and token usage display,"## Overview

This PR enhances the text generation panel in the workflow designer UI by adding execution time and token usage information to improve user visibility into generation performance and cost metrics.

<img width=""727"" alt=""image"" src=""https://github.com/user-attachments/assets/8e18699b-0b04-490c-b7f8-5fc19cdd75de"" />


## Changes

### ğŸ—ï¸ Core Schema Updates
- **Added `GenerationUsage` schema** to track prompt/completion tokens
- **Enhanced generation data structure** with optional `usage` field in completed generations
- **Integrated token usage tracking** when completing text generation

### ğŸ•’ Execution Time Display
- **Added execution time indicator** next to the ""Result"" header showing how long each generation took
- **Implemented smart time formatting**:
  - All durations under 1 minute: displayed in milliseconds with comma formatting (e.g., `1,234ms`, `15,678ms`)
  - Durations over 1 minute: displayed in minutes and seconds format (e.g., `2m 30s`)
- **Added timer icon** for clear visual indication

### ğŸ“Š Token Usage Display
- **Added token usage metrics** in the generation panel header for completed generations
- **Displays key metrics**:
  - **Prompt tokens** (â†—): Input tokens consumed
  - **Completion tokens** (â†˜): Output tokens generated
- **Compact, icon-based design** with proper number formatting using locale-aware comma separators
- **Positioned strategically** next to the ""Result"" header for easy reference

### ğŸ¨ UI/UX Improvements
- **Consistent styling** with existing design patterns using proper color tokens and typography
- **Responsive layout** that integrates seamlessly with existing panel structure
- **Performance-conscious rendering** - only displays when data is available
- **Proper TypeScript handling** with appropriate type casting for usage data

## Technical Details

### Files Modified
- **Data Schema**: Core generation usage tracking infrastructure
- **Generation Panel**: `internal-packages/workflow-designer-ui/src/editor/properties-panel/text-generation-node-properties-panel/generation-panel.tsx`
- **Generation View**: `internal-packages/workflow-designer-ui/src/ui/generation-view.tsx`

### Key Implementation Features
- **Helper function `formatExecutionTime()`** for consistent time formatting across the application
- **Conditional rendering** based on generation status and data availability
- **Proper icon integration** using Lucide React icons (`TimerIcon`, `ArrowUpIcon`, `ArrowDownIcon`)
- **Number formatting** using `toLocaleString()` for better readability
- **Type-safe usage data handling** with appropriate TypeScript patterns

## Before vs After

**Before:**
- No execution time visibility
- No token usage information in panel header
- Users couldn't easily assess generation performance or cost
- Basic generation completion tracking only

**After:**
- Clear execution time display: `1,234ms`, `15,678ms`, `2m 30s`
- Token usage metrics: `â†— 1,234t â†˜ 567t`
- Enhanced user experience with actionable performance data
- Complete generation analytics infrastructure

## Benefits

1. **Performance Monitoring**: Users can now track how long their generations take
2. **Cost Awareness**: Token usage display helps users understand API consumption
3. **Better UX**: Immediate feedback on generation efficiency
4. **Debugging Aid**: Execution time helps identify performance bottlenecks
5. **Resource Planning**: Token metrics assist in usage optimization
6. **Data Foundation**: Establishes infrastructure for future analytics features

## Testing

- âœ… Displays correctly for completed generations with usage data
- âœ… Gracefully handles generations without usage information
- âœ… Time formatting works correctly for various duration ranges
- âœ… Token numbers display with proper formatting
- âœ… UI remains responsive and visually consistent
- âœ… No impact on generations in progress or failed states
- âœ… Schema changes maintain backward compatibility

## Build & Type Safety

- âœ… `turbo build --filter '@giselle-sdk/*' --filter giselle-sdk --cache=local:rw`
- âœ… `turbo check-types --cache=local:rw`
- âœ… `turbo test --cache=local:rw`

## Future Considerations

This enhancement provides the foundation for future improvements such as:
- Historical performance tracking
- Cost estimation features
- Performance optimization recommendations
- Usage analytics dashboard
- Cost optimization alerts

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->

## Summary by CodeRabbit

- **New Features**
  - Added a visual section displaying detailed usage statistics for completed text generations, including execution time, prompt tokens, and completion tokens.
  - Execution time is now shown in a user-friendly format with a timer icon, and token counts are accompanied by intuitive icons.
- **Style**
  - Improved visual clarity by adding spacing in the generation message view.

<!-- end of auto-generated comment: release notes by coderabbit.ai -->",OpenAI_Codex,535254,toyamarinyon,closed,2025-05-28T09:22:12Z,2025-05-29T00:21:35Z,2025-05-29T00:21:35Z,845317529,https://api.github.com/repos/giselles-ai/giselle,https://github.com/giselles-ai/giselle/pull/983,"analysisWe need to classify as 'performance' or 'non-performance'. The PR adds execution time display and token usage. It's about performance monitoring. It includes execution time, token usage, performance monitoring. Keywords: execution time, performance monitoring, performance. So it's performance.assistantfinalperformance",1,1.0,True
3197380367,104,Implement context caching system,"## Summary
- add `MemoryCache`, `RedisCache`, and `ContextCache`
- extend `EnrichContext` with a `cache` property
- patch `FastMCP.get_context` in `EnrichMCP`
- provide new caching example and mention caching in README
- test cache backend and update example suite
- finish documentation on caching
- warn and fall back to request scope when user cache used without token

## Testing
- `pre-commit run --files docs/api/cache.md examples/caching/README.md examples/caching/app.py src/enrichmcp/cache/__init__.py tests/test_cache.py`
- `pytest -q`


------
https://chatgpt.com/codex/tasks/task_e_686589273e44832a9572b181db583a6a",OpenAI_Codex,64661186,simba-git,closed,2025-07-02T22:49:46Z,2025-07-02T23:07:17Z,,958026130,https://api.github.com/repos/featureform/enrichmcp,https://github.com/featureform/enrichmcp/pull/104,"analysisWe need to classify the pull request as performance or non-performance. The PR adds caching system, context caching, memory cache, redis cache, etc. This is about caching, which improves performance. The description mentions ""Implement context caching system"" and ""test cache backend"". Caching is a performance improvement. So classification: 'performance'.assistantfinalperformance",1,1.0,False
3078518733,9,Add FastMCP server,"## Summary
- add `run_fastmcp.py` for running DeepSearchAgents via FastMCP
- document running the new MCP server in README

## Testing
- `make test` *(fails: No route to host)*",OpenAI_Codex,11715013,lwyBZss8924d,open,2025-05-21T00:28:08Z,,,966679215,https://api.github.com/repos/lwyBZss8924d/DeepSearchAgents,https://github.com/lwyBZss8924d/DeepSearchAgents/pull/9,"analysisWe need to classify whether the primary purpose is performance. The PR adds a FastMCP server, presumably for performance. The title: ""Add FastMCP server"". The summary: add run_fastmcp.py for running DeepSearchAgents via FastMCP, document running the new MCP server in README. No mention of performance, but FastMCP likely stands for ""Fast Multi-Client Protocol"" or something. The PR is about adding a new server, maybe for performance. But the classification is based on keywords. The description doesn't contain performance keywords. But the title ""Add FastMCP server"" might imply performance. However, the instruction says performance-related PRs may contain keywords such as latency, optimization, slow, react, wait, memory, speed, responsive, minor, tim, instant, latenc, leak, lag, respons, stuck, perf, compile-time-hog, fast, hang, throughput, performance, perform, or any other terms referring to how faster and efficiently software runs. The word ""Fast"" is present. But ""Fast"" is a keyword. So it might be considered performance. But is ""Fast"" enough? The instruction says ""or any other terms referring to how faster and efficiently software runs."" ""Fast"" is a term. So likely performance. But we need to decide. The PR is adding a new server, maybe for performance. The title ""Add FastMCP server"" includes ""Fast"". That might be a performance improvement. But the body doesn't mention performance. But the presence of ""Fast"" might be enough. The guidelines: ""Performance-related pull requests may contain keywords such as... fast..."". So yes, ""fast"" is a keyword. So classify as performance.assistantfinalperformance",1,1.0,False
3126110678,829,k8s(resources): tune workloads and add pedrobot db,"## Summary
- tune CPU & memory requests across infrastructure and apps
- document resource profiles
- add MongoDB StatefulSet for pedrobot

## Testing
- `yamllint -c .yamllint.yml <files>` *(warnings about document start and comments)*
- `kustomize build --enable-helm k8s/applications/web/pedrobot`
- `kustomize build --enable-helm k8s/applications/web/babybuddy`
- `kustomize build --enable-helm k8s/applications/tools/whoami`
- `kustomize build --enable-helm k8s/applications/tools/unrar`
- `kustomize build --enable-helm k8s/applications/tools/it-tools`
- `kustomize build --enable-helm k8s/applications/network/omada`
- `kustomize build --enable-helm k8s/applications/media/whisperasr`
- `kustomize build --enable-helm k8s/applications/media/sabnzbd`
- `kustomize build --enable-helm k8s/applications/media/jellyseerr`
- `kustomize build --enable-helm k8s/applications/media/jellyfin`
- `kustomize build --enable-helm k8s/applications/media/immich/immich-server`
- `kustomize build --enable-helm k8s/applications/media/immich/immich-ml`
- `kustomize build --enable-helm k8s/applications/automation/mqtt`
- `kustomize build --enable-helm k8s/applications/ai/karakeep`
- `kustomize build --enable-helm k8s/applications/ai/openwebui`
- `kustomize build --enable-helm k8s/infrastructure/controllers/argocd` *(failed: helm repo inaccessible)*
- `kustomize build --enable-helm k8s/applications/media/immich/immich-redis` *(failed: helm repo inaccessible)*
- `kustomize build --enable-helm k8s/infrastructure/storage/longhorn` *(failed: helm repo inaccessible)*


------
https://chatgpt.com/codex/tasks/task_e_68437705b0848322ab44b13fcb63a27c",OpenAI_Codex,39008574,theepicsaxguy,closed,2025-06-06T23:38:47Z,2025-06-06T23:53:29Z,2025-06-06T23:53:29Z,933720213,https://api.github.com/repos/theepicsaxguy/homelab,https://github.com/theepicsaxguy/homelab/pull/829,"analysisWe need to classify as performance or non-performance. The PR title: ""k8s(resources): tune workloads and add pedrobot db"". Summary: tune CPU & memory requests across infrastructure and apps, document resource profiles, add MongoDB StatefulSet for pedrobot. This is about resource tuning, which is performance optimization. It may be considered performance. The keywords: tune, CPU & memory requests, resource profiles. That is performance. So classification: performance.assistantfinalperformance",1,1.0,False
2959068895,646,feat: implement HTTP caching with mitmproxy's native format,"This PR implements HTTP caching for connectors using mitmproxy. It supports four cache modes, separate read/write directories, and configurable serialization formats including mitmproxy's native format for better interoperability.

Link to Devin run: https://app.devin.ai/sessions/9bbcc89c5dc047cabfe064370d8ca798
Requested by: Aaron (""AJ"") Steers (aj@airbyte.io)",Devin,158243242,devin-ai-integration[bot],closed,2025-03-30T19:59:25Z,2025-04-24T15:23:57Z,,752526884,https://api.github.com/repos/airbytehq/PyAirbyte,https://github.com/airbytehq/PyAirbyte/pull/646,"analysisWe need to classify as performance or non-performance. The PR implements HTTP caching. Does that improve performance? Caching can improve performance by reducing network latency, but the primary purpose is to implement caching, not necessarily performance improvement. However caching is a performance optimization. The description: ""implement HTTP caching with mitmproxy's native format"". That is a performance improvement. The keywords: ""caching"" is not explicitly in the list but caching is a performance improvement. The list includes ""speed, perf, latency, throughput"". Caching reduces latency. So likely performance. The primary purpose is to implement caching, which is a performance improvement. So classify as 'performance'.assistantfinalperformance",1,1.0,False
3162346531,609,Replace random sampling with Farthest Point Sampling for better spatial coverage,"# Replace Random Sampling with Farthest Point Sampling for Better Spatial Coverage

## Overview
This PR replaces the current random sampling implementation in the hierarchical merge labelling step with Farthest Point Sampling (FPS) to achieve better spatial coverage of opinions across the entire opinion space.

## Changes Made
- **Added fpsample library import** to `hierarchical_merge_labelling.py`
- **Replaced random sampling logic** in `process_merge_labelling` function with FPS using x,y coordinates
- **Added robust error handling** to fallback to random sampling if x,y coordinates are unavailable or FPS fails
- **Maintained existing interface** - no changes to function signatures or sampling_num parameter behavior

## Benefits
- **Better spatial coverage**: FPS selects points that are maximally distant from each other in the x,y coordinate space
- **More representative sampling**: Ensures comprehensive coverage of the opinion space rather than potentially clustering around similar spatial regions
- **Robust fallback**: Gracefully handles edge cases by falling back to original random sampling when needed

## Technical Details
- Uses `fpsample.fps_sampling()` - a high-performance Rust-based FPS implementation (100x faster than numpy)
- Checks for presence of x,y coordinates before applying FPS
- Handles cases where sampling_num >= available data points
- Maintains backward compatibility with existing pipeline configuration

## Testing
- âœ… Lint checks pass (`python -m ruff check .`)
- âœ… Import verification successful
- âœ… Error handling tested for missing coordinates scenario

## Link to Devin run
https://app.devin.ai/sessions/ad4f0bb2409a43c798480409db4c336d

## Requested by
shinta.nakayama@gmail.com
",Devin,158243242,devin-ai-integration[bot],closed,2025-06-20T08:19:46Z,2025-06-28T14:56:24Z,,934897158,https://api.github.com/repos/digitaldemocracy2030/kouchou-ai,https://github.com/digitaldemocracy2030/kouchou-ai/pull/609,"analysisWe need to classify as performance or non-performance. The PR replaces random sampling with Farthest Point Sampling for better spatial coverage. It mentions fpsample library is high-performance Rust-based FPS implementation (100x faster than numpy). That is a performance improvement. The primary purpose is to improve spatial coverage, but also mentions performance: ""high-performance Rust-based FPS implementation (100x faster than numpy)"". The question: ""determine whether the primary purpose of the pull request is to improve performance."" The primary purpose seems to be better spatial coverage, but also performance improvement. The guidelines: performance-related PRs may contain keywords such as memory, optimization, leak, speed, perf, stuck, lag, fast, respons, slow, latency, performance, latenc, hang, tim, throughput, compile-time-hog, instant, responsive, react, perform, wait, minor, or any other terms referring to how faster and efficiently software runs. This PR includes ""high-performance Rust-based FPS implementation (100x faster than numpy)"". That is a performance improvement. But the main goal is better spatial coverage. However, the presence of performance keywords and mention of speed improvement qualifies as performance. The classification should be 'performance'.assistantfinalperformance",1,1.0,False
3097456896,22,Advanced Bandwidth Aggregation Optimization,"# Advanced Bandwidth Aggregation Optimization

This PR implements several advanced optimizations to further improve the bandwidth aggregation efficiency, building on the previous optimizations (out-of-order frame processing and dynamic allocation).

## New Optimizations

### 1. RTT-Based Congestion Control
- Added RTT statistics tracking with smoothed RTT and RTT variation measurements
- Enhanced congestion window adjustment based on network conditions
- Early congestion detection using RTT increases
- Improved slow start exit conditions based on RTT measurements

### 2. Adaptive Frame Sizing
- Dynamically adjusts frame size based on network conditions
- Increases frame size when network is stable (low RTT variation)
- Decreases frame size during congestion or unstable network conditions
- Configurable minimum and maximum frame size bounds

### 3. Predictive Retransmission
- Proactively retransmits frames that might be lost based on RTT timeout
- Reduces head-of-line blocking by not waiting for explicit timeouts
- Improves performance on networks with packet loss

## Expected Benefits

These optimizations should further improve bandwidth aggregation efficiency, especially in the following scenarios:
- Networks with variable latency
- Connections with occasional packet loss
- Highly asymmetric worker configurations
- Long-distance transfers with higher RTT

The adaptive frame sizing will help optimize memory usage and throughput based on actual network conditions, while the enhanced congestion control will better utilize available bandwidth while avoiding congestion collapse.

Link to Devin run: https://app.devin.ai/sessions/58cab6f5ac1f41a295ed3edc4e6f7eb5
Requested by: fatedier
",Devin,158243242,devin-ai-integration[bot],closed,2025-05-28T13:44:50Z,2025-05-30T08:52:49Z,,119536149,https://api.github.com/repos/fatedier/fft,https://github.com/fatedier/fft/pull/22,"analysisWe need to classify as performance or non-performance. The PR title: ""Advanced Bandwidth Aggregation Optimization"". Body describes optimizations to improve bandwidth aggregation efficiency, RTT-based congestion control, adaptive frame sizing, predictive retransmission. These are performance improvements. So classification: performance.assistantfinalperformance",1,1.0,False
3097403205,21,Optimize bandwidth aggregation with out-of-order processing and dynamic allocation,"# Bandwidth Aggregation Optimization

This PR implements two key optimizations to improve bandwidth aggregation efficiency when using workers with unbalanced bandwidth configurations:

## Changes Made

1. **Out-of-order Frame Processing**: Modified the receiver to buffer frames that arrive out of order, preventing slower workers from blocking faster ones
2. **Dynamic Frame Allocation**: Implemented performance tracking and dynamic frame distribution based on actual worker throughput

## Performance Results

- **Before**: 100KB/s + 500KB/s workers achieved only 34.72% efficiency (208.31KB/s actual vs 600KB/s expected)
- **After**: 100KB/s + 500KB/s workers achieved 69.44% efficiency (416.64KB/s actual vs 600KB/s expected)
- **Balanced Workers**: 300KB/s + 300KB/s workers achieved 100.59% efficiency (603.53KB/s actual vs 600KB/s expected)

## Testing

Tested with bandwidth-test tool using various worker configurations to verify improvements:
- Unbalanced workers (100KB/s + 500KB/s): Efficiency improved from 34.72% to 69.44%
- Balanced workers (300KB/s + 300KB/s): Achieved 100.59% efficiency

Link to Devin run: https://app.devin.ai/sessions/3571e39c6d55434a84d80937d3437d5b
Requested by: fatedier
",Devin,158243242,devin-ai-integration[bot],closed,2025-05-28T13:27:57Z,2025-05-28T13:36:35Z,2025-05-28T13:36:35Z,119536149,https://api.github.com/repos/fatedier/fft,https://github.com/fatedier/fft/pull/21,"analysisWe need to classify as 'performance' or 'non-performance'. The PR title: ""Optimize bandwidth aggregation with out-of-order processing and dynamic allocation"". Body describes performance improvements, speed, efficiency, etc. So it's performance.assistantfinalperformance",1,1.0,False
2976324699,246,[FEATURE] 1å›ã®extractionã§è¤‡æ•°ã®commentã‚’å‡¦ç†ã™ã‚‹,"# è¤‡æ•°ã‚³ãƒ¡ãƒ³ãƒˆã‚’1å›ã®extractionå‡¦ç†ã§å‡¦ç†ã™ã‚‹

## æ¦‚è¦
Issue #190 ã®å®Ÿè£…ã§ã™ã€‚1å›ã®LLMãƒªã‚¯ã‚¨ã‚¹ãƒˆã§è¤‡æ•°ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚’åŒæ™‚ã«å‡¦ç†ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã—ãŸã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å‡¦ç†é€Ÿåº¦ã®å‘ä¸Šã¨OpenAI APIã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®å•é¡Œã®ç·©å’ŒãŒæœŸå¾…ã§ãã¾ã™ã€‚

## å¤‰æ›´å†…å®¹
- extractionPromptã‚’æ›´æ–°ã—ã€è¤‡æ•°ã‚³ãƒ¡ãƒ³ãƒˆã‚’ä¸€åº¦ã«å‡¦ç†ã§ãã‚‹ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›´
- extraction.pyã«ãƒãƒƒãƒå‡¦ç†ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè£…
- parse_json_list.pyã‚’æ‹¡å¼µã—ã€æ–°ã—ã„JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼ã«å¯¾å¿œ

## ãƒ†ã‚¹ãƒˆ
- çŸ­ã„ã‚³ãƒ¡ãƒ³ãƒˆï¼ˆãƒ„ã‚¤ãƒ¼ãƒˆãƒ¬ãƒ™ãƒ«ï¼‰ã¨é•·ã„ã‚³ãƒ¡ãƒ³ãƒˆï¼ˆå…¬é–‹ã‚³ãƒ¡ãƒ³ãƒˆãƒ¬ãƒ™ãƒ«ï¼‰ã®ä¸¡æ–¹ã§ãƒ†ã‚¹ãƒˆ
- å¾“æ¥ã®æ–¹æ³•ã¨æ¯”è¼ƒã—ã¦å‡¦ç†é€Ÿåº¦ã®å‘ä¸Šã‚’ç¢ºèª

Link to Devin run: https://app.devin.ai/sessions/c9d31459eaed4fe9bc2274aaf699d893
Requested by: annyotaka@gmail.com
",Devin,158243242,devin-ai-integration[bot],closed,2025-04-07T10:02:28Z,2025-04-09T04:36:03Z,,934897158,https://api.github.com/repos/digitaldemocracy2030/kouchou-ai,https://github.com/digitaldemocracy2030/kouchou-ai/pull/246,"analysisWe need to classify as performance or non-performance. The PR title: ""[FEATURE] 1å›ã®extractionã§è¤‡æ•°ã®commentã‚’å‡¦ç†ã™ã‚‹"" and body says: ""Issue #190 Implementation. 1 LLM request can process multiple comments simultaneously. This improves processing speed and mitigates OpenAI API rate limit issues."" So it's about performance improvement. So classification: performance.assistantfinalperformance",1,1.0,False
2978149205,2533,Add Context Caching Support,"Fixes #2532

This PR adds support for context caching in CrewAI, leveraging LiteLLM's cache_control parameter. This feature improves performance and reduces costs by caching parts of prompts that remain unchanged across multiple API calls.

## Features
- Added `cache_enabled` and `cache_ttl` parameters to LLM class
- Modified the LLM.call method to include cache_control in API calls when enabled
- Added tests to verify caching functionality
- Updated documentation to explain how to use context caching

Different LLM providers handle caching differently:
- OpenAI and Deepseek models have prompt caching enabled automatically
- Anthropic and Gemini require explicit caching instructions
- Gemini also supports TTL configuration

## Testing
- Added unit tests for cache_control parameter
- Ran the full test suite to ensure compatibility

Link to Devin run: https://app.devin.ai/sessions/c0bef5c2769a4078a2087a245034e5b4
Request by @joao@crewai.com",Devin,158243242,devin-ai-integration[bot],closed,2025-04-07T22:39:49Z,2025-04-16T15:59:50Z,,710601088,https://api.github.com/repos/crewAIInc/crewAI,https://github.com/crewAIInc/crewAI/pull/2533,"analysisWe need to classify as performance or non-performance. The PR adds context caching support, which improves performance and reduces costs. It mentions caching, performance improvement. So it's performance.assistantfinalperformance",1,1.0,False
3074351366,1569,FIR-2006: Fix maxUrls and timeLimit parameters in Deep Research API,"# FIR-2006: Fix maxUrls and timeLimit parameters in Deep Research API

## Problem
The Deep Research API wasn't properly enforcing two key parameters:
1. When users set `maxUrls=5`, the API processed many more URLs (88 in the reported case)
2. When users set `timeLimit=180`, the API ran much longer (400s in the reported case)

## Solution
1. Fixed URL counting logic by replacing the async filter function with a for loop implementation that properly counts URLs and enforces the maxUrls limit immediately
2. Added a time limit helper function and more frequent time limit checks throughout the code
3. Added better logging for debugging URL counts and time limits

## Testing
- Added unit tests to verify maxUrls and timeLimit enforcement
- Tested locally to ensure parameters are properly respected

Link to Devin run: https://app.devin.ai/sessions/f04a46755b0a46438087d9dea98cf5b8
Requested by: Nicolas Camara
",Devin,158243242,devin-ai-integration[bot],closed,2025-05-19T16:04:38Z,2025-05-20T21:39:56Z,2025-05-20T21:39:56Z,787076358,https://api.github.com/repos/mendableai/firecrawl,https://github.com/mendableai/firecrawl/pull/1569,"analysisWe need to classify as performance or non-performance. The PR is about fixing maxUrls and timeLimit parameters in Deep Research API. It addresses that the API didn't enforce maxUrls and timeLimit properly, causing it to process too many URLs and run too long. The fix includes replacing async filter with for loop, adding time limit helper, more frequent checks, better logging. This is about ensuring the API respects limits, which is about performance? The issue is that the API was running longer than expected, processing too many URLs, causing performance issues. The fix is to enforce limits, which improves performance by preventing overuse. The description mentions ""time limit"" and ""maxUrls"" enforcement. It's about limiting resource usage, which is performance-related. The keywords: timeLimit, maxUrls, processing many more URLs, ran longer. The fix is to enforce limits, which is a performance improvement. So classify as 'performance'.

Thus output: performance.assistantfinalperformance",1,1.0,False
2932553016,2407,Add kickoff_for_each_parallel method using ThreadPoolExecutor,"Fixes #2406 - Adds a new method to run a crew multiple times in parallel on different inputs using ThreadPoolExecutor. This allows for better performance when running the same crew on many inputs.

Link to Devin run: https://app.devin.ai/sessions/3184eb8f13bf4af58cd59a2dc3133f63
Requested by: Joe Moura (joao@crewai.com)",Devin,158243242,devin-ai-integration[bot],closed,2025-03-19T17:05:51Z,2025-03-28T14:55:15Z,,710601088,https://api.github.com/repos/crewAIInc/crewAI,https://github.com/crewAIInc/crewAI/pull/2407,"analysisWe need to classify as performance or non-performance. The PR adds a method using ThreadPoolExecutor to run crew multiple times in parallel, improving performance. So it's performance.assistantfinalperformance",1,1.0,False
3067476387,98,feat: add caching layer for frequent identical requests,"# Project-level Caching Layer for Identical Requests

This PR adds a caching layer for frequent identical requests on a per-project basis, with configurable cache duration and API-based toggleability.

## Features

- Added `cacheEnabled` and `cacheDuration` columns to the project table
- Created API endpoints to toggle and configure caching settings
- Implemented caching logic in request handling for non-streaming requests
- Cache duration configurable between 10 seconds and 1 year (31536000 seconds)
- Cache is disabled by default and must be explicitly enabled per project

## Implementation Details

- Cache keys are generated based on all parameters that affect the response
- Only non-streaming requests are cached (streaming responses not suitable for caching)
- Cache integrates with the existing logging system to track both cache hits and misses
- Default cache duration is 1 hour (3600 seconds)

## Notes

There are test failures related to the database schema changes. The schema changes have been applied to the development database using `pnpm push`, but the test database appears to be using a different configuration.

## Link to Devin run
https://app.devin.ai/sessions/725b346a9bd34bde9c44274075b94806

Requested by: Luca Steeb
",Devin,158243242,devin-ai-integration[bot],closed,2025-05-15T22:08:58Z,2025-05-17T09:43:46Z,,965250949,https://api.github.com/repos/theopenco/llmgateway,https://github.com/theopenco/llmgateway/pull/98,analysisWe need to classify as performance or non-performance. The PR adds caching layer for frequent identical requests. Caching improves performance. So classification: performance.assistantfinalperformance,1,1.0,False
3216324404,950,Enhance multi-agent swarm structures,"Thank you for contributing to Swarms!

- Description: This PR significantly enhances the `HierarchicalSwarm` structure to improve its reliability, performance, and manageability. The core changes focus on making multi-agent task execution more robust and efficient for production environments.

  Key improvements include:
  - **Enhanced Reliability:** Implemented agent health monitoring, automatic failure detection, configurable retry mechanisms with exponential backoff, and graceful degradation.
  - **Improved Performance:** Enabled concurrent task execution using `ThreadPoolExecutor`, intelligent load balancing, and priority-based scheduling.
  - **Robust Task Management:** Added per-task timeouts, retry counts, and dependency management for complex workflows.
  - **Better Observability:** Introduced real-time performance metrics and comprehensive logging for easier monitoring and troubleshooting.
  - **Graceful Shutdown:** Added context manager support for proper resource cleanup.
- Issue: None
- Dependencies: None
- Tag maintainer: kye@apac.ai
- Twitter handle:

Please make sure your PR is passing linting and testing before submitting. Run `make format`, `make lint` and `make test` to check this locally.

See contribution guidelines for more information on how to write/run tests, lint, etc: 
https://github.com/kyegomez/swarms/blob/master/CONTRIBUTING.md

If you're adding a new integration, please include:
  1. a test for the integration, preferably unit tests that do not rely on network access,
  2. an example notebook showing its use.


Maintainer responsibilities:
  - General / Misc / if you don't know who to tag: kye@apac.ai
  - DataLoaders / VectorStores / Retrievers: kye@apac.ai
  - swarms.models: kye@apac.ai
  - swarms.memory: kye@apac.ai
  - swarms.structures: kye@apac.ai

If no one reviews your PR within a few days, feel free to email Kye at kye@apac.ai

See contribution guidelines for more information on how to write/run tests, lint, etc: https://github.com/kyegomez/swarms

<!-- readthedocs-preview swarms start -->
----
ğŸ“š Documentation preview ğŸ“š: https://swarms--950.org.readthedocs.build/en/950/

<!-- readthedocs-preview swarms end -->",Cursor,98760976,kyegomez,open,2025-07-09T15:25:27Z,,,639195966,https://api.github.com/repos/kyegomez/swarms,https://github.com/kyegomez/swarms/pull/950,"analysisWe need to classify as performance or non-performance. The PR title: ""Enhance multi-agent swarm structures"". Body: mentions improved performance, concurrent task execution, load balancing, priority scheduling. So it's performance-related. So label 'performance'.assistantfinalperformance",1,1.0,False
3239327534,135,implement context caching,"## Summary
- implement cached content resource and data handling
- expose cachedContent methods on client & client fake
- add model enum patch
- provide tests and fixtures for cached content

## Testing
- `composer lint`
- `vendor/bin/phpstan analyse --ansi`
- `vendor/bin/pest --colors=always`

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->
## Summary by CodeRabbit

* **New Features**
  * Introduced comprehensive cached content management: create, retrieve, list, update, and delete cached contents via the client interface.
  * Added detailed metadata and list responses for cached content, improving data handling and visibility.
  * Enhanced HTTP support by adding the PATCH method for partial updates.
* **Tests**
  * Delivered extensive unit tests covering all cached content operations and response behaviors for robustness and accuracy.
<!-- end of auto-generated comment: release notes by coderabbit.ai --",OpenAI_Codex,3524595,michabbb,open,2025-07-17T12:04:24Z,,,755889931,https://api.github.com/repos/google-gemini-php/client,https://github.com/google-gemini-php/client/pull/135,"analysisWe need to classify whether the primary purpose is performance. The PR implements context caching, cached content resource, data handling, expose methods, tests, etc. This is about caching, which is a performance improvement. The description mentions ""cached content"" and ""context caching"". That is performance. So label 'performance'.assistantfinalperformance",1,1.0,False
3155802145,849,Add shm volume to selenium pod recipe,"## Summary
- update selenium recipe with /dev/shm volume
- document that the 2Gi volume mirrors devcontainer and CI

## Testing
- `cargo fmt --all`
- `cargo check -q` *(fails: called `Result::unwrap()` on an `Err` value)*

------
https://chatgpt.com/codex/tasks/task_e_68526a9b6168832084855aefcfae2589",OpenAI_Codex,173255911,242816,closed,2025-06-18T07:33:24Z,2025-06-18T07:46:56Z,2025-06-18T07:46:56Z,663396489,https://api.github.com/repos/bionic-gpt/bionic-gpt,https://github.com/bionic-gpt/bionic-gpt/pull/849,"analysisWe need to classify whether the primary purpose is performance. The PR title: ""Add shm volume to selenium pod recipe"". Summary: update selenium recipe with /dev/shm volume, document that the 2Gi volume mirrors devcontainer and CI. This is about adding shared memory volume to Selenium pod. That likely improves performance? /dev/shm is used for shared memory, often to speed up certain operations, e.g., Chrome uses /dev/shm for performance. So adding shm volume could improve performance. But is the primary purpose performance? The description: ""update selenium recipe with /dev/shm volume"" and ""document that the 2Gi volume mirrors devcontainer and CI"". It's about adding a volume to improve performance of Selenium pods. The keywords: ""shm"" is memory, but not explicitly performance. But adding shared memory can improve performance. The classification: performance. The guidelines: performance-related PRs may contain keywords such as memory, optimization, leak, speed, perf, etc. Here ""shm"" is memory. So it's performance. So label 'performance'.assistantfinalperformance",1,1.0,False
3089770645,403,Fix cache headers on range requests,"## Summary
- set caching headers before handling byte ranges
- document X-Cache and related headers for 206 responses
- test that range responses include cache header

## Testing
- `npx eslint src test`
- `npm test` *(fails: fetch failed)*",OpenAI_Codex,1110,djwhitt,closed,2025-05-26T01:46:50Z,2025-05-27T17:26:37Z,2025-05-27T17:26:37Z,480614136,https://api.github.com/repos/ar-io/ar-io-node,https://github.com/ar-io/ar-io-node/pull/403,"analysisWe need to classify whether the primary purpose is performance improvement. The PR title: ""Fix cache headers on range requests"". Summary: set caching headers before handling byte ranges, document X-Cache and related headers for 206 responses, test that range responses include cache header. This is about caching headers, which affect performance (caching improves performance). But is it primarily performance? It's about cache headers, which is about performance optimization. The keywords: ""cache"" is not explicitly in the list but ""performance"" improvement. The list includes ""latency"", ""throughput"", ""speed"", ""memory"", etc. ""cache"" is not listed but caching is a performance improvement. The instruction: ""Performance-related pull requests may contain keywords such as memory, optimization, leak, speed, perf, stuck, lag, fast, respons, slow, latency, performance, latenc, hang, tim, throughput, compile-time-hog, instant, responsive, react, perform, wait, minor, or any other terms referring to how faster and efficiently software runs."" The PR is about caching headers, which is about performance. So classify as 'performance'.

Thus answer: performance.assistantfinalperformance",1,1.0,False
3070955999,28,Add translate-crowdin-parallel command,"## Summary
- allow Crowdin translations to run in parallel
- register new command in the service provider
- mention new command in README

## Testing
- `git status --short`",OpenAI_Codex,1438533,kargnas,closed,2025-05-17T17:36:09Z,2025-05-17T17:54:29Z,,822046563,https://api.github.com/repos/kargnas/laravel-ai-translator,https://github.com/kargnas/laravel-ai-translator/pull/28,"analysisWe need to classify whether the primary purpose of the PR is to improve performance. The PR adds a new command to run Crowdin translations in parallel. That is about performance: parallel execution speeds up translation process. The description: ""allow Crowdin translations to run in parallel"" - that is a performance improvement. So classification: 'performance'.assistantfinalperformance",1,1.0,False
3210894862,91,Optimize tool agent prompt usage,"## Summary
- ensure orchestrator only sees concise instructions for each tool-agent
- list full tool schemas only when running tool agents directly
- add a flag for including tool schemas in RunSessionWithHook

## Testing
- `go test ./...` *(fails: Get ""https://proxy.golang.org/..."": Forbidden)*

------
https://chatgpt.com/codex/tasks/task_e_686c8e7ba8348328bed227e3063b0439",OpenAI_Codex,1919951,Art9681,closed,2025-07-08T03:33:08Z,2025-07-09T22:19:07Z,,927421008,https://api.github.com/repos/intelligencedev/manifold,https://github.com/intelligencedev/manifold/pull/91,"analysisWe need to classify the pull request as performance or non-performance. The title: ""Optimize tool agent prompt usage"". Summary: ""ensure orchestrator only sees concise instructions for each tool-agent, list full tool schemas only when running tool agents directly, add a flag for including tool schemas in RunSessionWithHook"". Testing: go test fails. This PR is about optimizing tool agent prompt usage. That sounds like performance improvement: reducing prompt size, maybe improving speed. Keywords: optimize, performance. So classify as 'performance'.assistantfinalperformance",1,1.0,False
3135095932,1787,Cache request settings for Client,"## Summary
- compute `_headers` and `_timeout` during client initialization
- update request logic to reuse cached values
- refresh `_headers` when the API key changes
- update unit tests for header caching

## Testing
- `make format`
- `make lint`
- `make tests`


------
https://chatgpt.com/codex/tasks/task_e_6848a9ac7f0c832d9f48d9faac108d32",OpenAI_Codex,56902,nfcampos,closed,2025-06-11T00:02:14Z,2025-06-11T03:23:30Z,2025-06-11T03:23:30Z,647427819,https://api.github.com/repos/langchain-ai/langsmith-sdk,https://github.com/langchain-ai/langsmith-sdk/pull/1787,"analysisWe need to classify whether the primary purpose is performance. The PR title: ""Cache request settings for Client"". Summary: compute headers and timeout during client init, reuse cached values, refresh when API key changes, update tests. This is about caching to avoid recomputing each request. That is a performance improvement: reducing overhead per request. So classification: performance.assistantfinalperformance",1,1.0,False
3200836668,2529,Ensure at least once delivery in ClickHouse,"Apply `wait_end_of_query=1` to ClickHouse INSERT and DDL operations to ensure at least once delivery and DDL acknowledgment.

Initially, `wait_end_of_query=1` was incorrectly applied globally, which would have severely degraded SELECT query performance by forcing full response buffering. This PR corrects the implementation to apply the setting only to INSERT and DDL queries, preserving streaming and concurrency for SELECTs.",Cursor,4429209,callicles,closed,2025-07-03T22:50:53Z,2025-07-04T17:08:37Z,2025-07-04T17:08:37Z,668493044,https://api.github.com/repos/514-labs/moose,https://github.com/514-labs/moose/pull/2529,"analysisWe need to classify as performance or non-performance. The PR title: ""Ensure at least once delivery in ClickHouse"". Body: ""Apply `wait_end_of_query=1` to ClickHouse INSERT and DDL operations to ensure at least once delivery and DDL acknowledgment. Initially, `wait_end_of_query=1` was incorrectly applied globally, which would have severely degraded SELECT query performance by forcing full response buffering. This PR corrects the implementation to apply the setting only to INSERT and DDL queries, preserving streaming and concurrency for SELECTs.""

This is about performance: it mentions SELECT query performance, streaming, concurrency. It's about ensuring performance by limiting the setting to certain queries. So it's performance-related. So classification: performance.assistantfinalperformance",1,1.0,False
3196526840,1645,Investigate caching options for from_provider(),"> feat: add native caching support to from_provider

## Describe your changes

This PR introduces a native caching mechanism, allowing users to easily integrate various caching strategies with `from_provider`.

Key changes include:
- **New `instructor.cache` module**: Provides `BaseCache` interface and concrete implementations (`AutoCache` (in-process LRU), `DiskCache`, `RedisCache`).
- **Schema-aware cache keys**: `make_cache_key` ensures that any changes to the Pydantic response model's fields or descriptions automatically bust the cache.
- **Integrated into `patch`**: Caching logic is injected into `instructor/patch.py` for both synchronous and asynchronous calls, occurring *before* the retry mechanism.
- **`from_provider` API**: The `from_provider` function now accepts `cache` and `cache_ttl` keyword arguments, which are propagated to the underlying client.
- **Documentation**: The `docs/blog/posts/caching.md` has been updated to highlight the new built-in caching capabilities.
- **Tests**: Unit tests for cache key invalidation and an integration test demonstrating cache functionality with a mocked provider have been added.

This allows users to simply pass a cache instance to `from_provider` to enable caching, e.g., `from_provider(..., cache=AutoCache())`.

## Issue ticket number and link

## Checklist before requesting a review

- [x] I have performed a self-review of my code
- [x] If it is a core feature, I have added thorough tests.
- [x] If it is a core feature, I have added documentation.
<!-- ELLIPSIS_HIDDEN -->


----

> [!IMPORTANT]
> Introduces native caching support for `from_provider` with new cache implementations, API changes, and updated documentation and tests.
> 
>   - **Caching Mechanism**:
>     - Adds `instructor.cache` module with `BaseCache`, `AutoCache`, `DiskCache`, and `RedisCache`.
>     - `make_cache_key` generates schema-aware cache keys to auto-bust cache on model changes.
>     - Integrates caching into `instructor/patch.py` for sync/async calls before retry logic.
>   - **API Changes**:
>     - `from_provider` now accepts `cache` and `cache_ttl` arguments.
>   - **Documentation**:
>     - Updates `docs/blog/posts/caching.md` to include new caching features.
>   - **Testing**:
>     - Adds unit tests for cache key invalidation and integration tests with mocked providers.
> 
> <sup>This description was created by </sup>[<img alt=""Ellipsis"" src=""https://img.shields.io/badge/Ellipsis-blue?color=175173"">](https://www.ellipsis.dev?ref=567-labs%2Finstructor&utm_source=github&utm_medium=referral)<sup> for e60c46a29adf342cda3f50010a49f054b8a2003d. You can [customize](https://app.ellipsis.dev/567-labs/settings/summaries) this summary. It will automatically update as commits are pushed.</sup>


<!-- ELLIPSIS_HIDDEN -->",Cursor,4852235,jxnl,closed,2025-07-02T16:43:11Z,2025-07-07T18:50:34Z,,653589102,https://api.github.com/repos/567-labs/instructor,https://github.com/567-labs/instructor/pull/1645,"analysisWe need to classify as performance or non-performance. The PR adds caching support. Caching is a performance improvement. The description mentions caching, key invalidation, etc. So it's performance. The classification: 'performance'.assistantfinalperformance",1,1.0,False
3067474905,97,feat(cache): async log processing with queue,"# Use Valkey for async log processing with queue

Added Redis (Valkey) for async log processing using a message queue. Modified the insertLog function to send items to a queue, and created a worker that listens to the queue and handles the actual database insertions.

## Changes
- Added Valkey service to docker-compose.yml
- Created Redis client utility in redis.ts
- Modified insertLog to publish to queue instead of direct DB insert
- Created worker to consume from queue and handle DB insertions
- Updated serve.ts to start the worker

Link to Devin run: https://app.devin.ai/sessions/ab026fb2d0e3474ea172fa879b93df26
Requested by: Luca Steeb
",Devin,158243242,devin-ai-integration[bot],closed,2025-05-15T22:07:53Z,2025-05-16T13:03:44Z,2025-05-16T13:03:44Z,965250949,https://api.github.com/repos/theopenco/llmgateway,https://github.com/theopenco/llmgateway/pull/97,"analysisWe need to classify as performance or non-performance. The PR adds async log processing with queue, using Redis. It's about improving performance? It changes insertLog to queue, worker handles DB insertions. This likely improves performance by offloading DB writes, making API faster. So it's performance. The guidelines: performance-related PR may contain keywords like speed, performance, etc. This PR doesn't explicitly mention performance, but the change is about async processing, which is a performance improvement. The instruction says ""primary purpose of the pull request is to improve performance."" This PR's primary purpose is to improve performance by using async queue. So classify as 'performance'.assistantfinalperformance",1,1.0,False
3189869767,73,Implement maxAge fast scraping parameter,"
# Implement maxAge fast scraping parameter

## Summary

This PR implements the `maxAge` parameter for the firecrawl-mcp-server to enable faster scraping through caching, addressing GitHub issue #69. The implementation exposes the existing Firecrawl API `maxAge` parameter through the MCP server's tool schema.

**Key changes:**
- Added `maxAge` parameter to SCRAPE_TOOL inputSchema as optional number field
- Updated tool description and usage examples to highlight caching benefits  
- Added test coverage to verify parameter is passed through to Firecrawl API
- Merged latest main branch changes (version bump to 1.11.0)

The `maxAge` parameter allows users to specify a cache duration in milliseconds. When set, the system will use cached content if available and younger than the specified age, otherwise scrape fresh content.

## Review & Testing Checklist for Human

- [ ] **Test maxAge with real Firecrawl API calls** - Verify that setting maxAge actually enables caching behavior (most critical)
- [ ] **Validate performance claims** - Test whether maxAge actually provides significant speed improvements as claimed
- [ ] **Test edge cases** - Try invalid maxAge values (negative, non-numeric) to ensure proper error handling
- [ ] **Verify backwards compatibility** - Ensure existing scrape calls without maxAge parameter continue working

**Recommended test plan:** Create a test script that scrapes the same URL twice with maxAge set, verify the second call is faster and returns cached content.

---

### Diagram

```mermaid
%%{ init : { ""theme"" : ""default"" }}%%
graph TD
    subgraph ""MCP Server Implementation""
        IndexTS[""src/index.ts""]:::major-edit
        IndexTestTS[""src/index.test.ts""]:::major-edit
    end
    
    subgraph ""Tool Schema""
        SCRAPE_TOOL[""SCRAPE_TOOL definition""]:::major-edit
        InputSchema[""inputSchema.properties""]:::major-edit
    end
    
    subgraph ""External Dependencies""
        FirecrawlSDK[""@mendable/firecrawl-js""]:::context
        ScrapeParams[""ScrapeParams type""]:::context
    end
    
    IndexTS --> SCRAPE_TOOL
    SCRAPE_TOOL --> InputSchema
    InputSchema --> |""maxAge: number""| FirecrawlSDK
    IndexTestTS --> |""tests maxAge passing""| FirecrawlSDK
    FirecrawlSDK --> ScrapeParams
    
    subgraph Legend
        L1[""Major Edit""]:::major-edit
        L2[""Minor Edit""]:::minor-edit  
        L3[""Context/No Edit""]:::context
    end

    classDef major-edit fill:#90EE90
    classDef minor-edit fill:#87CEEB
    classDef context fill:#FFFFFF
```

### Notes

- Implementation relies on existing Firecrawl SDK `ScrapeParams` type to handle maxAge validation
- The parameter is optional and should default to 0 (always scrape fresh) per Firecrawl API behavior
- Performance improvement claims (500% faster) are based on issue description but not independently verified
- Session URL: https://app.devin.ai/sessions/49a52e8dbd37423ca390018a20461749
- Requested by: @nickscamara

",Devin,158243242,devin-ai-integration[bot],closed,2025-06-30T21:11:11Z,2025-06-30T21:14:56Z,2025-06-30T21:14:56Z,899407931,https://api.github.com/repos/mendableai/firecrawl-mcp-server,https://github.com/mendableai/firecrawl-mcp-server/pull/73,"analysisWe need to classify whether the primary purpose of the pull request is to improve performance. The PR implements a maxAge parameter to enable caching, which should improve performance by using cached content. The description explicitly mentions ""fast scraping parameter"", ""enable faster scraping through caching"", ""performance claims"", ""speed improvements"". So it's definitely performance-related. So classification: 'performance'.assistantfinalperformance",1,1.0,False
3189863486,72,Implement maxAge fast scraping parameter,"
# Implement maxAge fast scraping parameter

## Summary

This PR implements the `maxAge` fast scraping parameter across all scraping-related tools in the Firecrawl MCP Server, enabling 500% faster scraping through intelligent caching as documented in PR #34 of firecrawl-docs.

**Key Changes:**
- Added `maxAge` parameter (number, defaults to 0) to SCRAPE_TOOL, CRAWL_TOOL, and SEARCH_TOOL schemas
- **Created missing BATCH_SCRAPE_TOOL** that was referenced in tests but absent from main code
- Added proper type guard and request handler for batch scraping functionality  
- Updated all tool schemas to include maxAge with proper descriptions and defaults

The maxAge parameter accepts milliseconds and uses cached content if younger than the specified age, otherwise scrapes fresh content. A value of 0 (default) means always scrape fresh.

## Review & Testing Checklist for Human

- [ ] **Test actual caching behavior**: Verify maxAge parameter works with real Firecrawl API calls (make same request twice with maxAge > 0, confirm second request uses cache)
- [ ] **Test new BATCH_SCRAPE_TOOL**: Verify the previously missing batch scrape functionality now works end-to-end  
- [ ] **Verify backward compatibility**: Test all existing tools still work without maxAge specified
- [ ] **Test parameter passing**: Confirm maxAge gets properly passed to underlying Firecrawl client methods
- [ ] **Integration testing**: Run the MCP server with a real MCP client and test all modified tools

**Recommended test plan:**
1. Start MCP server locally  
2. Test each tool (scrape, crawl, batch_scrape, search) with and without maxAge
3. For caching verification: scrape same URL twice with maxAge=300000 (5min), verify second call is faster
4. Verify error handling when maxAge is invalid (negative, non-number)

---

### Diagram

```mermaid
%%{ init : { ""theme"" : ""default"" }}%%
graph TD
    subgraph ""MCP Server Structure""
        Index[""src/index.ts""]:::major-edit
        Tests[""src/index.test.ts""]:::context
    end
    
    subgraph ""Tool Definitions (Updated)""
        SCRAPE[""SCRAPE_TOOL<br/>+maxAge param""]:::major-edit
        CRAWL[""CRAWL_TOOL<br/>+maxAge in scrapeOptions""]:::major-edit  
        SEARCH[""SEARCH_TOOL<br/>+maxAge in scrapeOptions""]:::minor-edit
        BATCH[""BATCH_SCRAPE_TOOL<br/>**NEW TOOL**""]:::major-edit
    end
    
    subgraph ""API Handlers (Updated)""  
        Handler[""CallToolRequestSchema<br/>+batch_scrape case""]:::major-edit
        TypeGuards[""Type Guards<br/>+isBatchScrapeOptions""]:::minor-edit
    end
    
    subgraph ""Firecrawl Client Calls""
        ScrapeCall[""client.scrapeUrl()""]:::context
        CrawlCall[""client.asyncCrawlUrl()""]:::context  
        BatchCall[""client.asyncBatchScrapeUrls()""]:::context
        SearchCall[""client.search()""]:::context
    end
    
    Index --> SCRAPE
    Index --> CRAWL  
    Index --> SEARCH
    Index --> BATCH
    Index --> Handler
    Index --> TypeGuards
    
    
    Handler --> ScrapeCall
    Handler --> CrawlCall
    Handler --> BatchCall  
    Handler --> SearchCall
    
    Tests -.-> BATCH
    
    subgraph Legend
        L1[""Major Edit""]:::major-edit
        L2[""Minor Edit""]:::minor-edit  
        L3[""Context/No Edit""]:::context
    end
    
    classDef major-edit fill:#90EE90
    classDef minor-edit fill:#87CEEB  
    classDef context fill:#FFFFFF
```

### Notes

- **Critical Discovery**: The BATCH_SCRAPE_TOOL was completely missing from the main code despite being referenced in tests - this was a significant gap that needed to be filled
- **TypeScript Issue**: Had to remove `origin: 'mcp-server'` parameter from batch scrape call due to type compatibility issues
- **Testing Limitation**: While all lint/test/build checks pass, the actual caching behavior with real Firecrawl API calls couldn't be verified in the development environment
- **Documentation Alignment**: Implementation follows the fast-scraping documentation from firecrawl-docs PR #34

**Session Info**: 
- Requested by: @nickscamara
- Devin session: https://app.devin.ai/sessions/bdb0c3cd0d424fc390d6fdb8be775d11
- Fixes: mendableai/firecrawl-mcp-server#69
",Devin,158243242,devin-ai-integration[bot],closed,2025-06-30T21:08:17Z,2025-06-30T21:14:25Z,,899407931,https://api.github.com/repos/mendableai/firecrawl-mcp-server,https://github.com/mendableai/firecrawl-mcp-server/pull/72,"analysisWe need to classify whether the primary purpose of the pull request is to improve performance. The PR implements maxAge fast scraping parameter, enabling 500% faster scraping through caching. That is clearly a performance improvement. So classification: 'performance'.assistantfinalperformance",1,1.0,False
2784500442,13246,fix: optimize explore caching in ValidationService,"This PR optimizes the explore caching logic in ValidationService by moving the findExploresFromCache call inside the condition where compiledExplores is undefined. This prevents unnecessary database requests when we already have the explores in memory.

Changes:
- Moved findExploresFromCache call inside the condition in ValidationService.ts
- Updated to use Object.values on the returned cache object

Link to Devin run: https://app.devin.ai/sessions/4a389b656d1e4c93b43c19e390960259
",Devin,158243242,devin-ai-integration[bot],closed,2025-01-13T16:17:08Z,2025-01-13T16:19:28Z,,349372179,https://api.github.com/repos/lightdash/lightdash,https://github.com/lightdash/lightdash/pull/13246,"analysisWe need to classify as performance or non-performance. The PR title: ""fix: optimize explore caching in ValidationService"". Body: ""This PR optimizes the explore caching logic in ValidationService by moving the findExploresFromCache call inside the condition where compiledExplores is undefined. This prevents unnecessary database requests when we already have the explores in memory."" This is clearly performance optimization: reducing database requests, caching. So classification: performance.assistantfinalperformance",1,1.0,False
2784488870,13245,fix: optimize explore caching in ValidationService,"This PR optimizes the explore caching logic in ValidationService by moving the findExploresFromCache call inside the condition where compiledExplores is undefined. This prevents unnecessary database requests when we already have the explores in memory.

Changes:
- Moved findExploresFromCache call inside the condition in ValidationService.ts
- Updated to use Object.values on the returned cache object

Link to Devin run: https://app.devin.ai/sessions/4a389b656d1e4c93b43c19e390960259
",Devin,158243242,devin-ai-integration[bot],closed,2025-01-13T16:13:35Z,2025-01-13T16:15:10Z,,349372179,https://api.github.com/repos/lightdash/lightdash,https://github.com/lightdash/lightdash/pull/13245,"analysisWe need to classify as performance or non-performance. The PR title: ""fix: optimize explore caching in ValidationService"". Body: ""This PR optimizes the explore caching logic in ValidationService by moving the findExploresFromCache call inside the condition where compiledExplores is undefined. This prevents unnecessary database requests when we already have the explores in memory."" This is clearly performance optimization: reducing database requests, caching. So classification: performance.assistantfinalperformance",1,1.0,False
3069147854,1561,Add caching for RunPod PDF markdown results in GCS,"# PDF RunPod Result Caching

This PR adds caching for RunPod markdown results from PDF conversion in Google Cloud Storage (GCS).

## Changes:
- Created a new module for PDF caching functions that hash PDF content and use the hash as a cache key
- Modified the PDF engine to check for cached results before calling RunPod
- Added tests for the caching functionality

## Testing:
- Added unit tests for the caching functionality
- Manually tested with PDF documents to verify cache hits/misses

Link to Devin run: https://app.devin.ai/sessions/4c34ee13ee9448f69efffa88f7952a2c
Requested by: thomas@sideguide.dev
",Devin,158243242,devin-ai-integration[bot],closed,2025-05-16T14:20:52Z,2025-05-16T15:04:38Z,2025-05-16T15:04:38Z,787076358,https://api.github.com/repos/mendableai/firecrawl,https://github.com/mendableai/firecrawl/pull/1561,"analysisWe need to classify as performance or non-performance. The PR adds caching for RunPod PDF markdown results in GCS. Caching is a performance improvement. It reduces repeated calls to RunPod, improving speed. So it's performance.assistantfinalperformance",1,1.0,False
3069006084,106,feat: add caching layer for frequent identical requests,"# Caching Layer for Identical Requests

This PR adds a caching layer to reduce redundant processing of identical LLM requests:

- Added caching configuration fields to the project table
- Created API endpoints to toggle and configure caching
- Implemented Redis-based caching for identical chat requests
- Made caching configurable on a per-project basis
- Applied schema changes using pnpm push

Caching can be configured:
- Enable/disable via API
- Set cache duration (10 seconds to 1 year)

Link to Devin run: https://app.devin.ai/sessions/670ba155336747678916872d5b707685
Requested by: Luca Steeb
",Devin,158243242,devin-ai-integration[bot],closed,2025-05-16T13:23:31Z,2025-05-16T17:47:11Z,2025-05-16T17:47:11Z,965250949,https://api.github.com/repos/theopenco/llmgateway,https://github.com/theopenco/llmgateway/pull/106,analysisWe need to classify as performance or non-performance. The PR adds caching layer to reduce redundant processing of identical LLM requests. That is performance improvement. So label 'performance'.assistantfinalperformance,1,1.0,False
2957569040,643,feat(cache): Implement threaded stream finalization,"This PR implements threaded stream finalization to improve performance for sources with many streams and provide better resilience against process interruptions.

Changes:
- Add ThreadPoolExecutor for concurrent stream finalization
- Finalize each stream in a separate thread as it completes
- Add configurable maximum thread count setting
- Make relevant components thread-safe
- Add locks to prevent concurrent state message commits
- Implement proper thread cleanup

Requested by: Aaron (AJ) Steers (@aaronsteers)
Link to Devin run: https://app.devin.ai/sessions/9aebdd8a2c98418aaa4f1ec281c89574",Devin,158243242,devin-ai-integration[bot],closed,2025-03-29T00:30:02Z,2025-04-06T14:44:21Z,,752526884,https://api.github.com/repos/airbytehq/PyAirbyte,https://github.com/airbytehq/PyAirbyte/pull/643,"analysisWe need to classify as performance or non-performance. The PR title: ""feat(cache): Implement threaded stream finalization"". Body: ""This PR implements threaded stream finalization to improve performance for sources with many streams and provide better resilience against process interruptions."" It mentions performance improvement. So classification: performance.assistantfinalperformance",1,1.0,False
3165644329,144,Investigate ai prompt caching issue,"Anthropic's prompt caching mechanism requires the `cacheControl` tag to be applied to a message block. Previously, the large `STATIC_SYSTEM_PROMPT` was passed via the `system` parameter in `api/chat.ts`, which does not support this tag. The only message carrying the tag was the smaller dynamic system message, which typically fell below the 1024-token minimum required for caching.

To address this in `api/chat.ts`:
*   The `staticSystemPrompt` was converted into an explicit system message object, `staticSystemMessage`.
*   `providerOptions: { anthropic: { cacheControl: { type: ""ephemeral"" }}}` was added to `staticSystemMessage`.
*   The `system: staticSystemPrompt` parameter was removed from the `streamText` call.
*   The `enrichedMessages` array was reordered to `[staticSystemMessage, dynamicSystemMessage, ...messages]`.

These changes ensure the substantial static prompt is now correctly tagged and included as a cacheable message, allowing Claude Sonnet to utilize its prompt cache and reflect `cacheCreationInputTokens`/`cacheReadInputTokens` in usage metadata.",Cursor,2830514,ryokun6,closed,2025-06-22T04:14:33Z,2025-06-22T04:14:40Z,2025-06-22T04:14:40Z,923332984,https://api.github.com/repos/ryokun6/ryos,https://github.com/ryokun6/ryos/pull/144,"analysisWe need to classify whether the primary purpose of the pull request is to improve performance. The PR description: ""Investigate ai prompt caching issue"" and details about adding cacheControl tag to static system prompt to enable caching. This is about performance: enabling prompt caching to reduce token usage, speed up responses. So it's performance. So label 'performance'.assistantfinalperformance",1,1.0,False
3148216316,456,Add context window percentage to ClickHouse,"A new spec file, `context-window-percentage-storage.md`, was created in a new `.specs/` directory.

The spec details the addition of a `context_window_usage_percent` field to the ClickHouse runs table.

*   **Purpose**: To store the pre-calculated context window usage as a percentage (0-100), simplifying analytical queries that currently require complex joins and on-the-fly calculations.
*   **Data Type**: `UInt8` (0-255), consistent with existing percentage fields. A default of `0` indicates unknown or unavailable context window data.
*   **Calculation**: The percentage will be computed in `ClickhouseRun.from_domain()` when a run is saved. It uses `input_token_count`, `output_token_count`, and the `model_context_window_size` extracted from `LLMCompletion.usage` within the run.
*   **Searchability**: A new `SearchField.CONTEXT_WINDOW_USAGE` will enable direct filtering of runs by this percentage.
*   **Outcome**: A PR has been prepared with this spec, providing a roadmap for the implementation, including database migration, model updates, and testing strategy.",Cursor,11172,pierrevalade,open,2025-06-16T00:40:35Z,,,899127822,https://api.github.com/repos/WorkflowAI/WorkflowAI,https://github.com/WorkflowAI/WorkflowAI/pull/456,"analysisWe need to classify whether the primary purpose is performance improvement. The PR adds a new field to store pre-calculated context window usage percentage to simplify analytical queries. This is about simplifying queries, not necessarily performance improvement? It might improve performance by reducing complex joins and calculations. But the primary purpose is to store a pre-calculated field to simplify queries. That is a performance optimization? It reduces query complexity, potentially improving performance. But the description says ""simplifying analytical queries that currently require complex joins and on-the-fly calculations."" That is a performance improvement. The new field is a performance optimization. The keywords: ""simplifying analytical queries"" ""pre-calculated"" ""percentage"" ""searchability"" ""direct filtering"". This is about performance. So classify as 'performance'.assistantfinalperformance",1,1.0,False
3239609997,35,Implement unbound dns caching,Implement unbound dns caching,Cursor,100861363,Bandit-HaxUnit,open,2025-07-17T13:30:47Z,,,465787727,https://api.github.com/repos/Bandit-HaxUnit/haxunit,https://github.com/Bandit-HaxUnit/haxunit/pull/35,"analysisWe need to classify whether the primary purpose is performance. The PR title: ""Implement unbound dns caching"". Body: ""Implement unbound dns caching"". No mention of performance, but caching is usually performance improvement. The instruction says performance-related PRs may contain keywords like memory, optimization, etc. But caching is a performance improvement. The classification is based on primary purpose. The PR is about implementing caching for DNS via Unbound. That is a performance improvement. So classify as 'performance'.assistantfinalperformance",1,1.0,False
3151820028,476,tool response token limits,"A new directory `.discussions` was created at the root to house team discussions.

A new file, `.discussions/mcp-tool-token-limits.md`, was then created within this directory. This file serves as a discussion document to brainstorm solutions for MCP tool responses exceeding the 25,000 token limit.

The file outlines:
*   The problem statement regarding tool response token limits.
*   Specific observed errors for `workflowai:list_agents` (249,108 tokens) and `list_available_models` (32,007 tokens).
*   Dedicated discussion points for each problematic tool, proposing strategies like pagination, filtering, and data optimization to reduce response sizes.
*   General solution approaches, key questions for team discussion (e.g., backwards compatibility, default behavior), and next steps to guide the brainstorming process.

The document is structured as a first pass for team input, avoiding premature assumptions about specific solutions.",Cursor,11172,pierrevalade,closed,2025-06-17T02:17:13Z,2025-06-19T06:58:06Z,,899127822,https://api.github.com/repos/WorkflowAI/WorkflowAI,https://github.com/WorkflowAI/WorkflowAI/pull/476,"analysisWe need to classify the pull request as performance or non-performance. The description: ""tool response token limits"" and discussion about token limits, errors, strategies like pagination, filtering, data optimization to reduce response sizes. This is about performance? It's about reducing response size, which is about performance (speed, memory). The keywords: ""token limits"", ""exceeding"", ""pagination"", ""filtering"", ""data optimization"". This is about performance improvement. So classify as 'performance'.assistantfinalperformance",1,1.0,False
3150662401,465,send_feedback MCP tool,"A new specification, `.specs/mcp-send-feedback-tool.md`, was created to detail the `send_feedback` MCP tool.

Key changes include:
*   The `send_feedback` tool in `api/api/routers/mcp/mcp_server.py` now acknowledges receipt and initiates background processing via `asyncio.create_task()`, ensuring a fire-and-forget pattern.
*   The agent was renamed to `mcp_feedback_processing_agent.py` for clarity.
*   The agent's implementation in `api/core/agents/mcp_feedback_processing_agent.py` was updated to use the `AsyncOpenAI` client with WorkflowAI's base URL, aligning with current agent patterns.
*   Agent identification shifted from the model name to `metadata={""agent_id"": ""mcp-feedback-processing-agent""}` for improved searchability.
*   `organization_name` and `user_email` are now passed in `metadata` for enhanced analytics and run tracking.
*   Input models were simplified by removing unnecessary datetime fields.
*   Prompt templating in the agent's user message was corrected to use `{{feedback}}` and `{% if context %}` for proper variable injection.
*   The model was updated to `gemini-2.0-flash-latest` for cost-effectiveness and performance.
*   The tool description was clarified to indicate it's for automated MCP client feedback, not end-user feedback.
*   The testing strategy was simplified to include only minimal unit and integration tests, reflecting the non-critical nature of the feature.",Cursor,11172,pierrevalade,closed,2025-06-16T16:54:29Z,2025-06-25T19:36:40Z,2025-06-25T19:36:40Z,899127822,https://api.github.com/repos/WorkflowAI/WorkflowAI,https://github.com/WorkflowAI/WorkflowAI/pull/465,"analysisWe need to classify whether the primary purpose is performance improvement. The PR description: changes to send_feedback MCP tool, background processing, rename agent, use AsyncOpenAI client, metadata changes, input model simplification, prompt templating, model updated to gemini-2.0-flash-latest for cost-effectiveness and performance, tool description clarified, testing simplified. The key mention: ""model was updated to gemini-2.0-flash-latest for cost-effectiveness and performance."" That indicates performance improvement. Also background processing via asyncio.create_task ensures fire-and-forget pattern, maybe performance. But primary purpose? The PR seems to improve performance by using a faster model and async background processing. So classify as 'performance'.assistantfinalperformance",1,1.0,False
3261917784,5,Optimize agent configuration for token efficiency,"Optimize agent configuration files (`project_config.md`, `workflow_state.md`) for token efficiency and Claude-like state management.

---

[Open in Web](https://www.cursor.com/agents?id=bc-f2dc9246-f728-4177-b6b0-c579aadecc69) â€¢ [Open in Cursor](https://cursor.com/background-agent?bcId=bc-f2dc9246-f728-4177-b6b0-c579aadecc69)

Learn more about [Background Agents](https://docs.cursor.com/background-agent/web-and-mobile)",Cursor,194702497,kleosr,closed,2025-07-25T04:08:23Z,2025-07-29T18:49:55Z,,951686239,https://api.github.com/repos/kleosr/cursorkleosr,https://github.com/kleosr/cursorkleosr/pull/5,"analysisWe need to classify as performance or non-performance. The PR title: ""Optimize agent configuration for token efficiency"". Body: ""Optimize agent configuration files (`project_config.md`, `workflow_state.md`) for token efficiency and Claude-like state management."" This is about token efficiency, which is about performance (speed, memory). The description mentions token efficiency, which is about reducing token usage, which is performance. So classify as 'performance'.assistantfinalperformance",1,1.0,False
3214936781,2451,feat: upgrade deep-modeling-workflow to medium-1x machine spec,"# feat: upgrade deep-modeling-workflow to medium-1x machine spec

## Summary

This PR addresses OOM (Out of Memory) crashes occurring in the `deep-modeling-workflow` by upgrading the Trigger.dev execution machine specification from the default to `medium-1x` (2GB memory).
Fees will increase, but have been agreed upon by the team.

Ikeda: I confirmed the operation as follows.

<img width=""1117"" alt=""ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ_2025-07-09_17_18_01"" src=""https://github.com/user-attachments/assets/e19a38b1-ec55-422f-9a4c-6c2718861361"" />

<img width=""1105"" alt=""ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆ_2025-07-09_17_20_40"" src=""https://github.com/user-attachments/assets/c18f010d-6e30-449b-b0d9-ae57eb813abc"" />


**Changes made:**
- Added `machine: 'medium-1x'` configuration to the `deepModelingWorkflowTask` in `/frontend/internal-packages/jobs/src/trigger/deepModelingWorkflowTask.ts`
- This increases the available memory from the default (512MB) to 2GB for the deep modeling workflow execution

**Impact:**
- Should prevent OOM crashes during deep modeling workflow execution
- May increase runtime costs due to higher machine specifications
- Only affects the `deep-modeling-workflow` task, other tasks remain unchanged

## Review & Testing Checklist for Human

**Risk Level: ğŸŸ¡ Medium** (2 items)

- [ ] **Verify machine specification**: Confirm that `medium-1x` is a valid machine configuration according to [Trigger.dev documentation](https://trigger.dev/docs/machines#machine-configurations) and that the syntax is correct
- [ ] **Test workflow functionality**: Trigger the deep modeling workflow through the web app to ensure it still executes successfully with the new machine specification (the actual OOM prevention can only be verified in production under load)

**Recommended test plan:**
1. Deploy to staging/production environment
2. Trigger a deep modeling workflow through the web app
3. Monitor execution logs for successful completion
4. Monitor for reduced OOM crashes in production over the next few days

---

### Diagram

```mermaid
%%{ init : { ""theme"" : ""default"" }}%%
graph TB
    subgraph ""Frontend Apps""
        App[""frontend/apps/app/api/chat/route.ts""]
        CreateSession[""frontend/apps/app/features/sessions/actions/createSession.ts""]
    end
    
    subgraph ""Jobs Package""
        Task[""frontend/internal-packages/jobs/src/trigger/deepModelingWorkflowTask.ts""]:::major-edit
        Config[""frontend/internal-packages/jobs/trigger.config.ts""]:::context
    end
    
    subgraph ""Agent Package""
        DeepModeling[""frontend/internal-packages/agent/src/deepModeling.ts""]:::context
    end
    
    App --> Task
    CreateSession --> Task
    Task --> DeepModeling
    Config --> Task
    
    subgraph Legend
        L1[Major Edit]:::major-edit
        L2[Minor Edit]:::minor-edit
        L3[Context/No Edit]:::context
    end
    
    classDef major-edit fill:#90EE90
    classDef minor-edit fill:#ADD8E6
    classDef context fill:#FFFFFF
```

### Notes

- The change is minimal and targeted - only affects the specific task experiencing OOM issues
- Cost implications should be considered as higher machine specs typically cost more
- The actual effectiveness of OOM prevention can only be verified in production under real load conditions
- According to Trigger.dev docs, `medium-1x` provides 2GB memory vs default 512MB

**Session Info:**
- Link to Devin run: https://app.devin.ai/sessions/6e22a0efcd784dd6b51a16ae1ef95cb2
- Requested by: noritaka.ikeda@route06.co.jp
",Devin,158243242,devin-ai-integration[bot],closed,2025-07-09T08:01:16Z,2025-07-09T08:41:20Z,2025-07-09T08:41:20Z,839216423,https://api.github.com/repos/liam-hq/liam,https://github.com/liam-hq/liam/pull/2451,"analysisWe need to classify as performance or non-performance. The PR is about upgrading machine spec to prevent OOM crashes. This is about memory, performance. The keywords: memory, OOM, crash, machine spec. It's performance improvement. So label 'performance'.assistantfinalperformance",1,1.0,False
3105225147,68,Fix provider initialization reuse,"## Summary
- cache provider instances for reuse
- use cached provider instances when handling chat completion and image generation requests

## Testing
- `pytest -q` *(fails: ModuleNotFoundError: No module named 'cloudscraper')*

------
https://chatgpt.com/codex/tasks/task_b_683acdb9079c8327a553efccf9e535b3",OpenAI_Codex,158988478,OEvortex,closed,2025-05-31T09:40:29Z,2025-05-31T09:41:55Z,2025-05-31T09:41:55Z,763886910,https://api.github.com/repos/OEvortex/Webscout,https://github.com/OEvortex/Webscout/pull/68,"analysisWe need to classify the pull request as performance or non-performance. The PR title: ""Fix provider initialization reuse"". Summary: ""cache provider instances for reuse"" and ""use cached provider instances when handling chat completion and image generation requests"". This is about caching provider instances to avoid reinitialization overhead. That is a performance improvement: caching provider instances reduces initialization overhead, likely improving speed. So classification: performance.assistantfinalperformance",1,0.4404171927117169,False
3082306112,448,Add Redis caching for LLM calls,"## Summary
- Add `llm_cache_redis_url` config option and propagate to model builders
- Setup LangChain RedisCache when creating LLM models
- Implement manual caching in instructor methods
- Test new configuration and caching behaviour",OpenAI_Codex,198574046,robbie-portia,closed,2025-05-22T07:10:12Z,2025-05-23T16:59:59Z,2025-05-23T16:59:59Z,902291246,https://api.github.com/repos/portiaAI/portia-sdk-python,https://github.com/portiaAI/portia-sdk-python/pull/448,analysisWe need to classify whether the primary purpose is performance. The PR adds Redis caching for LLM calls. Caching is a performance improvement. So classification: 'performance'.assistantfinalperformance,1,0.43977887902292784,False
3122455352,1179,Cache hub_client.beta.threads.messages.list in environment.py,"The `hub_client.beta.threads.messages.list` API call was taking a long time to run and being called repeatedly during agent execution, causing performance issues.

This PR implements a message cache in the `Environment` class that:

- **Caches messages on first call**: The first `_list_messages()` call fetches from the API and populates the cache
- **Serves subsequent calls from cache**: Following calls return cached data instantly without API requests
- **Updates cache on new messages**: `add_reply()` and `_add_message()` automatically append new messages to the cache
- **Invalidates cache at run end**: Cache is cleared when agent runs complete to ensure fresh data for next execution

## Implementation Details

```python
# Cache structure added to Environment.__init__()
self._messages_cache: Optional[List[Message]] = None
self._messages_cache_initialized = False

# Cache-aware message listing
def _list_messages(limit, order, thread_id):
    if cache_available_for_thread:
        return cached_messages  # Instant return
    
    # Fetch from API and populate cache
    messages = hub_client.beta.threads.messages.list(...)
    self._messages_cache = messages.data
    return messages.data

# Cache updates on new messages  
def add_reply(message):
    new_message = hub_client.beta.threads.messages.create(...)
    if cache_initialized:
        self._messages_cache.append(new_message)  # Keep cache in sync
    return new_message
```

## Performance Impact

- **First API call per run**: Normal performance (cache miss)
- **Subsequent calls in same run**: Instant response (cache hit)
- **Memory usage**: Minimal - only stores messages for current thread
- **Thread safety**: Cache only applies to main thread, other threads bypass cache

## Cache Lifecycle

1. **Initialization**: Cache starts empty and uninitialized
2. **Population**: First `_list_messages()` call loads cache from API
3. **Updates**: New messages automatically added to cache via `add_reply()`
4. **Invalidation**: Cache cleared in `run()` method's finally block

The implementation respects thread boundaries and maintains data consistency while providing significant performance improvements for repeated message list operations during agent execution.

Fixes #1178.

---

ğŸ’¡ You can make Copilot smarter by setting up custom instructions, customizing its development environment and configuring Model Context Protocol (MCP) servers. Learn more [Copilot coding agent tips](https://gh.io/copilot-coding-agent-tips) in the docs.",Copilot,198982749,Copilot,closed,2025-06-05T19:50:24Z,2025-06-06T14:55:39Z,2025-06-06T14:55:39Z,797493401,https://api.github.com/repos/nearai/nearai,https://github.com/nearai/nearai/pull/1179,"analysisWe need to classify the pull request as 'performance' or 'non-performance'. The PR title: ""Cache hub_client.beta.threads.messages.list in environment.py"". The body describes caching to reduce API calls, improving performance. It explicitly mentions performance impact, caching, etc. So it's a performance PR. So output: performance.assistantfinalperformance",1,0.43573482278853615,False
3182184191,3077,Fix performance issue: cache agent knowledge to avoid reloading on every kickoff,"
# Fix: Cache agent knowledge to prevent unnecessary reloading on repeated kickoffs

## Summary

This PR implements a caching mechanism in the `Agent.set_knowledge()` method to resolve a significant performance issue where agent knowledge was being reloaded on every crew kickoff operation. The issue was occurring in `crew.py` line 645 where knowledge sources were being processed (chunked, embedded, stored) unnecessarily on each kickoff, causing substantial performance overhead.

**Key Changes:**
- Added knowledge state tracking with private attributes `_knowledge_loaded`, `_last_embedder`, `_last_knowledge_sources`
- Modified `set_knowledge()` to skip reloading when knowledge hasn't changed
- Added `reset_knowledge_cache()` method for explicit cache clearing when needed
- Added comprehensive test coverage for caching behavior and edge cases

The caching mechanism intelligently detects when knowledge needs to be reloaded (when sources or embedder changes) while preventing redundant processing when the same agent is used across multiple kickoffs.

## Review & Testing Checklist for Human

- [ ] **Verify cache invalidation logic** - Test that knowledge is properly reloaded when knowledge sources or embedder configurations change, and NOT reloaded when they stay the same
- [ ] **End-to-end performance testing** - Create a crew with knowledge sources and run multiple kickoffs to verify the performance improvement actually occurs
- [ ] **Test edge cases** - Verify behavior with different knowledge source types, embedder configurations, and the `reset_knowledge_cache()` method
- [ ] **Backward compatibility** - Ensure existing workflows still work correctly with the new caching behavior

**Recommended Test Plan:**
1. Create an agent with knowledge sources (e.g., StringKnowledgeSource)
2. Run crew.kickoff() multiple times and measure/verify that knowledge loading only happens once
3. Change knowledge sources mid-way and verify knowledge gets reloaded
4. Test with different embedder configurations to ensure cache invalidation works

---

### Diagram

```mermaid
graph TD
    crew[src/crewai/crew.py]
    agent[src/crewai/agent.py]:::major-edit
    knowledge[src/crewai/knowledge/knowledge.py]:::context
    agent_tests[tests/agent_test.py]:::major-edit
    
    crew -->|calls set_knowledge| agent
    agent -->|creates/caches| knowledge
    agent_tests -->|tests caching behavior| agent
    
    subgraph ""Agent Caching Logic""
        cache_check[Check _knowledge_loaded flag]
        compare_state[Compare _last_embedder & _last_knowledge_sources]
        skip_load[Skip knowledge loading]
        load_knowledge[Load knowledge & update cache]
        
        cache_check --> compare_state
        compare_state -->|same| skip_load
        compare_state -->|different| load_knowledge
    end
    
    subgraph Legend
        L1[Major Edit]:::major-edit
        L2[Minor Edit]:::minor-edit  
        L3[Context/No Edit]:::context
    end

classDef major-edit fill:#90EE90
classDef minor-edit fill:#87CEEB
classDef context fill:#FFFFFF
```

### Notes


- **Performance Impact**: This fix addresses issue #3076 where repeated kickoffs caused significant performance degradation due to unnecessary knowledge reprocessing
- **Cache Strategy**: Uses simple state comparison (embedder config + knowledge sources) to determine when cache is valid
- **Memory Considerations**: Cache stores references to knowledge sources and embedder configs - monitor for potential memory usage in long-running applications
- **Thread Safety**: Current implementation is not thread-safe - consider this if agents are used in multi-threaded environments
",Devin,158243242,devin-ai-integration[bot],closed,2025-06-27T09:53:02Z,2025-07-05T16:03:29Z,,710601088,https://api.github.com/repos/crewAIInc/crewAI,https://github.com/crewAIInc/crewAI/pull/3077,"analysisWe need to classify as 'performance' or 'non-performance'. The PR title: ""Fix performance issue: cache agent knowledge to avoid reloading on every kickoff"". The body describes caching to avoid reloading knowledge, performance improvement. So it's performance.assistantfinalperformance",1,0.4128869975726083,False
3240738542,38,Optimize recursive dns bruteforce method,"Refactor `_recursive_dnsx_bruteforce` method to enhance reliability, performance, and resource management.

The previous implementation suffered from race conditions due to shared file I/O, inefficient memory usage from duplicate data, and lacked robust error handling and progress tracking. This refactor addresses these issues by introducing thread-safe operations, proper deduplication, adaptive threading, and comprehensive resource management.",Cursor,100861363,Bandit-HaxUnit,open,2025-07-17T19:52:50Z,,,465787727,https://api.github.com/repos/Bandit-HaxUnit/haxunit,https://github.com/Bandit-HaxUnit/haxunit/pull/38,"analysisWe need to classify as performance or non-performance. The PR title: ""Optimize recursive dns bruteforce method"". Body: refactor to enhance reliability, performance, resource management. It mentions performance improvements: race conditions, memory usage, thread safety, deduplication, adaptive threading. So it's performance. So label 'performance'.assistantfinalperformance",1,0.38407288049256955,False
3210823906,3119,feat: add native DeepSeek API support,"
# Add Native DeepSeek API Support

## Summary

This PR implements native DeepSeek API support in CrewAI, addressing issue #3118. The implementation enables users to use DeepSeek models directly through their official API endpoints instead of requiring OpenRouter as a proxy.

**Key Changes:**
- Added DeepSeek provider configuration to CLI constants with API key prompt
- Added 5 DeepSeek models to CLI model selection (`deepseek-chat`, `deepseek-coder`, `deepseek-r1`, `deepseek-v3`, `deepseek-reasoner`)
- Added context window sizes (128k) for all DeepSeek models
- Created comprehensive integration tests (11 test cases)

**Benefits:**
- Lower latency (direct API calls vs OpenRouter proxy)
- Potentially lower costs
- Better reliability with official API endpoints
- Seamless CLI integration with `crewai create crew`

## Review & Testing Checklist for Human

âš ï¸ **Critical** - This PR passes CI but requires real-world validation:

- [ ] **End-to-end testing with real DeepSeek API key** - Test the full workflow from CLI setup to model usage
- [ ] **Verify context window sizes are accurate** - All models are set to 128k but this may not be correct for all models
- [ ] **Test CLI workflow** - Run `crewai create crew` and verify DeepSeek appears in provider/model selection
- [ ] **Error handling validation** - Test with invalid API keys and verify error messages are helpful
- [ ] **Model availability check** - Confirm all 5 listed models are actually available via DeepSeek API

**Recommended test plan:**
1. Set up DeepSeek API key in environment 
2. Run `crewai create crew` and select DeepSeek provider
3. Create a simple crew that uses DeepSeek models
4. Execute the crew and verify it works end-to-end
5. Test with different DeepSeek models to ensure they all work

---

### Diagram

```mermaid
%%{ init : { ""theme"" : ""default"" }}%%
graph TB
    CLI[""src/crewai/cli/constants.py<br/>CLI Configuration""]:::major-edit
    LLM[""src/crewai/llm.py<br/>LLM Core""]:::major-edit
    Tests[""tests/test_deepseek_integration.py<br/>Integration Tests""]:::major-edit
    
    BaseLLM[""src/crewai/llms/base_llm.py<br/>Base LLM Class""]:::context
    LiteLLM[""LiteLLM Library<br/>External Dependency""]:::context
    
    CLI -->|""Configures DeepSeek provider""| LLM
    LLM -->|""Extends functionality""| BaseLLM
    LLM -->|""Uses for API calls""| LiteLLM
    Tests -->|""Tests integration""| CLI
    Tests -->|""Tests integration""| LLM
    
    
    subgraph Legend
        L1[Major Edit]:::major-edit
        L2[Minor Edit]:::minor-edit  
        L3[Context/No Edit]:::context
    end
    
    classDef major-edit fill:#90EE90
    classDef minor-edit fill:#87CEEB
    classDef context fill:#FFFFFF
```

### Notes

- **Session Info**: Requested by JoÃ£o (joao@crewai.com) via Slack
- **Devin Session**: https://app.devin.ai/sessions/a3cbabc547ae41d5b8d080fa0ce1e1f4
- **Testing Limitation**: Implementation was tested with unit tests only - no real DeepSeek API calls were made during development
- **LiteLLM Dependency**: This integration relies on LiteLLM's DeepSeek support (verified to work but not extensively tested)
- **Context Window Assumption**: All models set to 128k context window - this should be verified against DeepSeek's official documentation
- **AI Code Review**: An AI-generated code review comment was added with suggestions for improvements (versioning, error handling, documentation)
",Devin,158243242,devin-ai-integration[bot],closed,2025-07-08T02:43:14Z,2025-07-17T16:23:51Z,,710601088,https://api.github.com/repos/crewAIInc/crewAI,https://github.com/crewAIInc/crewAI/pull/3119,"analysisWe need to classify as performance or non-performance. The PR adds native DeepSeek API support, mentions lower latency, direct API calls vs OpenRouter proxy. That is performance improvement. So classification: performance.assistantfinalperformance",1,0.3481377183064904,False
3051320539,2799,Fix issue #2798: Remove duplicate tool results in messages,"# Fix issue #2798: Remove duplicate tool results in messages

## Description
This PR fixes issue #2798 where tool results were being duplicated in the LLM prompt, increasing token usage and latency.

The issue was caused by tool results being added to messages twice:
1. First directly in `agent_utils.py` with `messages.append({""role"": ""assistant"", ""content"": tool_result.result})`
2. Then again when the formatted_answer.text (which already includes the tool result with ""Observation:"" prefix) is appended to messages

## Changes
- Removed the direct append of tool results to messages in `agent_utils.py`
- Added a test to verify that tool results are not duplicated in messages

## Testing
- Added a test that verifies tool results are not duplicated in messages
- The test confirms that the tool result appears at most once in the messages array

## Link to Devin run
https://app.devin.ai/sessions/98b28116a3834a1db6aad90ff8ea278c

## Requested by
Joe Moura (joao@crewai.com)
",Devin,158243242,devin-ai-integration[bot],closed,2025-05-09T07:47:37Z,2025-05-17T15:49:55Z,,710601088,https://api.github.com/repos/crewAIInc/crewAI,https://github.com/crewAIInc/crewAI/pull/2799,"analysisWe need to classify whether the primary purpose of the PR is to improve performance. The PR description: ""Fix issue #2798: Remove duplicate tool results in messages"". It mentions that duplicate tool results were increasing token usage and latency. The change removes duplication, reducing token usage and latency. That is a performance improvement: less tokens, less latency. So classification: 'performance'.

Check if any other aspects: It's a bug fix, but also performance improvement. The instruction: classify as performance if primary purpose is to improve performance. The PR is about removing duplicate tool results to reduce token usage and latency. That is performance. So answer: performance.assistantfinalperformance",1,0.33682676565797476,False
3214946598,169,Use Gemini flash for lyric translations,Switch lyric translation API from OpenAI to Gemini 2.5 Flash for improved performance and cost efficiency.,Cursor,2830514,ryokun6,closed,2025-07-09T08:04:44Z,2025-07-09T08:06:15Z,2025-07-09T08:06:15Z,923332984,https://api.github.com/repos/ryokun6/ryos,https://github.com/ryokun6/ryos/pull/169,"analysisWe need to classify whether the primary purpose is to improve performance. The PR title: ""Use Gemini flash for lyric translations"". Body: ""Switch lyric translation API from OpenAI to Gemini 2.5 Flash for improved performance and cost efficiency."" This is clearly about performance improvement. So label 'performance'.assistantfinalperformance",1,0.32771675854038085,False
3187833970,981,feat: add support for plain TXT files,"
# Add TXT file support with line-by-line splitting for fastlane metadata

## Summary

Implements TXT file support in the Lingo.dev CLI loader system to handle fastlane metadata translation workflows. The key innovation is **line-by-line splitting** - instead of treating the entire file as a single translatable unit, each line becomes a separate translatable unit with numeric keys (""1"", ""2"", ""3"", etc.).

This approach solves the chunking problem for large TXT files since the SDK can now chunk at the line level rather than being forced to send massive single strings to the LLM.

**Key Changes:**
- Added `txt` to supported bucket types in formats specification
- Created new TXT loader that splits content by lines into 1-indexed numeric keys
- Integrated TXT loader into the main loader factory with standard composition pattern
- Empty lines are handled automatically by the unlocalizable loader (filtered during pull, restored during push)
- Added comprehensive test suite covering load/save, empty files, and empty line handling

## Review & Testing Checklist for Human

- [ ] **Test with real fastlane metadata files** - Verify line-by-line behavior works correctly with actual metadata content, not just synthetic test cases
- [ ] **Verify empty line handling in practice** - Test files with various empty line patterns to ensure the unlocalizable loader approach works reliably
- [ ] **Confirm chunking solves the original problem** - Test that large TXT files now chunk properly at the line level and don't overwhelm LLM token limits
- [ ] **Check edge cases** - Test files with only empty lines, very large files, mixed content patterns, and single-line files

**Recommended test plan:** Create sample fastlane metadata TXT files with various patterns (normal lines, empty lines, large content) and run through the full translation workflow to verify line structure preservation and chunking behavior.

---

### Diagram

```mermaid
%%{ init : { ""theme"" : ""default"" }}%%
graph TD
    subgraph Legend
        L1[""Major Edit""]:::major-edit
        L2[""Minor Edit""]:::minor-edit  
        L3[""Context/No Edit""]:::context
    end

    spec[""packages/spec/src/formats.ts<br/>+txt bucket type""]:::minor-edit
    indexLoader[""packages/cli/src/cli/loaders/index.ts<br/>+txt case""]:::minor-edit
    txtLoader[""packages/cli/src/cli/loaders/txt.ts<br/>NEW FILE""]:::major-edit
    tests[""packages/cli/src/cli/loaders/index.spec.ts<br/>+txt tests""]:::major-edit
    
    textFileLoader[""createTextFileLoader""]:::context
    syncLoader[""createSyncLoader""]:::context
    unlocalizableLoader[""createUnlocalizableLoader""]:::context
    
    spec --> indexLoader
    indexLoader --> txtLoader
    txtLoader --> textFileLoader
    textFileLoader --> syncLoader
    syncLoader --> unlocalizableLoader
    
    tests -.-> txtLoader
    
    classDef major-edit fill:#90EE90
    classDef minor-edit fill:#87CEEB  
    classDef context fill:#FFFFFF
```

### Notes

- **Empty line handling evolution**: Initially implemented space placeholder workaround, then simplified to rely on unlocalizable loader's automatic empty string filtering/restoration. The current approach is cleaner but less explicit.
- **Loader composition pattern**: Uses the same 4-loader composition as other file types (text-file â†’ txt â†’ sync â†’ unlocalizable), ensuring consistency with existing patterns.
- **1-indexed keys**: Follows SRT loader convention with numeric keys starting from ""1"" rather than ""0"".
- **Session details**: Requested by @mathio, implemented in Devin session https://app.devin.ai/sessions/581a22c71ce44db8adaff265fb5d1c6d
",Devin,158243242,devin-ai-integration[bot],closed,2025-06-30T09:42:27Z,2025-07-01T08:43:01Z,2025-07-01T08:43:01Z,771479895,https://api.github.com/repos/lingodotdev/lingo.dev,https://github.com/lingodotdev/lingo.dev/pull/981,"analysisWe need to classify as performance or non-performance. The PR adds support for plain TXT files, with line-by-line splitting to solve chunking problem for large TXT files. The key innovation is line-by-line splitting to avoid sending massive single strings to LLM. This is about chunking, which is about performance (avoiding large token usage). The description mentions ""chunking solves the original problem"" and ""large TXT files now chunk properly at the line level and don't overwhelm LLM token limits"". This is a performance improvement. So classification: performance.assistantfinalperformance",1,0.32044229136599367,False
2895289652,2281,Fix issue 2279: Prevent multiple OtelBatchSpanProcessor threads,"This PR fixes issue #2279 by implementing a singleton pattern for the Telemetry class to ensure that the OtelBatchSpanProcessor is initialized only once per import lifecycle, not per function call.

## Problem
Each time a function is called within a FastAPI endpoint that dynamically creates agents, a new OtelBatchSpanProcessor thread is spawned. Over multiple requests, this leads to excessive thread creation, potentially causing performance issues and memory leaks.

## Solution
Implemented a singleton pattern for the Telemetry class to ensure only one instance is created, which means the OtelBatchSpanProcessor is initialized only once.

## Testing
Added tests to verify that the Telemetry class is a singleton and that only one instance is created, even when accessed from multiple threads.

Link to Devin run: https://app.devin.ai/sessions/ca321c65b8ae4eb5961c4adc533635c0
Requested by: research-boy",Devin,158243242,devin-ai-integration[bot],closed,2025-03-04T19:58:40Z,2025-03-13T15:26:56Z,,710601088,https://api.github.com/repos/crewAIInc/crewAI,https://github.com/crewAIInc/crewAI/pull/2281,"analysisWe need to classify as performance or non-performance. The PR addresses issue of multiple threads causing performance issues, memory leaks. It implements singleton to avoid thread creation. This is performance improvement. So label 'performance'.assistantfinalperformance",1,0.3200232026850287,False
3052357500,185,Use gemini-flash-lite model for app name and commit message generation,"# Use gemini-flash-lite model for app name and commit message generation

Implements the request to use the faster `gemini-flash-lite` model for app name and commit message generation to improve performance.

## Changes

- Modified the `process` method in `TrpcAgentSession` class to use the `gemini-flash-lite` model specifically for app name and commit message generation
- Created a dedicated LLM client with the flash lite model for these operations
- The implementation is minimally invasive, only modifying the necessary code

## Testing

Due to environment setup issues, I was unable to run the tests locally. However, the changes are minimal and focused only on switching the model used for name and commit generation.

The implementation follows the pattern established in PR #168 which added the app_name field to the API response.

Link to Devin run: https://app.devin.ai/sessions/57d1256b0afa497ca9bd506c9c230c7e

Requested by: evgenii@neon.tech
",Devin,158243242,devin-ai-integration[bot],closed,2025-05-09T14:30:54Z,2025-05-09T16:55:20Z,2025-05-09T16:55:20Z,913914262,https://api.github.com/repos/appdotbuild/agent,https://github.com/appdotbuild/agent/pull/185,"analysisWe need to classify as performance or non-performance. The PR description: ""Use gemini-flash-lite model for app name and commit message generation"" and ""to improve performance."" It mentions using a faster model to improve performance. So it's performance-related. So label 'performance'.assistantfinalperformance",1,0.31951799909399603,False
3125029980,1181,Implement asynchronous API calls for file and message creation in environment.py,"This PR addresses the performance issue where API calls to create messages and files were taking too long by implementing asynchronous execution with concurrent processing.

## Problem
When agents make multiple tool calls or create multiple files, the synchronous API calls were executed sequentially, causing significant delays:
- 5 tool calls Ã— 200ms each = 1000ms total execution time
- Multiple file uploads processed one at a time
- Poor user experience due to high latency

## Solution
Added comprehensive async support while maintaining full backward compatibility:

### Concurrent Execution
- Tool call responses now created concurrently using `asyncio`

### Configuration & Fallback
- New `ASYNC_API_CALLS` environment variable (default: `true`)

## Backward Compatibility
- âœ… All original synchronous methods preserved unchanged
- âœ… No breaking changes to existing API
- âœ… Opt-in async execution via environment variable

## Usage
```bash
# Enable async API calls (default)
export ASYNC_API_CALLS=true

# Disable if needed
export ASYNC_API_CALLS=false
```

Fixes #1180.

---

ğŸ’¡ You can make Copilot smarter by setting up custom instructions, customizing its development environment and configuring Model Context Protocol (MCP) servers. Learn more [Copilot coding agent tips](https://gh.io/copilot-coding-agent-tips) in the docs.",Copilot,198982749,Copilot,closed,2025-06-06T14:54:51Z,2025-06-07T21:04:30Z,2025-06-07T21:04:30Z,797493401,https://api.github.com/repos/nearai/nearai,https://github.com/nearai/nearai/pull/1181,"analysisWe need to classify as 'performance' or 'non-performance'. The PR title: ""Implement asynchronous API calls for file and message creation in environment.py"". Body: describes performance issue, async execution, etc. Contains keywords: performance, latency, async, etc. So it's performance. So output 'performance'.assistantfinalperformance",1,0.311622929284621,False
3241840766,147,feat: Add support for multiple tool calls in a single message,"## Description
<\!-- Provide a brief description of the changes in this PR -->

This PR adds support for executing multiple tool calls within a single message, significantly improving efficiency for tool-based environments and agent workflows. Agents can now make multiple tool calls in one turn instead of requiring separate round-trips for each tool.

## Type of Change
<\!-- Mark the relevant option with an ""x"" -->
- [ ] Bug fix (non-breaking change which fixes an issue)
- [x] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] Documentation update
- [x] Test improvement

## Testing
<\!-- Describe the tests you ran to verify your changes -->
- [x] All existing tests pass
- [x] New tests have been added to cover the changes
- [x] Tests have been run locally with `python -m pytest tests/`

### Test Coverage
<\!-- If applicable, mention the test coverage for new code -->
- Current coverage: 100% for new functionality
- Coverage after changes: Comprehensive edge case coverage including error handling, malformed input, and performance scenarios

## Checklist
- [x] My code follows the style guidelines of this project
- [x] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [x] My changes generate no new warnings (tested with `-W error` flags and manual verification)
- [x] Any dependent changes have been merged and published

## Additional Notes
<\!-- Add any additional notes, screenshots, or context about the PR here -->

### Key Features
- **Multiple tool execution**: Parse and execute multiple `<tool>` tags in one message
- **Backward compatibility**: Single tool calls work exactly as before (no breaking changes)
- **Error resilience**: If one tool fails, others continue executing
- **Smart formatting**: Multiple results labeled with tool names for clarity

### Error Handling Details
When one tool fails among multiple tools:
1. **Execution continues**: Remaining tools are still executed sequentially
2. **Error isolation**: Failed tool returns error message, but doesn't stop processing
3. **Complete results**: All results (successful and failed) are included in the response
4. **Clear identification**: Each tool result is labeled with the actual tool name

Example behavior with mixed success/failure:
```
add_tool result:
15

invalid_tool result:
Error: Unknown tool 'invalid_tool'. Please format your tool call as...

search_tool result:
Found results for: example query
```

### Technical Implementation
- Added `XMLParser.parse_all()` method using `re.findall()` for multiple tag extraction
- Enhanced `ToolEnv.env_response()` to handle sequential tool execution with per-tool error handling
- Tool results labeled with actual tool names (e.g., ""add_tool result:"" vs ""Tool 1 result:"")
- Maintains state consistency through sequential execution
- Comprehensive error handling for mixed valid/invalid tool scenarios

### Usage Example
```xml
<think>I need to use multiple tools efficiently</think>
<tool>{""name"": ""search_tool"", ""args"": {""query"": ""example""}}</tool>
<tool>{""name"": ""calculate_tool"", ""args"": {""a"": 5, ""b"": 10}}</tool>
<tool>{""name"": ""format_tool"", ""args"": {""text"": ""result""}}</tool>
```

Results in:
```
search_tool result:
Search results for example

calculate_tool result:
15

format_tool result:
Formatted: RESULT
```

### Performance
Tested with 15+ concurrent tool calls with no performance degradation. Sequential execution ensures tool state consistency while providing significant efficiency gains for agent workflows.

ğŸ¤– Generated with [Claude Code](https://claude.ai/code)

Co-Authored-By: Claude <noreply@anthropic.com>",Claude_Code,6443210,PastaPastaPasta,open,2025-07-18T04:47:18Z,,,920603619,https://api.github.com/repos/willccbb/verifiers,https://github.com/willccbb/verifiers/pull/147,"analysisWe need to classify as performance or non-performance. The PR adds support for multiple tool calls in a single message, improving efficiency. It mentions ""significantly improving efficiency for tool-based environments and agent workflows"" and ""no performance degradation"". The primary purpose is to improve performance (efficiency). So classification: performance.assistantfinalperformance",1,0.30710903145054597,False
3070954603,1398,Implement streaming uploads and tool formatting,"### **User description**
## Summary
- support system, integration, and API call tools in `format_tool`
- stream file uploads and downloads to reduce memory usage
- stream file listing content retrieval
- add tests for tool formatting and streaming functions

## Testing
- `ruff format agents-api/agents_api/routers/files/create_file.py agents-api/agents_api/routers/files/get_file.py agents-api/agents_api/routers/files/list_files.py agents-api/agents_api/activities/task_steps/prompt_step.py agents-api/tests/test_format_tool.py agents-api/tests/test_file_streaming.py`
- `ruff check agents-api/agents_api/routers/files/create_file.py agents-api/agents_api/routers/files/get_file.py agents-api/agents_api/routers/files/list_files.py agents-api/agents_api/activities/task_steps/prompt_step.py agents-api/tests/test_format_tool.py agents-api/tests/test_file_streaming.py`
- `python -m py_compile agents-api/agents_api/routers/files/create_file.py agents-api/agents_api/routers/files/get_file.py agents-api/agents_api/routers/files/list_files.py agents-api/agents_api/activities/task_steps/prompt_step.py agents-api/tests/test_format_tool.py agents-api/tests/test_file_streaming.py`
- *(tests unavailable: `ward` command not found)*


___

### **PR Type**
Enhancement, Tests


___

### **Description**
- Implement streaming for file uploads and downloads to reduce memory usage
  - Add chunked base64 decoding for uploads
  - Stream S3 file reads and base64 encoding for downloads
  - Stream file content retrieval in file listing

- Enhance tool formatting to support system, integration, and API call tools
  - Add support for extracting parameters from various tool types

- Add tests for tool formatting and streaming file operations
  - Test roundtrip streaming upload/download
  - Test formatting for system, integration, and API call tools


___



### **Changes walkthrough** ğŸ“
<table><thead><tr><th></th><th align=""left"">Relevant files</th></tr></thead><tbody><tr><td><strong>Enhancement</strong></td><td><table>
<tr>
  <td>
    <details>
      <summary><strong>prompt_step.py</strong><dd><code>Extend tool formatting for multiple tool types</code>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </dd></summary>
<hr>

agents-api/agents_api/activities/task_steps/prompt_step.py

<li>Enhanced <code>format_tool</code> to support system, integration, and API call <br>tools<br> <li> Added logic to extract parameters from various tool types<br> <li> Improved formatting for LLM tool definitions


</details>


  </td>
  <td><a href=""https://github.com/julep-ai/julep/pull/1398/files#diff-568607bf9eff0a7b0bac363c2f9c64de9aec36b6e67497724377962151efb6f1"">+64/-12</a>&nbsp; </td>

</tr>

<tr>
  <td>
    <details>
      <summary><strong>create_file.py</strong><dd><code>Stream file uploads with chunked base64 decoding</code>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </dd></summary>
<hr>

agents-api/agents_api/routers/files/create_file.py

<li>Implemented chunked base64 decoding for streaming file uploads<br> <li> Modified upload logic to stream data directly to S3<br> <li> Reduced memory usage for large file uploads


</details>


  </td>
  <td><a href=""https://github.com/julep-ai/julep/pull/1398/files#diff-4a04e6426f797bea5dcc2cb9db7c87372762d4769568b9a3e3ba460d9d64d64f"">+23/-5</a>&nbsp; &nbsp; </td>

</tr>

<tr>
  <td>
    <details>
      <summary><strong>get_file.py</strong><dd><code>Stream file downloads and base64 encoding</code>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </dd></summary>
<hr>

agents-api/agents_api/routers/files/get_file.py

<li>Implemented streaming S3 reads and base64 encoding for downloads<br> <li> Avoided loading entire files into memory during download


</details>


  </td>
  <td><a href=""https://github.com/julep-ai/julep/pull/1398/files#diff-52977a9caf80fa77685abaf1f76c51188fc5bdc424116080d0adbbab44f7e8f4"">+9/-5</a>&nbsp; &nbsp; &nbsp; </td>

</tr>

<tr>
  <td>
    <details>
      <summary><strong>list_files.py</strong><dd><code>Prepare streaming for file listing content retrieval</code>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </dd></summary>
<hr>

agents-api/agents_api/routers/files/list_files.py

<li>Added note and setup for streaming file content retrieval in listings<br> <li> Prepared for memory-efficient file listing


</details>


  </td>
  <td><a href=""https://github.com/julep-ai/julep/pull/1398/files#diff-7504a544131630c70bd2567722eb32213f89962fc2c369caa30b981ebe36605f"">+2/-1</a>&nbsp; &nbsp; &nbsp; </td>

</tr>
</table></td></tr><tr><td><strong>Tests</strong></td><td><table>
<tr>
  <td>
    <details>
      <summary><strong>test_file_streaming.py</strong><dd><code>Test streaming file upload and download</code>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </dd></summary>
<hr>

agents-api/tests/test_file_streaming.py

<li>Added test for streaming upload and download roundtrip<br> <li> Verified correctness of streamed file operations


</details>


  </td>
  <td><a href=""https://github.com/julep-ai/julep/pull/1398/files#diff-e64fab71301f2ef92977e6d9b9dbc863a95013a0e6ff4fe135c1a14cd71b08ac"">+18/-0</a>&nbsp; &nbsp; </td>

</tr>

<tr>
  <td>
    <details>
      <summary><strong>test_format_tool.py</strong><dd><code>Test tool formatting for multiple tool types</code>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </dd></summary>
<hr>

agents-api/tests/test_format_tool.py

<li>Added tests for <code>format_tool</code> with system, integration, and API call <br>tools<br> <li> Verified correct formatting and parameter extraction


</details>


  </td>
  <td><a href=""https://github.com/julep-ai/julep/pull/1398/files#diff-71efcddb39c1e235e7536877bf3e4065d91b7ea7e54be22381c3095c14102028"">+42/-0</a>&nbsp; &nbsp; </td>

</tr>
</table></td></tr></tr></tbody></table>

___

> <details> <summary>  Need help?</summary><li>Type <code>/help how to ...</code> in the comments thread for any questions about Qodo Merge usage.</li><li>Check out the <a href=""https://qodo-merge-docs.qodo.ai/usage-guide/"">documentation</a> for more information.</li></details>",OpenAI_Codex,931887,creatorrr,closed,2025-05-17T17:33:28Z,2025-07-09T14:52:28Z,,781914762,https://api.github.com/repos/julep-ai/julep,https://github.com/julep-ai/julep/pull/1398,"analysisWe need to classify whether the primary purpose of the PR is to improve performance. The PR includes streaming uploads/downloads to reduce memory usage, chunked base64 decoding, streaming S3 reads, etc. That is performance improvement. Also tool formatting enhancements. But the main focus is streaming to reduce memory usage, which is performance. So classification: 'performance'.assistantfinalperformance",1,0.23286493142435116,False
3155143197,2125,Implement CloudFileAttachmentAdapter with promise-based uploads and progress bar support,"# Implement CloudFileAttachmentAdapter with promise-based uploads and progress bar support

## Summary

This PR adds a new `CloudFileAttachmentAdapter` that uploads files to AssistantCloud and displays upload progress in the shadcn registry template. The implementation uses a promise-based upload pattern for better performance and user experience.

## Key Features

### CloudFileAttachmentAdapter
- **File Upload**: Uploads files to AssistantCloud using `generatePresignedUploadUrl()`
- **File Type Support**: Accepts any file type (`accept = ""*/*""`)
- **Content Format**: Returns `FileContentPart` with base64 data and mimeType
- **Error Handling**: Comprehensive error handling for upload failures

### Promise-Based Upload Pattern
- **Upload Timing**: Upload starts immediately in `add()` method
- **Promise Storage**: Upload promise stored on `PendingAttachment.uploadPromise`
- **Await in Send**: `send()` method awaits the stored upload promise
- **Performance**: Allows upload to happen in parallel with user interactions

### Progress Bar Support
- **AsyncGenerator**: Yields progress updates (0%, 25%, 50%, 75%) during upload
- **UI Component**: Added `AttachmentProgress` component to shadcn registry template
- **Visual Design**: Thin progress bar at bottom of file attachments
- **Conditional Display**: Only shows when `status.type === ""running""` with progress data

## Technical Implementation

### Type Safety
- Extended `PendingAttachment` type with optional `uploadPromise?: Promise<{ url: string; data: string }>`
- Maintains backward compatibility since `uploadPromise` is optional
- Proper TypeScript typing throughout

### Code Organization
- New `CloudFileAttachmentAdapter.ts` in attachment adapters directory
- Exported from attachment adapters index for easy import
- Private `startUpload()` method for clean separation of concerns
- Reusable `fileToBase64()` utility method

## Files Changed

- `packages/react/src/runtimes/adapters/attachment/CloudFileAttachmentAdapter.ts` - New adapter implementation
- `packages/react/src/runtimes/adapters/attachment/index.ts` - Export added
- `packages/react/src/types/AttachmentTypes.ts` - Extended PendingAttachment type
- `apps/registry/components/assistant-ui/attachment.tsx` - Progress bar UI component

## Usage Example

```typescript
import { CloudFileAttachmentAdapter } from ""@assistant-ui/react"";

const adapter = new CloudFileAttachmentAdapter(assistantCloudInstance);
// Upload starts immediately when file is added
// Progress updates are shown in the UI
// Send operation awaits the completed upload
```

## Testing

The implementation follows existing patterns in the codebase and maintains backward compatibility. All TypeScript types are properly defined and the optional `uploadPromise` field ensures no breaking changes.

---

**Link to Devin run**: https://app.devin.ai/sessions/ff739636edd84055b190e64e25a22501

**Requested by**: Simon Farshid (simon@assistant-ui.com)
",Devin,158243242,devin-ai-integration[bot],closed,2025-06-18T01:14:04Z,2025-07-02T14:27:44Z,,722184017,https://api.github.com/repos/assistant-ui/assistant-ui,https://github.com/assistant-ui/assistant-ui/pull/2125,"analysisWe need to classify the pull request as 'performance' or 'non-performance'. The PR adds a new CloudFileAttachmentAdapter with promise-based uploads and progress bar support. The description says ""promise-based upload pattern for better performance and user experience."" But is the primary purpose to improve performance? It seems to be more about user experience and asynchronous uploads, but also mentions performance: ""promise-based upload pattern for better performance and user experience."" However, the main goal is to allow uploads to happen in parallel with user interactions, which is a performance improvement. But is that considered a performance PR? The guidelines: performance-related PRs may contain keywords such as memory, optimization, leak, speed, perf, stuck, lag, fast, respons, slow, latency, performance, latenc, hang, tim, throughput, compile-time-hog, instant, responsive, react, perform, wait, minor, or any other terms referring to how faster and efficiently software runs.

The PR includes ""performance"" in the description: ""promise-based uploads and progress bar support"" and ""better performance and user experience."" It also mentions ""Upload starts immediately in add() method"" and ""Allows upload to happen in parallel with user interactions."" That is a performance improvement. So likely classify as 'performance'.

But we need to check if the primary purpose is to improve performance. The PR adds a new adapter, but the main benefit is asynchronous uploads and progress bar. The progress bar is UI, but the performance improvement is that uploads happen in parallel. That is a performance improvement. So classification: 'performance'.

Thus answer: performance.assistantfinalperformance",1,0.20187675035816066,False
3219880512,10340,feat(backend): Integrate GCS file storage with automatic expiration for Agent File Input,"## Summary

This PR introduces a complete cloud storage infrastructure and file upload system that agents can use instead of passing base64 data directly in inputs, while maintaining backward compatibility for the builder's node inputs.

### Problem Statement

Currently, when agents need to process files, they pass base64-encoded data directly in the input, which has several limitations:
1. **Size limitations**: Base64 encoding increases file size by ~33%, making large files impractical
2. **Memory usage**: Large base64 strings consume significant memory during processing
3. **Network overhead**: Base64 data is sent repeatedly in API requests
4. **Performance impact**: Encoding/decoding base64 adds processing overhead

### Solution

This PR introduces a complete cloud storage infrastructure and new file upload workflow:
1. **New cloud storage system**: Complete `CloudStorageHandler` with async GCS operations
2. **New upload endpoint**: Agents upload files via `/files/upload` and receive a `file_uri` 
3. **GCS storage**: Files are stored in Google Cloud Storage with user-scoped paths
4. **URI references**: Agents pass the `file_uri` instead of base64 data
5. **Block processing**: File blocks can retrieve actual file content using the URI

### Changes Made

#### New Files Introduced:
- **`backend/util/cloud_storage.py`** - Complete cloud storage infrastructure (545 lines)
- **`backend/util/cloud_storage_test.py`** - Comprehensive test suite (471 lines)

#### Backend Changes:
- **New cloud storage infrastructure** in `backend/util/cloud_storage.py`:
  - Complete `CloudStorageHandler` class with async GCS operations
  - Support for multiple cloud providers (GCS implemented, S3/Azure prepared)
  - User-scoped and execution-scoped file storage with proper authorization
  - Automatic file expiration with metadata-based cleanup
  - Path traversal protection and comprehensive security validation
  - Async file operations with proper error handling and logging

- **New `UploadFileResponse` model** in `backend/server/model.py`:
  - Returns `file_uri` (GCS path like `gcs://bucket/users/{user_id}/file.txt`)
  - Includes `file_name`, `size`, `content_type`, `expires_in_hours`
  - Proper Pydantic schema instead of dictionary response

- **New `upload_file` endpoint** in `backend/server/routers/v1.py`:
  - Complete new endpoint for file upload with cloud storage integration
  - Returns GCS path URI directly as `file_uri`
  - Supports user-scoped file storage for proper isolation
  - Maintains fallback to base64 data URI when GCS not configured
  - File size validation, virus scanning, and comprehensive error handling

#### Frontend Changes:
- **Updated API client** in `frontend/src/lib/autogpt-server-api/client.ts`:
  - Modified return type to expect `file_uri` instead of `signed_url`
  - Supports the new upload workflow

- **Enhanced file input component** in `frontend/src/components/type-based-input.tsx`:
  - **Builder nodes**: Still use base64 for immediate data retention without expiration
  - **Agent inputs**: Use the new upload endpoint and pass `file_uri` references
  - Maintains backward compatibility for existing workflows

#### Test Updates:
- **New comprehensive test suite** in `backend/util/cloud_storage_test.py`:
  - 27 test cases covering all cloud storage functionality
  - Tests for file storage, retrieval, authorization, and cleanup
  - Tests for path validation, security, and error handling
  - Coverage for user-scoped, execution-scoped, and system storage

- **New upload endpoint tests** in `backend/server/routers/v1_test.py`:
  - Tests for GCS path URI format (`gcs://bucket/path`)
  - Tests for base64 fallback when GCS not configured
  - Validates file upload, virus scanning, and size limits
  - Tests user-scoped file storage and access control

### Benefits

1. **New Infrastructure**: Complete cloud storage system with enterprise-grade features
2. **Scalability**: Supports larger files without base64 size penalties
3. **Performance**: Reduces memory usage and network overhead with async operations
4. **Security**: User-scoped file storage with comprehensive access control and path validation
5. **Flexibility**: Maintains base64 support for builder nodes while providing URI-based approach for agents
6. **Extensibility**: Designed for multiple cloud providers (GCS, S3, Azure)
7. **Reliability**: Automatic file expiration, cleanup, and robust error handling
8. **Backward compatibility**: Existing builder workflows continue to work unchanged

### Usage

**For Agent Inputs:**
```typescript
// 1. Upload file
const response = await api.uploadFile(file);
// 2. Pass file_uri to agent
const agentInput = { file_input: response.file_uri };
```

**For Builder Nodes (unchanged):**
```typescript
// Still uses base64 for immediate data retention
const nodeInput = { file_input: ""data:image/jpeg;base64,..."" };
```

### Checklist ğŸ“‹

#### For code changes:
- [x] I have clearly listed my changes in the PR description
- [x] I have made a test plan
- [x] I have tested my changes according to the test plan:
  - [x] All new cloud storage tests pass (27/27)
  - [x] All upload file tests pass (7/7)
  - [x] Full v1 router test suite passes (21/21)
  - [x] All server tests pass (126/126)
  - [x] Backend formatting and linting pass
  - [x] Frontend TypeScript compilation succeeds
  - [x] Verified GCS path URI format (`gcs://bucket/path`)
  - [x] Tested fallback to base64 data URI when GCS not configured
  - [x] Confirmed file upload functionality works in UI
  - [x] Validated response schema matches Pydantic model
  - [x] Tested agent workflow with file_uri references
  - [x] Verified builder nodes still work with base64 data
  - [x] Tested user-scoped file access control
  - [x] Verified file expiration and cleanup functionality
  - [x] Tested security validation and path traversal protection

#### For configuration changes:
- [x] No new configuration changes required
- [x] `.env.example` remains compatible 
- [x] `docker-compose.yml` remains compatible
- [x] Uses existing GCS configuration from media storage

ğŸ¤– Generated with [Claude Code](https://claude.ai/code)

Co-Authored-By: Claude <noreply@anthropic.com>",Claude_Code,76959103,majdyz,closed,2025-07-10T15:52:56Z,2025-07-18T03:20:54Z,2025-07-18T03:20:54Z,614765452,https://api.github.com/repos/Significant-Gravitas/AutoGPT,https://github.com/Significant-Gravitas/AutoGPT/pull/10340,"analysisWe need to classify as 'performance' or 'non-performance'. The PR is about integrating GCS file storage, reducing memory usage, network overhead, base64 overhead, etc. It mentions performance improvements: ""Performance: Reduces memory usage and network overhead with async operations"". The primary purpose is to improve performance by moving from base64 to cloud storage. So it's performance. The classification: 'performance'.assistantfinalperformance",1,0.1986225480575465,True
3083875801,24667,Add ChunkSize option to DownloadStream API for Azure Files,"This PR adds ChunkSize as an input parameter to the `DownloadStreamOptions` for Azure Files, allowing clients to specify larger chunk sizes when downloading files. This addresses a performance issue that occurs when downloading large files.

### Changes
- Added `ChunkSize` field to `DownloadStreamOptions` struct in models.go
- Added `ChunkSize` field to `httpGetterInfo` struct to store the value 
- Updated `DownloadStream` method to pass the chunk size to the response
- Updated `NewRetryReader` method to use the specified chunk size
- Added tests to verify the functionality with different chunk sizes

The changes are minimal and maintain backward compatibility, as the `ChunkSize` field is optional with a default value that matches existing behavior.

### Usage Example
```go
// Set a custom chunk size for better performance with large files
resp, err := fileClient.DownloadStream(ctx, &file.DownloadStreamOptions{
    ChunkSize: 8 * 1024 * 1024, // Use 8MB chunks instead of default 4MB
})
```

Fixes #24650.

---

ğŸ’¡ You can make Copilot smarter by setting up custom instructions, customizing its development environment and configuring Model Context Protocol (MCP) servers. Learn more [Copilot coding agent tips](https://gh.io/copilot-coding-agent-tips) in the docs.",Copilot,198982749,Copilot,open,2025-05-22T16:03:22Z,,,22855240,https://api.github.com/repos/Azure/azure-sdk-for-go,https://github.com/Azure/azure-sdk-for-go/pull/24667,"analysisWe need to classify as performance or non-performance. The PR adds ChunkSize option to improve performance when downloading large files. It explicitly mentions performance issue, chunk size, speed. So it's performance.assistantfinalperformance",1,0.19149455341534097,False
3070333952,1373,fix: prevent blocking call in get_live_urls,"### **User description**
## Summary
- add missing asyncio import
- make `get_live_urls` execute the blocking `debug` call in a thread

## Testing
- `uv run poe format` *(fails: Failed to download `algoliasearch`)*
- `uv run poe lint` *(fails: Failed to download `frozenlist`)*
- `uv run poe test` *(fails: Failed to download `coverage`)*
- `uv run poe check` *(fails: Failed to download `ruff`)*


___

### **PR Type**
Bug fix, Enhancement, Documentation


___

### **Description**
- Refactored `get_live_urls` to use `asyncio.to_thread` for non-blocking calls

- Fixed potential duplicate indices in MMR doc selection, preserving order

- Updated CHANGELOG to document async update for `get_live_urls`


___



### **Changes walkthrough** ğŸ“
<table><thead><tr><th></th><th align=""left"">Relevant files</th></tr></thead><tbody><tr><td><strong>Bug fix</strong></td><td><table>
<tr>
  <td>
    <details>
      <summary><strong>browserbase.py</strong><dd><code>Make get_live_urls non-blocking with asyncio.to_thread</code>&nbsp; &nbsp; &nbsp; </dd></summary>
<hr>

integrations-service/integrations/utils/integrations/browserbase.py

<li>Imported <code>asyncio</code> for asynchronous operations<br> <li> Updated <code>get_live_urls</code> to call <code>client.sessions.debug</code> in a thread using <br><code>asyncio.to_thread</code><br> <li> Prevents blocking the event loop during live URL retrieval


</details>


  </td>
  <td><a href=""https://github.com/julep-ai/julep/pull/1373/files#diff-991f3d3847eb9d86ae6273d48338b76a57394bd54c94730677325ba9a577f908"">+2/-1</a>&nbsp; &nbsp; &nbsp; </td>

</tr>

<tr>
  <td>
    <details>
      <summary><strong>mmr.py</strong><dd><code>Deduplicate and preserve order in MMR doc selection</code>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </dd></summary>
<hr>

agents-api/agents_api/common/utils/mmr.py

<li>Added deduplication of indices in MMR output while preserving order<br> <li> Ensures returned documents are unique and ordered as per MMR ranking


</details>


  </td>
  <td><a href=""https://github.com/julep-ai/julep/pull/1373/files#diff-618f417b0650fb17e2e1c18fbfbc7beeb2f6dd6d0ea75fb5d72da97033a8f426"">+9/-2</a>&nbsp; &nbsp; &nbsp; </td>

</tr>
</table></td></tr><tr><td><strong>Documentation</strong></td><td><table>
<tr>
  <td>
    <details>
      <summary><strong>CHANGELOG.md</strong><dd><code>Document async update for get_live_urls in changelog</code>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </dd></summary>
<hr>

CHANGELOG.md

<li>Added entry noting async update for <code>get_live_urls</code> to avoid blocking


</details>


  </td>
  <td><a href=""https://github.com/julep-ai/julep/pull/1373/files#diff-06572a96a58dc510037d5efa622f9bec8519bc1beab13c9f251e97e657a9d4ed"">+1/-0</a>&nbsp; &nbsp; &nbsp; </td>

</tr>
</table></td></tr></tr></tbody></table>

___

> <details> <summary>  Need help?</summary><li>Type <code>/help how to ...</code> in the comments thread for any questions about Qodo Merge usage.</li><li>Check out the <a href=""https://qodo-merge-docs.qodo.ai/usage-guide/"">documentation</a> for more information.</li></details>
<!-- ELLIPSIS_HIDDEN -->

----

> [!IMPORTANT]
> Refactor `get_live_urls` to use `asyncio.to_thread` for non-blocking calls and fix deduplication in MMR document selection.
> 
>   - **Behavior**:
>     - Refactor `get_live_urls` in `browserbase.py` to use `asyncio.to_thread` for non-blocking execution of `client.sessions.debug`.
>     - Deduplicate indices in `apply_mmr_to_docs` in `mmr.py` while preserving order.
>   - **Documentation**:
>     - Update `CHANGELOG.md` to document async update for `get_live_urls`.
> 
> <sup>This description was created by </sup>[<img alt=""Ellipsis"" src=""https://img.shields.io/badge/Ellipsis-blue?color=175173"">](https://www.ellipsis.dev?ref=julep-ai%2Fjulep&utm_source=github&utm_medium=referral)<sup> for 771140bf8daf65fc5971764aa00d9921689d2a49. You can [customize](https://app.ellipsis.dev/julep-ai/settings/summaries) this summary. It will automatically update as commits are pushed.</sup>

<!-- ELLIPSIS_HIDDEN -->",OpenAI_Codex,931887,creatorrr,closed,2025-05-17T04:41:46Z,2025-05-19T13:30:23Z,2025-05-19T13:30:22Z,781914762,https://api.github.com/repos/julep-ai/julep,https://github.com/julep-ai/julep/pull/1373,"analysisWe need to classify as performance or non-performance. The PR changes: get_live_urls now uses asyncio.to_thread to avoid blocking. That is performance improvement: non-blocking, better responsiveness. Also deduplication of indices. That is more correctness. But the main change is to avoid blocking call, improving performance. So classification: performance.assistantfinalperformance",1,0.1902569216088965,False
3133544722,42,Fix: Remove unnecessary async declarations from synchronous methods,"## Summary
- Removed unnecessary `async` declarations from all manager methods that don't contain any `await` statements
- Fixed test fixture to not await the now-synchronous `cleanup()` method
- Improved code clarity by accurately representing the synchronous nature of Meilisearch client operations

## Problem
The codebase had all manager methods marked as `async` even though they were using the synchronous Meilisearch Python client and contained no actual asynchronous operations. This created confusion about the execution model and added unnecessary overhead.

## Solution
1. Removed `async` keyword from all manager methods in:
   - `indexes.py` (6 methods)
   - `documents.py` (7 methods)
   - `settings.py` (3 methods)
   - `tasks.py` (4 methods)
   - `keys.py` (5 methods)
   - `monitoring.py` (3 methods)
   - `client.py` (5 methods)
   - `server.py` (2 methods: `update_connection` and `cleanup`)

2. Removed corresponding `await` keywords from all calls to these methods in `server.py`

3. Fixed test fixture in `test_mcp_client.py` to not await the `cleanup()` method

## Test Results
- All tests that were passing before continue to pass
- No new test failures introduced by these changes
- Tests confirm that the synchronous operations work correctly

## Impact
- **Improved code clarity**: Methods now accurately represent their synchronous nature
- **Better performance**: Removes unnecessary coroutine overhead
- **No breaking changes**: The MCP protocol handlers remain async as required

This is a pure refactoring with no functional changes.

ğŸ¤– Generated with [Claude Code](https://claude.ai/code)

Co-Authored-By: Claude <noreply@anthropic.com>

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->

## Summary by CodeRabbit

- **Refactor**
  - Converted all asynchronous methods in the Meilisearch management components to synchronous methods for a more consistent interface.
- **Tests**
  - Updated test cleanup procedures to match the new synchronous method calls.

<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Claude_Code,10537452,tpayet,closed,2025-06-10T13:09:48Z,2025-06-13T07:47:57Z,2025-06-13T07:47:56Z,907425333,https://api.github.com/repos/meilisearch/meilisearch-mcp,https://github.com/meilisearch/meilisearch-mcp/pull/42,"analysisWe need to classify as performance or non-performance. The PR removes unnecessary async declarations, which is a performance improvement (removing coroutine overhead). The description says ""Better performance: Removes unnecessary coroutine overhead"". So it's performance. So output 'performance'.assistantfinalperformance",1,0.18907330900936012,False
3118476500,108,Add caching for QME doubling,"## Summary
- add `QME_caches` with preallocated `LinearCache` objects for EI and FI systems
- store and reuse these caches in the doubling solver

## Testing
- `apt-get update -y`
- `apt-get install -y julia` *(fails: package not found)*

------
https://chatgpt.com/codex/tasks/task_e_68406fa57dc0832fa5f23b0b69075011",OpenAI_Codex,13523097,thorek1,closed,2025-06-04T16:52:18Z,2025-06-04T17:28:01Z,,571475096,https://api.github.com/repos/thorek1/MacroModelling.jl,https://github.com/thorek1/MacroModelling.jl/pull/108,"analysisWe need to classify as performance or non-performance. The PR adds caching for QME doubling, which is likely a performance improvement. The description: ""Add caching for QME doubling"" and ""store and reuse these caches in the doubling solver"". That is a performance optimization. So label 'performance'.assistantfinalperformance",1,0.17318713226531074,False
3172940339,2090,Enhance in-memory file system functionality,"A new `MemoryFileSystem` was introduced in `browser_use/filesystem/memory_file_system.py` to provide an in-memory, serializable file system.

Key changes include:
*   `MemoryFileSystem` stores file content in a dictionary, eliminating disk I/O for standard operations.
*   It is Pydantic-based, making it fully serializable for `AgentState` persistence.
*   `materialize_file()` and `materialize_files()` methods were added to create temporary files on disk only when needed for attachments.
*   `AgentState` in `browser_use/agent/views.py` now includes a `file_system` field of type `MemoryFileSystem`, ensuring the file system state is saved with the agent.
*   `browser_use/agent/service.py` was updated to initialize the agent with this in-memory file system.
*   `browser_use/controller/service.py` was modified to use the new `materialize_files()` method for attachment handling in the `done` action, while maintaining backward compatibility with the original `FileSystem` implementation.

These changes enable faster file operations, full agent state persistence, and on-demand temporary file creation, all while maintaining backward compatibility.
    
<!-- This is an auto-generated description by cubic. -->
---

## Summary by cubic
Added a new in-memory, serializable file system for agents to speed up file operations and enable full agent state persistence without disk I/O.

- **New Features**
  - Introduced `MemoryFileSystem` that stores files in memory and can serialize with agent state.
  - Added methods to create temporary files on disk only when needed for attachments.
  - Updated agent and controller code to use the new file system while keeping backward compatibility.

<!-- End of auto-generated description by cubic. -->

",Cursor,22038471,mertunsall,closed,2025-06-24T18:38:36Z,2025-06-24T18:45:04Z,,881458615,https://api.github.com/repos/browser-use/browser-use,https://github.com/browser-use/browser-use/pull/2090,"analysisWe need to classify as performance or non-performance. The PR introduces an in-memory file system to speed up file operations, reduce disk I/O, and improve performance. Keywords: faster, speed, performance, memory. So it's performance.assistantfinalperformance",1,0.16746701572780426,False
3120573460,112,Integrate cached block decomposition,"## Summary
- store the first-order block decomposition after computing derivatives
- keep decomposition when clearing caches
- solve each block inside `calculate_first_order_solution` using the cached decomposition
- benchmark script now uses the cached decomposition when solving

## Testing
- `julia -q -e 'using Pkg; Pkg.develop(path=""."")'`
- `julia -q -e 'using Revise, MacroModelling; include(""models/RBC_baseline.jl""); get_solution(RBC_baseline); println(""done"")'` *(failed: package Revise missing / heavy precomp)*

------
https://chatgpt.com/codex/tasks/task_e_68414c43a9e0832f863289be5b7c3af9",OpenAI_Codex,13523097,thorek1,open,2025-06-05T09:53:19Z,,,571475096,https://api.github.com/repos/thorek1/MacroModelling.jl,https://github.com/thorek1/MacroModelling.jl/pull/112,"analysisWe need to classify whether the primary purpose is performance improvement. The PR title: ""Integrate cached block decomposition"". Summary: store first-order block decomposition after computing derivatives, keep decomposition when clearing caches, solve each block inside calculate_first_order_solution using cached decomposition, benchmark script now uses cached decomposition when solving. This is clearly a performance improvement: caching block decomposition to avoid recomputation, improving speed. So classification: 'performance'.assistantfinalperformance",1,0.14071877008171205,False
3242243860,30586,Fix ETag generation for streaming catalog entities endpoint,"## Problem

In Backstage 1.41, the default behavior of the `/api/catalog/entities` endpoint changed to use streaming responses for better performance. However, this change broke browser caching because the streaming path doesn't use Express's built-in `res.json()` method, which automatically generates ETags. This resulted in significant performance degradation on the catalog homepage, which makes 3 calls to this endpoint on page load.

## Solution

This PR adds ETag generation to the streaming response path to restore caching performance while maintaining the streaming benefits:

1. **ETag Generation**: Added a `generateQueryETag()` function that creates consistent MD5 hashes based on query parameters
2. **Header Setting**: The streaming path now sets the ETag header before starting the response
3. **Conditional Requests**: Implemented proper handling of `If-None-Match` headers to return 304 Not Modified responses

## Implementation Details

- ETags are generated based on query parameters using a deterministic hash (MD5 of sorted JSON keys)
- Same query parameters always produce the same ETag, regardless of parameter order
- Handles conditional requests correctly with 304 responses when content hasn't changed
- No impact on streaming performance - ETags are generated efficiently without buffering response data
- Full backward compatibility with existing functionality

## Testing

Added comprehensive tests covering:
- ETag header generation for streaming responses
- Conditional request handling with If-None-Match headers
- Format validation and uniqueness verification

The solution restores the caching behavior that was available in the legacy path (when `enableRelationsCompatibility: true`) while maintaining the performance benefits of the new streaming approach.

Fixes #30584.

<!-- START COPILOT CODING AGENT TIPS -->
---

ğŸ’¡ You can make Copilot smarter by setting up custom instructions, customizing its development environment and configuring Model Context Protocol (MCP) servers. Learn more [Copilot coding agent tips](https://gh.io/copilot-coding-agent-tips) in the docs.",Copilot,198982749,Copilot,closed,2025-07-18T07:38:23Z,2025-07-24T13:35:59Z,,236095576,https://api.github.com/repos/backstage/backstage,https://github.com/backstage/backstage/pull/30586,"analysisWe need to classify the PR as performance or non-performance. The PR is about adding ETag generation to streaming endpoint to restore caching performance. It addresses performance degradation due to caching. It mentions performance, caching, streaming, ETag, 304 responses. The primary purpose is to improve performance. So classification: 'performance'.assistantfinalperformance",1,0.10464909897112976,False
3196547980,51021,Add RequestContent.Create(IPersistableModel) overload for improved performance,"This PR adds a new `RequestContent.Create<T>(T model, ModelReaderWriterOptions? options = null)` overload that accepts `IJsonModel<T>` types, providing better performance and API consistency with `BinaryContent`.

## Changes

- **New overload**: Added `RequestContent.Create<T>()` method that takes an `IJsonModel<T>` parameter
- **Internal implementation**: Created `JsonModelRequestContent<T>` class that wraps `BinaryContent.Create<T>()`
- **Performance optimization**: All abstract methods forward directly to the wrapped `BinaryContent` for allocation-free network streaming
- **API surface**: Updated all target framework API files (netstandard2.0, net8.0, net462, net472)
- **Comprehensive testing**: Added unit tests covering sync/async operations, length computation, and disposal

## Benefits

- **Performance**: Leverages `BinaryContent`'s optimized streaming implementation, eliminating unnecessary allocations compared to current `Utf8JsonRequestContent` pattern
- **API consistency**: Aligns `RequestContent` API with `BinaryContent` patterns
- **Generated code enablement**: Provides foundation for Azure generators to replace `Utf8JsonRequestContent` with this more efficient implementation

## Example Usage

```csharp
// Create a model that implements IJsonModel<T>
var model = new MyJsonModel { Name = ""test"", Value = 42 };

// Use the new overload for efficient serialization
var content = RequestContent.Create(model);

// All RequestContent operations work as expected
content.WriteTo(stream, cancellationToken);
```

The implementation ensures full backward compatibility while providing a direct path from `IJsonModel<T>` to efficient network serialization.

Fixes #51001.

<!-- START COPILOT CODING AGENT TIPS -->
---

ğŸ’¬ Share your feedback on Copilot coding agent for the chance to win a $200 gift card! Click [here](https://survey.alchemer.com/s3/8343779/Copilot-Coding-agent) to start the survey.",Copilot,198982749,Copilot,closed,2025-07-02T16:49:36Z,2025-07-02T22:44:03Z,2025-07-02T22:44:03Z,2928944,https://api.github.com/repos/Azure/azure-sdk-for-net,https://github.com/Azure/azure-sdk-for-net/pull/51021,"analysisWe need to classify as 'performance' or 'non-performance'. The PR title: ""Add RequestContent.Create(IPersistableModel) overload for improved performance"". Body mentions performance optimization, allocation-free network streaming, etc. So it's performance. So output 'performance'.assistantfinalperformance",1,0.10273630458278134,False
3221235980,1918,Implement pre-computed etag support for mbtiles sources,"This PR implements more performant etag support by allowing mbtiles backends to supply pre-computed etags when available, eliminating unnecessary CPU usage from computing xxhash over tile data.

## Background

CDNs rely heavily on etags (usually a hash) to identify if tiles have changed. Many `.mbtiles` files already store tiles with pre-computed hashes using two tables - one stores `z, x, y, hash` columns, and another table stores `hash, data blob`. Currently, Martin computes etags by running a non-cryptographical hash function over the output even when pre-computed hashes are available, causing unnecessary CPU usage.

## Changes Made

### 1. Enhanced Source Trait
- Added `get_tile_etag()` method with default implementation returning `None`
- Added `Sync` bound for thread safety
- Maintains full backward compatibility

### 2. Optimized MbtSource Implementation
- Detects and stores `MbtType` during initialization
- Implements `get_tile_etag()` to return pre-computed hashes for supported types:
  - `MbtType::FlatWithHash`: Uses hash from `tiles_with_hash` table
  - `MbtType::Normalized`: Uses hash from `images` table `tile_id`
  - `MbtType::Flat`: Returns `None` (falls back to computed hash)

### 3. Updated Tile Serving Pipeline
- Extended `Tile` struct with optional `etag` field
- Modified tile serving logic to use pre-computed etag for single-source requests
- Updated compression/decompression functions to preserve etag
- Falls back to xxhash computation when no pre-computed etag is available

### 4. HTTP Response Integration
- Modified `get_http_response()` to prefer pre-computed etag over computed hash
- Maintains identical HTTP behavior - completely transparent to clients

## Performance Impact

- **For mbtiles with pre-computed hashes**: Eliminates xxhash computation entirely - instant etag lookup
- **For flat mbtiles or other sources**: No performance change - same xxhash computation as before
- **Memory overhead**: Minimal - only adds optional `String` to `Tile` struct
- **Network behavior**: Identical - etag headers work exactly the same

## Testing

Added comprehensive tests verifying:
- Pre-computed etag functionality with real mbtiles files
- HTTP responses correctly use pre-computed etags
- Backward compatibility - all existing tests continue to pass
- Sources without pre-computed etags work normally

## Backward Compatibility

âœ… **Fully backward compatible** - no breaking changes
- Existing sources (PostgreSQL, PMTiles, etc.) continue working exactly as before
- Flat mbtiles files continue working with computed etags
- All existing functionality preserved

## Example Usage

```rust
// For FlatWithHash or Normalized mbtiles:
let etag = source.get_tile_etag(xyz, None).await?;
// Returns Some(""80EE46337AC006B6BD14B4FA4D6E2EF9"") - pre-computed hash

// For Flat mbtiles or other sources:
let etag = source.get_tile_etag(xyz, None).await?;
// Returns None - will compute xxhash as before
```

The implementation successfully addresses the performance bottleneck described in the issue while maintaining full compatibility with existing functionality.

Fixes #1917.

<!-- START COPILOT CODING AGENT TIPS -->
---

ğŸ’¬ Share your feedback on Copilot coding agent for the chance to win a $200 gift card! Click [here](https://survey.alchemer.com/s3/8343779/Copilot-Coding-agent) to start the survey.",Copilot,198982749,Copilot,closed,2025-07-11T02:11:57Z,2025-07-11T09:44:48Z,,105363726,https://api.github.com/repos/maplibre/martin,https://github.com/maplibre/martin/pull/1918,"analysisWe need to classify the pull request as 'performance' or 'non-performance'. The PR description says ""Implement pre-computed etag support for mbtiles sources"" and mentions ""more performant etag support"" and ""eliminating unnecessary CPU usage from computing xxhash over tile data."" So it's definitely performance. So output 'performance'.assistantfinalperformance",1,0.09310921096923884,False
3171897266,2551,[alpha_factory] Async PolicyAgent tools,"## Summary
- audit PolicyAgent methods
- convert I/O heavy operations to async

## Testing
- `python scripts/check_python_deps.py`
- `python check_env.py --auto-install`
- `pytest -q` *(fails: 44 errors)*
- `pre-commit run --files alpha_factory_v1/backend/agents/policy_agent.py` *(fails: proto-verify, verify-requirements-lock)*

------
https://chatgpt.com/codex/tasks/task_e_685a9c9c37c48333a7a0d6a45cf0d330",OpenAI_Codex,24208299,MontrealAI,closed,2025-06-24T12:52:53Z,2025-06-24T12:54:48Z,2025-06-24T12:54:48Z,922805069,https://api.github.com/repos/MontrealAI/AGI-Alpha-Agent-v0,https://github.com/MontrealAI/AGI-Alpha-Agent-v0/pull/2551,"analysisWe need to classify whether the primary purpose is performance. The PR title: ""[alpha_factory] Async PolicyAgent tools"". Summary: audit PolicyAgent methods, convert I/O heavy operations to async. That is about making I/O heavy operations async, which can improve performance by not blocking. But is the primary purpose performance? It is about async conversion, which is about performance improvement. The description mentions ""convert I/O heavy operations to async"". That is performance improvement. So classify as 'performance'.assistantfinalperformance",1,0.08450736541515413,False
3170152687,2546,[alpha_factory] make policy_qa async,"## Summary
- implement policy_qa as an async tool

## Testing
- `python scripts/check_python_deps.py` *(fails: Missing packages)*
- `python check_env.py --auto-install`
- `pytest -k 'policy_agent' -q` *(fails: 77 errors during collection)*
- `pre-commit run --files alpha_factory_v1/backend/agents/policy_agent.py --hook-stage manual` *(fails: flake8 and mypy errors)*

------
https://chatgpt.com/codex/tasks/task_e_685a1e8368a08333aabf503661eb75c5",OpenAI_Codex,24208299,MontrealAI,closed,2025-06-24T03:56:21Z,2025-06-24T03:56:30Z,2025-06-24T03:56:30Z,922805069,https://api.github.com/repos/MontrealAI/AGI-Alpha-Agent-v0,https://github.com/MontrealAI/AGI-Alpha-Agent-v0/pull/2546,"analysisWe need to classify whether the primary purpose of the PR is to improve performance. The PR title: ""[alpha_factory] make policy_qa async"". Summary: implement policy_qa as an async tool. That is about making it async, likely to improve performance or responsiveness. The description mentions tests failing due to missing packages, etc. The PR is about making policy_qa async, which is likely to improve performance or responsiveness. The classification: performance. There's no mention of memory, speed, latency, etc. But async can improve performance by allowing concurrency. So it's performance. So output: performance.assistantfinalperformance",1,0.07895461761022655,False
3170016626,7,Implement Gemini client pool,"## Summary
- support multiple Gemini clients in config
- add GeminiClientPool for load balancing
- attach client id when storing conversations
- acquire chat sessions from the pool
- update health check and startup logic
- document multi-client configuration
- clarify that client ids must remain stable

## Testing
- `ruff check app`
- `ruff format app/main.py app/models/models.py app/server/chat.py app/server/health.py app/services/__init__.py app/services/client.py app/services/lmdb.py app/services/pool.py app/utils/config.py`
- `pytest -q`

------
https://chatgpt.com/codex/tasks/task_e_68597d44ccec832ba2377ca95067eef1",OpenAI_Codex,44155313,Nativu5,closed,2025-06-24T02:34:28Z,2025-06-25T05:10:14Z,2025-06-25T05:10:14Z,1000933836,https://api.github.com/repos/Nativu5/Gemini-FastAPI,https://github.com/Nativu5/Gemini-FastAPI/pull/7,"analysisWe need to classify the pull request as performance or non-performance. The PR implements Gemini client pool, load balancing, attach client id, health check, startup logic, documentation. Does it improve performance? It adds a client pool for load balancing, which could improve performance by distributing load. But the primary purpose? The title: ""Implement Gemini client pool"". The description: support multiple Gemini clients, add GeminiClientPool for load balancing, attach client id when storing conversations, acquire chat sessions from the pool, update health check and startup logic, document multi-client configuration, clarify that client ids must remain stable. This is about adding a pool for load balancing, which is performance improvement. It might also be about scaling, but it's about performance. The keywords: load balancing, performance. So classify as 'performance'.

Thus answer: performance.assistantfinalperformance",1,0.07839327825677024,False
3265921844,236,Implement connection pooling for LLM classes to optimize performance,"# Implement connection pooling for LLM classes to optimize performance

## Summary

This PR implements HTTP connection pooling for both OpenAI and LiteLLM classes to reduce connection establishment overhead and improve performance for concurrent LLM requests. The implementation addresses the performance issues identified in the `TaskManager._run_llm_task()` method that was taking 299ms and triggering asyncio warnings.

**Key Changes:**
- Added `ConnectionPoolManager` singleton for shared HTTP connection pools across all LLM instances
- Modified `OpenAiLLM` to use pooled `AsyncOpenAI` clients with shared aiohttp sessions
- Added connection pooling infrastructure to `LiteLLM` class (limited by library constraints)
- Integrated connection cleanup in `TaskManager.handle_cancellation()` for proper resource management
- Configured pools with optimal settings: 100 total connections, 20 per host, 30s keepalive timeout

## Review & Testing Checklist for Human

**âš ï¸ Risk Level: YELLOW** - Core functionality should work but requires thorough testing due to limited test coverage

- [ ] **Test end-to-end LLM functionality** - Verify both OpenAI and LiteLLM classes work correctly with real API calls (most critical)
- [ ] **Validate connection pooling effectiveness** - Use network monitoring to confirm connections are being reused and pooled properly
- [ ] **Test error handling scenarios** - Verify graceful fallback when connection pooling fails and proper cleanup on errors
- [ ] **Performance verification** - Measure actual performance improvement and ensure no regressions in LLM response times
- [ ] **Resource cleanup testing** - Test application shutdown and task cancellation to ensure no connection leaks

**Recommended Test Plan:**
1. Run the application with debug logging enabled to see connection pool creation messages
2. Make multiple concurrent LLM requests and monitor connection reuse
3. Test both OpenAI and LiteLLM providers with real API keys
4. Verify the original 299ms asyncio warnings are reduced
5. Test graceful shutdown and task cancellation scenarios

---

### Diagram

```mermaid
%%{ init : { ""theme"" : ""default"" }}%%
graph TB
    TaskManager[""bolna/agent_manager/<br/>task_manager.py""]:::major-edit
    BaseLLM[""bolna/llms/<br/>llm.py""]:::major-edit
    OpenAiLLM[""bolna/llms/<br/>openai_llm.py""]:::major-edit
    LiteLLM[""bolna/llms/<br/>litellm.py""]:::minor-edit
    
    
    TaskManager -->|""creates & manages""| OpenAiLLM
    TaskManager -->|""creates & manages""| LiteLLM
    TaskManager -->|""cleanup_connections()""| BaseLLM
    
    OpenAiLLM -->|""inherits from""| BaseLLM
    LiteLLM -->|""inherits from""| BaseLLM
    
    BaseLLM -->|""manages""| ConnectionPoolManager[""ConnectionPoolManager<br/>(singleton)""]:::major-edit
    ConnectionPoolManager -->|""provides""| HTTPSessions[""aiohttp.ClientSession<br/>(pooled)""]:::context
    
    subgraph Legend
        L1[Major Edit]:::major-edit
        L2[Minor Edit]:::minor-edit  
        L3[Context/No Edit]:::context
    end

    classDef major-edit fill:#90EE90
    classDef minor-edit fill:#87CEEB
    classDef context fill:#FFFFFF
```

### Notes

**âš ï¸ Testing Limitations:** Due to missing dependencies in the development environment, the LLM classes couldn't be fully tested with real API calls. The connection pooling infrastructure was verified in isolation, but the actual integration with OpenAI and LiteLLM APIs requires human testing.

**Technical Concerns:**
- OpenAI client session injection uses private attributes (`client._client._session`) which may be fragile across library versions
- LiteLLM shows diagnostic errors that couldn't be fully resolved - the integration may need refinement
- Performance benefits are theoretical until measured with real workloads

**Link to Devin run:** https://app.devin.ai/sessions/30cf723242b7483d9d5a15d1d169a7a3  
**Requested by:** @prateeksachan",Devin,158243242,devin-ai-integration[bot],open,2025-07-26T17:34:50Z,,,846923721,https://api.github.com/repos/bolna-ai/bolna,https://github.com/bolna-ai/bolna/pull/236,"analysisWe need to classify as 'performance' or 'non-performance'. The PR title: ""Implement connection pooling for LLM classes to optimize performance"". The body describes performance improvements, connection pooling, reducing overhead, etc. So it's performance. So output 'performance'.assistantfinalperformance",1,0.07761735889013957,False
3258871327,3648,Implement HTTP connection pooling for WxPayServiceApacheHttpImpl,"This PR implements HTTP connection pooling for `WxPayServiceApacheHttpImpl` to address performance issues caused by creating new HttpClient instances for each request.

## Problem
The current implementation creates a new `HttpClient` for every HTTP request:
```java
HttpClientBuilder httpClientBuilder = this.createHttpClientBuilder(useKey);
try (CloseableHttpClient httpClient = httpClientBuilder.build()) {
    // Make request and close client
}
```

This approach causes:
- Unnecessary overhead from frequent connection establishment/teardown
- Poor performance in high-concurrency scenarios
- Inefficient resource utilization

## Solution
Implemented connection pooling using Apache HttpClient's `PoolingHttpClientConnectionManager`:

### Key Changes
1. **Added connection pool configuration to WxPayConfig**:
   - `maxConnTotal` (default: 20) - Maximum total connections
   - `maxConnPerRoute` (default: 10) - Maximum connections per route
   - Separate pooled clients for SSL and non-SSL requests

2. **Enhanced WxPayConfig with pooled client initialization**:
   ```java
   public CloseableHttpClient initHttpClient() throws WxPayException
   public CloseableHttpClient initSslHttpClient() throws WxPayException
   ```

3. **Modified request methods to use pooled clients**:
   ```java
   CloseableHttpClient httpClient = this.createHttpClient(useKey);
   // Reuse pooled client instead of creating new one
   ```

### Features
- **Backward Compatible**: Zero breaking changes, existing code works without modification
- **SSL Support**: Separate connection pool for SSL requests with proper certificate handling
- **Proxy Support**: Maintains existing proxy configuration functionality
- **Customizable**: Supports existing `HttpClientBuilderCustomizer` interface
- **Configurable**: Adjustable pool size parameters

### Performance Benefits
- Eliminates per-request HttpClient creation overhead
- Enables HTTP connection reuse and keep-alive
- Better resource management for high-concurrency applications
- Improved scalability for payment processing workflows

### Usage
Connection pooling is automatically enabled with sensible defaults:
```java
WxPayConfig config = new WxPayConfig();
config.setMaxConnTotal(50);      // Optional: customize max connections
config.setMaxConnPerRoute(20);   // Optional: customize per-route limit

WxPayServiceApacheHttpImpl payService = new WxPayServiceApacheHttpImpl();
payService.setConfig(config);
// All requests now use connection pooling automatically
```

Fixes #3640.

<!-- START COPILOT CODING AGENT TIPS -->
---

ğŸ’¬ Share your feedback on Copilot coding agent for the chance to win a $200 gift card! Click [here](https://survey.alchemer.com/s3/8343779/Copilot-Coding-agent) to start the survey.",Copilot,198982749,Copilot,open,2025-07-24T07:42:17Z,,,49122742,https://api.github.com/repos/binarywang/WxJava,https://github.com/binarywang/WxJava/pull/3648,"analysisWe need to classify the pull request as 'performance' or 'non-performance'. The PR description clearly states it's about implementing HTTP connection pooling to improve performance. It mentions performance issues, overhead, high concurrency, etc. So it's a performance PR. So output 'performance'.assistantfinalperformance",1,0.07532996875974111,False
3122154581,1623,[alpha_factory] switch business bridge to httpx,"## Summary
- replace `requests` with `httpx.AsyncClient` in business demo bridge
- adjust wait helper and tool functions to await httpx calls
- patch tests to mock `AsyncClient`
- harden `llm_provider` metrics registration

## Testing
- `python check_env.py --auto-install`
- `pytest tests/test_openai_bridge_integration.py::TestBusinessAgentIntegration::test_list_agents -q`
- `pytest tests/test_llm_cache.py::TestLLMCacheLRU::test_eviction -q`
- `pytest -q` *(fails: Duplicated timeseries, ImportError, KeyboardInterrupt)*

------
https://chatgpt.com/codex/tasks/task_e_6841d8e9b6208333a711c9881a616e2b",OpenAI_Codex,24208299,MontrealAI,closed,2025-06-05T18:06:36Z,2025-06-05T18:06:47Z,2025-06-05T18:06:47Z,922805069,https://api.github.com/repos/MontrealAI/AGI-Alpha-Agent-v0,https://github.com/MontrealAI/AGI-Alpha-Agent-v0/pull/1623,"analysisWe need to classify whether the primary purpose is performance improvement. The PR title: ""[alpha_factory] switch business bridge to httpx"". Summary: replace requests with httpx.AsyncClient in business demo bridge, adjust wait helper and tool functions to await httpx calls, patch tests to mock AsyncClient, harden llm_provider metrics registration. This is about switching to async HTTP client, likely for performance or concurrency. But is the primary purpose performance? The description mentions ""switch business bridge to httpx"" and ""adjust wait helper and tool functions to await httpx calls"". That suggests moving to async to improve performance or responsiveness. However, the PR also patches tests and hardens metrics registration. The main change is replacing requests with httpx.AsyncClient. That is likely to improve performance by enabling async calls, reducing blocking. So classification: performance. There's no mention of memory, optimization, speed, etc. But the change is to use async client, which is performance improvement. So label 'performance'.assistantfinalperformance",1,0.07033578893417433,False
