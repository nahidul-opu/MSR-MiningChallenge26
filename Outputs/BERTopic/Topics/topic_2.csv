id,number,title,body,agent,user_id,user,state,created_at,closed_at,merged_at,repo_id,repo_url,html_url,llm_output,Topic,Probability,Representative_document
3138362649,1847,‚ú® feat: implement MODEXP precompile (EIP-198) with EIP-2565 gas optimization,"## Summary

- Implements MODEXP precompile at address 0x05 
- Full EIP-198 compliance with modular exponentiation
- EIP-2565 gas calculation optimization
- Comprehensive test suite with edge cases
- Gas overflow protection and input validation

## Key Features

- Efficient gas calculation with complexity analysis
- Proper handling of zero cases (0^0 = 1, base^0 = 1)
- Input validation and DoS protection
- Big-endian integer parsing for Ethereum compatibility
- Comprehensive test coverage including edge cases

## Test Results

All Zig tests pass ()

ü§ñ Generated with [Claude Code](https://claude.ai/code)

Co-Authored-By: Claude <noreply@anthropic.com>",Claude_Code,35039927,roninjin10,closed,2025-06-11T23:40:21Z,2025-06-12T02:24:02Z,2025-06-12T02:24:02Z,601475124,https://api.github.com/repos/evmts/tevm-monorepo,https://github.com/evmts/tevm-monorepo/pull/1847,"analysisWe need to classify as performance or non-performance. The PR implements MODEXP precompile with gas optimization. The description mentions ""EIP-2565 gas optimization"" and ""Efficient gas calculation"". This is about gas calculation optimization, which is performance improvement. The keywords include ""optimization"", ""gas optimization"", ""efficient"". So it's performance. So label 'performance'.assistantfinalperformance",2,1.0,False
3254647682,59071,skip unnecessary alias-check in collect(::AbstractArray) from copyto\!,"As discussed on Slack with @MasonProtter & @jakobnissen, `collect` currently does a usually cheap - but sometimes expensive - aliasing check (via `unalias`->`mightalias`->`dataid` -> `objectid`) before copying contents over; this check is unnecessary, however, since the source array is newly created and cannot possibly alias the input.

This PR fixes that by swapping from `copyto\!` to `copyto_unaliased\!` in the `_collect_indices` implementations where the swap is straightforward (e.g., it is not so straightforward for the fallback `_collect_indices(indsA, A)`, so I skipped it there).

This improves the following example substantially:
```julia
struct GarbageVector{N} <: AbstractVector{Int}
    v :: Vector{Int}
    garbage :: NTuple{N, Int}
end
GarbageVector{N}(v::Vector{Int}) where N = GarbageVector{N}(v, ntuple(identity, Val(N)))
Base.getindex(gv::GarbageVector, i::Int) = gv.v[i]
Base.size(gv::GarbageVector) = size(gv.v)

using BenchmarkTools
v = rand(Int, 10)
gv = GarbageVector{100}(v)
@btime collect($v);  # 30 ns (v1.10.4)  -> 30 ns (PR)
@btime collect($gv); # 179 ns (v1.10.4) -> 30 ns (PR)
```

Rebased version of JuliaLang/julia#55748

ü§ñ Generated with [Claude Code](https://claude.ai/code)

Co-Authored-By: Claude <noreply@anthropic.com>",Claude_Code,1814174,ChrisRackauckas,closed,2025-07-23T02:52:46Z,2025-07-23T23:55:54Z,,1644196,https://api.github.com/repos/JuliaLang/julia,https://github.com/JuliaLang/julia/pull/59071,"analysisWe need to classify whether the primary purpose is to improve performance. The PR title: ""skip unnecessary alias-check in collect(::AbstractArray) from copyto!"" The body explains that they skip alias-check, improving performance. So it's a performance improvement. So label 'performance'.assistantfinalperformance",2,1.0,False
3138341820,2255,Optimize BigInt hex conversion,"## Summary
- improve `BigInt::to_hex` performance by using `StringBuilder`

## Testing
- `moon info`
- `moon test` *(fails: no output)*

------
https://chatgpt.com/codex/tasks/task_e_68499a1746208320b6d98eb4cbce0581",OpenAI_Codex,747051,bobzhang,closed,2025-06-11T23:21:52Z,2025-06-23T06:46:51Z,2025-06-23T06:46:51Z,749681472,https://api.github.com/repos/moonbitlang/core,https://github.com/moonbitlang/core/pull/2255,"analysisWe need to classify the pull request as performance or non-performance. The title: ""Optimize BigInt hex conversion"". Summary: ""improve BigInt::to_hex performance by using StringBuilder"". That is clearly a performance improvement. So classification: 'performance'.assistantfinalperformance",2,1.0,False
3146329050,25066,[WIP] [tracking] Improve Sequence operator handling,"Analyzing sequence operator handling efficiency improvements in ONNX Runtime

**Overview**: The current sequence operator implementation relies heavily on tensor copies as noted in the TODO comment. This PR aims to improve efficiency by reducing unnecessary tensor copying through better use of move semantics and avoiding redundant allocations.

**Plan**:
- [x] Analyze current sequence operator implementation and identify inefficiencies
- [x] Understand the current test structure and codebase organization
- [x] Optimize SequenceInsert to reduce tensor copying where possible
- [x] Optimize SequenceConstruct to use move semantics when appropriate
- [x] Add CreateTensorOrtValue function to enable direct OrtValue creation with move semantics
- [ ] Create performance benchmark tests to measure current copying overhead
- [ ] Investigate SequenceAt optimization opportunities (limited due to operational requirements)
- [ ] Run existing tests to ensure no regression
- [ ] Validate performance improvements with benchmarks

**Key optimizations implemented**:
- Added `CreateTensorOrtValue()` function that creates OrtValue directly with move semantics instead of creating Tensor first then converting
- Updated SequenceInsert and SequenceConstruct to use the optimized function
- This reduces the number of copy operations and temporary object creation
- Maintains compatibility with DataTransferManager for cross-EP support

**Note**: SequenceErase is already well-optimized as it avoids copying non-erased tensors. SequenceAt requires copying due to operational requirements but uses efficient DataTransferManager.

Fixes #18355.

<!-- START COPILOT CODING AGENT TIPS -->
---

üí° You can make Copilot smarter by setting up custom instructions, customizing its development environment and configuring Model Context Protocol (MCP) servers. Learn more [Copilot coding agent tips](https://gh.io/copilot-coding-agent-tips) in the docs.",Copilot,198982749,Copilot,open,2025-06-14T16:00:00Z,,,156939672,https://api.github.com/repos/microsoft/onnxruntime,https://github.com/microsoft/onnxruntime/pull/25066,"analysisWe need to classify as performance or non-performance. The PR title: ""[WIP] [tracking] Improve Sequence operator handling"". Body: mentions reducing tensor copying, move semantics, performance improvements. Keywords: efficiency, reduce copying, performance improvements. So it's performance. So label 'performance'.assistantfinalperformance",2,1.0,False
3220760486,299,Reduce memory allocations in display code,"## Summary
- store timezone data as flash strings
- place Serial message literals in flash using `F()`
- store WebUI error messages in flash
",OpenAI_Codex,2579336,jniebuhr,closed,2025-07-10T21:25:13Z,2025-07-13T17:03:05Z,2025-07-13T17:03:05Z,876327247,https://api.github.com/repos/jniebuhr/gaggimate,https://github.com/jniebuhr/gaggimate/pull/299,"analysisWe need to classify as performance or non-performance. The PR title: ""Reduce memory allocations in display code"". Body: store timezone data as flash strings, place Serial message literals in flash using F(), store WebUI error messages in flash. This is about memory usage, reducing allocations, using flash strings to reduce RAM usage. That is performance improvement (memory optimization). So classification: performance.assistantfinalperformance",2,1.0,False
3220636286,1047,Add helper for ragged paged attention,"## Summary
- gate optional TPU ragged paged attention import
- introduce `ragged_paged_attention` helper to dispatch to TPU or reference code
- use new helper from `paged_decode`

## Testing
- `pre-commit run --files src/levanter/layers/attention.py`
- `pytest tests -m ""not entry and not slow and not ray""` *(fails: ModuleNotFoundError for jax, equinox, etc.)*

------
https://chatgpt.com/codex/tasks/task_e_687023c7a5fc83319a642eb2548a03f7",OpenAI_Codex,9633,dlwh,closed,2025-07-10T20:41:57Z,2025-07-10T20:42:21Z,2025-07-10T20:42:21Z,496005961,https://api.github.com/repos/stanford-crfm/levanter,https://github.com/stanford-crfm/levanter/pull/1047,"analysisWe need to classify whether the primary purpose of the PR is to improve performance. The PR adds helper for ragged paged attention, gating optional TPU ragged paged attention import, introduces ragged_paged_attention helper to dispatch to TPU or reference code, use new helper from paged_decode. This seems to be about adding a helper for ragged paged attention, likely to improve performance by using TPU-specific implementation. The description mentions ""gate optional TPU ragged paged attention import"" and ""use new helper from paged_decode"". This is likely a performance improvement: ragged paged attention is a performance optimization for attention. So classification: 'performance'.

Check if any other context: The PR is about adding a helper for ragged paged attention, which is a performance optimization. So yes, performance.assistantfinalperformance",2,1.0,False
3183021016,20687,Implement array length minify optimization,"## Summary
- optimize property accesses on array literals when minifying
- add a bundler test for array length folding

## Testing
- `bun bd test test/bundler/bundler_minify.test.ts` *(fails: Configuring incomplete, errors occurred)*

------
https://chatgpt.com/codex/tasks/task_e_685ea751ffa4832886a1a0b55859074c",OpenAI_Codex,709451,Jarred-Sumner,closed,2025-06-27T14:21:26Z,2025-07-20T08:39:28Z,,357728969,https://api.github.com/repos/oven-sh/bun,https://github.com/oven-sh/bun/pull/20687,"analysisWe need to classify the pull request as performance or non-performance. The title: ""Implement array length minify optimization"". Summary: ""optimize property accesses on array literals when minifying"". This is about minification, which is a performance optimization? It's about code size and speed? It's about minifying, which reduces code size, potentially improving load time. The keywords: ""optimize"", ""minify"", ""array length folding"". This is about optimization of minification, which is a performance improvement. The classification: performance. So output ""performance"".assistantfinalperformance",2,1.0,False
3222683231,7751,Improve Java compiler string concatenation,"## Summary
- enhance Java backend to use `StringBuilder` when translating string concatenations
- add new follow-up items to Java machine README

## Testing
- `go test ./...`

------
https://chatgpt.com/codex/tasks/task_e_6870f08bdb6083208d78e4956c95bd2a",OpenAI_Codex,1218621,tamnd,closed,2025-07-11T12:09:49Z,2025-07-11T12:10:02Z,2025-07-11T12:10:02Z,985853139,https://api.github.com/repos/mochilang/mochi,https://github.com/mochilang/mochi/pull/7751,"analysisWe need to classify as performance or non-performance. The PR title: ""Improve Java compiler string concatenation"". Summary: ""enhance Java backend to use StringBuilder when translating string concatenations"". That is a performance improvement: using StringBuilder reduces string concatenation overhead. So classification: performance.assistantfinalperformance",2,1.0,False
3104406142,823,Improve vector performance,"## Summary
- cache tail size in `PersistentVector` and `TransientVector`
- update tail size when modifying transient vectors
",OpenAI_Codex,5256287,Chemaclass,closed,2025-05-30T22:38:34Z,2025-05-30T23:00:10Z,2025-05-30T23:00:10Z,257098489,https://api.github.com/repos/phel-lang/phel-lang,https://github.com/phel-lang/phel-lang/pull/823,"analysisWe need to classify as performance or non-performance. The title: ""Improve vector performance"". Body: caching tail size, update tail size when modifying transient vectors. This is about performance optimization. So classification: performance.assistantfinalperformance",2,1.0,False
3137138306,2253,Optimize string operations,"## Summary
- avoid creating intermediate strings by using `write_substring`
- remove unused TODO comments in string methods
- update array joining helpers to directly append views

## Testing
- `moon fmt`
- `moon info`

------
https://chatgpt.com/codex/tasks/task_e_6849936bc8548320a98083b692ec4327",OpenAI_Codex,747051,bobzhang,closed,2025-06-11T14:51:38Z,2025-06-11T15:33:27Z,2025-06-11T15:33:26Z,749681472,https://api.github.com/repos/moonbitlang/core,https://github.com/moonbitlang/core/pull/2253,"analysisWe need to classify as performance or non-performance. The PR title: ""Optimize string operations"". Body: mentions avoid creating intermediate strings, use write_substring, remove unused TODO comments, update array joining helpers to directly append views. This is about performance optimization of string operations. So classification: performance.assistantfinalperformance",2,1.0,False
3234031765,1066,Optimize page table slicing,"## Summary
- optimize loops in page_table
- slice updated sequences and token counts by num_active
- update tests for new PageBatchInfo signature

## Testing
- `pre-commit run --files src/levanter/layers/page_table.py`
- `pytest tests/test_page_table.py -q`

------
https://chatgpt.com/codex/tasks/task_e_6876ec7ca3dc83318ce01950914aa611",OpenAI_Codex,9633,dlwh,closed,2025-07-16T00:16:42Z,2025-07-16T00:16:51Z,2025-07-16T00:16:50Z,496005961,https://api.github.com/repos/stanford-crfm/levanter,https://github.com/stanford-crfm/levanter/pull/1066,"analysisWe need to classify as performance or non-performance. The PR title: ""Optimize page table slicing"". Summary: ""optimize loops in page_table"", ""slice updated sequences and token counts by num_active"", ""update tests for new PageBatchInfo signature"". This is about optimizing loops, slicing, performance. So it's performance. So output 'performance'.assistantfinalperformance",2,1.0,False
3071827885,120,Improve string/comment parsing performance,"## Summary
- avoid repeated concatenations while parsing strings and comments
- accumulate characters in lists and join at the end

## Testing
- `python3 -m pytest -q` *(fails: No module named pytest)*",OpenAI_Codex,4247706,mangiucugna,closed,2025-05-18T14:27:00Z,2025-05-18T14:37:56Z,,688141049,https://api.github.com/repos/mangiucugna/json_repair,https://github.com/mangiucugna/json_repair/pull/120,"analysisWe need to classify the pull request as 'performance' or 'non-performance'. The title: ""Improve string/comment parsing performance"". The body: mentions avoid repeated concatenations, accumulate characters in lists and join at the end. This is clearly performance improvement. So classification: 'performance'.assistantfinalperformance",2,1.0,False
3223281368,82,Use of generators to reduce memory usage via a LazyMultiSet,"Also converts almost all operators to use them wherever possible.

We should benchmark this against main to see how what it does with the memory usage and allocations.",Cursor,31130,samwillis,open,2025-07-11T15:19:11Z,,,884778513,https://api.github.com/repos/electric-sql/d2ts,https://github.com/electric-sql/d2ts/pull/82,"analysisWe need to classify the pull request as performance or non-performance. The title: ""Use of generators to reduce memory usage via a LazyMultiSet Also converts almost all operators to use them wherever possible."" Body: ""We should benchmark this against main to see how what it does with the memory usage and allocations.""

Keywords: memory usage, reduce memory usage, benchmark, allocations. This is performance improvement. So classify as 'performance'.assistantfinalperformance",2,1.0,False
3264767865,1102,Optimize careless LM evaluation,"## Summary
- incorporate `DataLoader` for text windows
- store character ranges alongside examples
- add configuration options to trade memory for throughput
- batch suffix probability gathering to reduce host transfers

## Testing
- `python -m py_compile src/levanter/main/eval_careless_lm.py`
- `pytest -q` *(fails: ImportError while importing tests)*

------
https://chatgpt.com/codex/tasks/task_e_68841b9691448327bde32d2d8d99cfb0",OpenAI_Codex,30680697,ahmeda14960,open,2025-07-26T00:44:55Z,,,496005961,https://api.github.com/repos/stanford-crfm/levanter,https://github.com/stanford-crfm/levanter/pull/1102,"analysisWe need to classify as performance or non-performance. The PR title: ""Optimize careless LM evaluation"". Summary: incorporate DataLoader for text windows, store character ranges, add config options to trade memory for throughput, batch suffix probability gathering to reduce host transfers. This is clearly performance optimization. So label 'performance'.assistantfinalperformance",2,1.0,False
3099945080,809,Optimize vector iteration,"## Summary
- reduce overhead in RangeIterator
",OpenAI_Codex,5256287,Chemaclass,closed,2025-05-29T10:50:20Z,2025-05-29T10:55:18Z,2025-05-29T10:55:18Z,257098489,https://api.github.com/repos/phel-lang/phel-lang,https://github.com/phel-lang/phel-lang/pull/809,"analysisWe need to classify as performance or non-performance. The title: ""Optimize vector iteration"". Summary: ""- reduce overhead in RangeIterator"". This is about performance optimization. So label 'performance'.assistantfinalperformance",2,1.0,False
3142406085,2266,Improve retain performance,"## Summary
- refactor `Array::retain` to avoid repeated length lookups and reads

## Testing
- `moon fmt`
- `moon info`
- `moon check`
- `moon test`

------
https://chatgpt.com/codex/tasks/task_e_684ba864a968832085bc835c8821880c",OpenAI_Codex,747051,bobzhang,closed,2025-06-13T06:21:03Z,2025-06-13T07:36:47Z,2025-06-13T07:36:47Z,749681472,https://api.github.com/repos/moonbitlang/core,https://github.com/moonbitlang/core/pull/2266,"analysisWe need to classify as performance or non-performance. The title: ""Improve retain performance"". The body: refactor Array::retain to avoid repeated length lookups and reads. That is a performance improvement. So classification: performance.assistantfinalperformance",2,1.0,False
3142207549,2265,Improve list rev_fold performance,"## Summary
- avoid array allocation in list `rev_fold`
- implement direct recursion for `rev_foldi`
- run `moon fmt` and `moon info`

## Testing
- `moon fmt`
- `moon info`

------
https://chatgpt.com/codex/tasks/task_e_684b84f0d1148320bbfa1f922c8f1f42",OpenAI_Codex,747051,bobzhang,closed,2025-06-13T04:24:38Z,2025-06-15T12:05:50Z,,749681472,https://api.github.com/repos/moonbitlang/core,https://github.com/moonbitlang/core/pull/2265,"analysisWe need to classify as 'performance' or 'non-performance'. The title: ""Improve list rev_fold performance"". The body mentions avoid array allocation, direct recursion, etc. It's clearly performance improvement. So label 'performance'.assistantfinalperformance",2,1.0,False
3207776307,1040,Add paged decode helper to Llama transformer,"## Summary
- add helper `_forward` to reuse logic between decoding calls
- implement per-layer paged decode using the new helper
- handle alternate axis names in `default_ragged_paged_attention`
- rename variables flagged by ruff

## Testing
- `ruff check src/levanter/models/llama.py src/levanter/layers/attention.py`
- `black src/levanter/models/llama.py src/levanter/layers/attention.py`
- `pytest -q tests/test_paged_attention.py::test_ragged_paged_attention_single_seq -vv`

------
https://chatgpt.com/codex/tasks/task_e_6868cef10c0c83318ca7130fcbfc9d96",OpenAI_Codex,9633,dlwh,closed,2025-07-07T06:56:16Z,2025-07-10T23:37:13Z,,496005961,https://api.github.com/repos/stanford-crfm/levanter,https://github.com/stanford-crfm/levanter/pull/1040,"analysisWe need to classify whether the primary purpose of the PR is to improve performance. The PR adds a paged decode helper to Llama transformer, implements per-layer paged decode, handles alternate axis names, rename variables flagged by ruff. The test is about paged attention. This seems to be about performance optimization: paged decode is a performance improvement for transformer decoding. So classification: 'performance'.assistantfinalperformance",2,1.0,False
3070936351,4050,chore: Improve memory usage when sharing results,"## Summary
- add batched sample and counting helpers on `Eval`
- stream results in `sendChunkedResults`
",OpenAI_Codex,1080611,sklein12,closed,2025-05-17T17:00:54Z,2025-05-19T21:58:14Z,2025-05-19T21:58:14Z,633927609,https://api.github.com/repos/promptfoo/promptfoo,https://github.com/promptfoo/promptfoo/pull/4050,"analysisWe need to classify as performance or non-performance. The title: ""chore: Improve memory usage when sharing results"". Body: add batched sample and counting helpers on Eval, stream results in sendChunkedResults. This is about memory usage, which is performance. So classify as 'performance'.assistantfinalperformance",2,1.0,False
3141942061,2262,Optimize loops by caching length,"## Summary
- optimize loops in `Bytes::to_array` and `Bytes` addition
- reduce `length()` calls in array reversal functions
- cache size in `Deque::to_array`

## Testing
- `moon fmt`
- `moon info`
- `moon check --verbose`
- `moon test`

------
https://chatgpt.com/codex/tasks/task_e_684ae1d2a7d483209a8fab3f9a02c07b",OpenAI_Codex,747051,bobzhang,closed,2025-06-13T01:11:54Z,2025-06-13T01:36:06Z,2025-06-13T01:36:06Z,749681472,https://api.github.com/repos/moonbitlang/core,https://github.com/moonbitlang/core/pull/2262,"analysisWe need to classify as performance or non-performance. The PR title: ""Optimize loops by caching length"". Summary: optimize loops in Bytes::to_array and Bytes addition, reduce length() calls in array reversal functions, cache size in Deque::to_array. This is clearly performance optimization. So label 'performance'.assistantfinalperformance",2,1.0,False
3200979351,1429,Optimize logits worker I/O,"## Summary
- cut per-element conversions in sliding logits worker
- store logits batches as record batches in Parquet
- compute log-probs on device before moving tensors to CPU

## Testing
- `pre-commit run --files marin/generation/sliding_logits.py` *(fails: command not found)*
- `pytest -q` *(fails: ModuleNotFoundError: No module named 'ray')*

------
https://chatgpt.com/codex/tasks/task_e_6867227b07bc8327916802ace9a0cc74",OpenAI_Codex,30680697,ahmeda14960,closed,2025-07-04T00:58:38Z,2025-07-04T01:12:51Z,2025-07-04T01:12:51Z,775839592,https://api.github.com/repos/marin-community/marin,https://github.com/marin-community/marin/pull/1429,"analysisWe need to classify as performance or non-performance. The PR title: ""Optimize logits worker I/O"". Summary: cut per-element conversions, store logits batches as record batches in Parquet, compute log-probs on device before moving tensors to CPU. This is about performance optimization: I/O, conversions, compute on device. So classification: performance.assistantfinalperformance",2,1.0,False
3201015713,1431,Speed up P(z) and fix Parquet write,"## Summary
- compute P(z) on device before transferring logits to CPU
- build rows with explicit dtype to satisfy PyArrow

## Testing
- `ruff check --output-format concise marin/generation/sliding_logits.py`
- `black --check marin/generation/sliding_logits.py`
- `pytest -q` *(fails: ModuleNotFoundError: No module named 'ray')*


------
https://chatgpt.com/codex/tasks/task_e_6867227b07bc8327916802ace9a0cc74",OpenAI_Codex,30680697,ahmeda14960,closed,2025-07-04T01:20:53Z,2025-07-04T01:23:57Z,2025-07-04T01:23:57Z,775839592,https://api.github.com/repos/marin-community/marin,https://github.com/marin-community/marin/pull/1431,"analysisWe need to classify as performance or non-performance. The PR title: ""Speed up P(z) and fix Parquet write"". Summary: compute P(z) on device before transferring logits to CPU; build rows with explicit dtype to satisfy PyArrow. This is about speed up P(z) and fix Parquet write. So it's performance improvement. So label 'performance'.assistantfinalperformance",2,1.0,False
3267314311,1489,Fix inference OOM by streaming dataset,"## Summary
- avoid loading entire dataset in memory when running classification inference
- stream input shards in batches and write results incrementally

## Testing
- `pytest tests/processing/tokenize -q`


------
https://chatgpt.com/codex/tasks/task_e_68866679275483318b55e211a2a91eee",OpenAI_Codex,9633,dlwh,open,2025-07-27T18:17:29Z,,,775839592,https://api.github.com/repos/marin-community/marin,https://github.com/marin-community/marin/pull/1489,"analysisWe need to classify whether the primary purpose is performance improvement. The PR title: ""Fix inference OOM by streaming dataset"". Body: avoid loading entire dataset in memory, stream input shards in batches, write results incrementally. This is about memory usage, performance. It's about reducing memory consumption, which is performance improvement. So classify as 'performance'.assistantfinalperformance",2,1.0,False
3226180108,1056,Improve sample_lm generation,"## Summary
- jit-loop the autoregressive decode in `sample_lm`
- remove unused timing logic

## Testing
- `pre-commit run --all-files`

------
https://chatgpt.com/codex/tasks/task_e_6872fb5b14248331a6da4edac2e82635",OpenAI_Codex,9633,dlwh,closed,2025-07-13T06:20:47Z,2025-07-19T06:39:31Z,,496005961,https://api.github.com/repos/stanford-crfm/levanter,https://github.com/stanford-crfm/levanter/pull/1056,"analysisWe need to classify as performance or non-performance. The PR title: ""Improve sample_lm generation"". Body: ""jit-loop the autoregressive decode in sample_lm, remove unused timing logic"". This is about performance: JIT loop, remove timing logic. So classification: performance.assistantfinalperformance",2,1.0,False
3203674177,380,Optimize dataset storage with pyarrow,"Implement PyArrow-based incremental checkpoint storage with deduplication to reduce disk usage and enhance debugging capabilities.

Closes #221 

Previously, DocETL stored complete intermediate datasets as separate JSON files for each operation, leading to significant storage redundancy and inefficient disk usage, particularly when data records were identical or overlapped across pipeline steps. This change introduces a new architecture that stores unique records once in a central PyArrow dataset and uses an index to reference them, drastically reducing the overall storage footprint. It also adds programmatic inspection tools and documentation for easier pipeline debugging.",Cursor,6224969,shreyashankar,closed,2025-07-04T20:45:02Z,2025-07-07T19:32:05Z,,826111692,https://api.github.com/repos/ucbepic/docetl,https://github.com/ucbepic/docetl/pull/380,"analysisWe need to classify as performance or non-performance. The PR title: ""Optimize dataset storage with pyarrow"". Body: describes reducing disk usage, deduplication, storage footprint. This is about storage efficiency, disk usage, deduplication. That is performance improvement? It's about storage efficiency, which is a performance aspect (disk usage, speed of storage). The guidelines: performance-related PRs may contain keywords like memory, optimization, leak, speed, perf, etc. This PR mentions ""reduce disk usage"" and ""enhance debugging capabilities"". It's about storage optimization. That qualifies as performance. So classify as 'performance'.assistantfinalperformance",2,1.0,False
3174654401,4855,ref(relay): Use `to_owned` instead of `to_string`.,"~~AI~~ :shipit: 

Another W for not using AI, Cursor replaces 105 occurrences for a few dollars and 30 minutes agent, me googling syntax of ripgrep and xargs and writing the regex ~10 minutes and it finds 1000+ more occurrences. Cursor also only did the trivial case (which the regex just does better). 

#skip-changelog",Cursor,255721,Dav1dde,closed,2025-06-25T08:11:01Z,2025-06-25T08:49:37Z,2025-06-25T08:49:37Z,117757315,https://api.github.com/repos/getsentry/relay,https://github.com/getsentry/relay/pull/4855,"analysisWe need to classify whether the primary purpose is performance. The PR title: ""ref(relay): Use `to_owned` instead of `to_string`."" Body: some commentary about using Cursor, etc. The change is replacing to_string with to_owned. That is a performance improvement? to_owned vs to_string: to_owned creates a new owned string from a borrowed string slice? Actually to_string creates a new String by cloning the string slice? to_owned also clones? In Rust, to_owned is similar to clone for types that implement ToOwned. For &str, to_owned is same as to_string? But to_owned may be more efficient? I think to_owned is more efficient because it uses the underlying allocation? But the change is likely a small performance improvement. The PR is about refactoring code to use to_owned instead of to_string. That is a performance improvement. So classify as 'performance'.assistantfinalperformance",2,1.0,False
3233988388,1065,Optimize PageTable allocation loop,"## Summary
- page_table: iterate allocations only for updated sequences
- skip allocation for invalid sequence IDs

## Testing
- `pre-commit run --files src/levanter/layers/page_table.py`
- `pytest tests/test_paged_attention.py -k test_ragged_paged_attention_incremental_single_seq -vv` *(fails: ValueError during broadcasting)*

------
https://chatgpt.com/codex/tasks/task_e_6876e5422f2083318b8bb6b5937bbf81",OpenAI_Codex,9633,dlwh,closed,2025-07-15T23:48:19Z,2025-07-15T23:49:55Z,2025-07-15T23:49:55Z,496005961,https://api.github.com/repos/stanford-crfm/levanter,https://github.com/stanford-crfm/levanter/pull/1065,"analysisWe need to classify the pull request as performance or non-performance. The title: ""Optimize PageTable allocation loop"". Summary: ""page_table: iterate allocations only for updated sequences; skip allocation for invalid sequence IDs"". This is about optimizing allocation loop, likely performance improvement. Keywords: optimize, allocation loop, skip allocation. It's about performance. So classification: 'performance'.assistantfinalperformance",2,1.0,False
3189896520,1413,Add TPU-compatible logits inference,"## Summary
- run compute_logits on TPU via XLA multiprocessing
- write out per-core shards then merge
- update forward logits tutorial

## Testing
- `pre-commit run --files marin/generation/logits.py experiments/tutorials/exp1350_forward_logits.py` *(fails: command not found)*
- `pytest -q tests/quickstart-data` *(fails: ModuleNotFoundError: No module named 'ray')*

------
https://chatgpt.com/codex/tasks/task_e_6862ed00e7e883278bfe1292cfce04df",OpenAI_Codex,30680697,ahmeda14960,closed,2025-06-30T21:24:01Z,2025-06-30T21:24:29Z,2025-06-30T21:24:29Z,775839592,https://api.github.com/repos/marin-community/marin,https://github.com/marin-community/marin/pull/1413,"analysisWe need to classify whether the primary purpose is performance. The PR title: ""Add TPU-compatible logits inference"". Summary: run compute_logits on TPU via XLA multiprocessing, write out per-core shards then merge, update forward logits tutorial. This is about adding TPU compatibility, likely to improve performance by using TPU. The keywords: ""TPU"", ""XLA"", ""multiprocessing"", ""shards"", ""merge"". It's about performance improvement. So classification: 'performance'.assistantfinalperformance",2,1.0,False
3079638107,11432,Allow ParticleBuffer to shrink buffers,"## Summary
- extend `ParticleBufferOptions` with a `minSize` field
- keep track of `_minSize` and reduce allocated space when particle counts drop
- test ParticleBuffer growth and shrinkage behaviour

## Testing
- `npm test` *(fails: run-s not found)*",OpenAI_Codex,3112434,GoodBoyDigital,open,2025-05-21T09:52:11Z,,,7741856,https://api.github.com/repos/pixijs/pixijs,https://github.com/pixijs/pixijs/pull/11432,"analysisWe need to classify whether the primary purpose is performance improvement. The PR: ""Allow ParticleBuffer to shrink buffers"". Summary: extend options with minSize, keep track of _minSize and reduce allocated space when particle counts drop, test growth and shrinkage. This is about memory usage, reducing allocated space, performance. It's about memory optimization. So it's performance. So label 'performance'.assistantfinalperformance",2,0.34293882298548206,False
3048846835,11865,Make `set_len` a tokio-epoll-uring operation,"# Make `set_len` a tokio-epoll-uring operation

Fixes #11817

## Changes
- Added a new `ftruncate` operation to the tokio-epoll-uring library
- Updated the `set_len` method in `io_engine.rs` to use the new operation
- Follows the same pattern as other operations like `fsync` and `fdatasync`

## Testing
- Ran unit tests with `cargo nextest run --workspace --exclude walproposer`

Link to Devin run: https://app.devin.ai/sessions/1b08d1ef4cbe4872a0c46f7d01d6d490
Requested by: christian@neon.tech
",Devin,158243242,devin-ai-integration[bot],closed,2025-05-08T12:24:26Z,2025-05-08T12:55:44Z,,351806852,https://api.github.com/repos/neondatabase/neon,https://github.com/neondatabase/neon/pull/11865,"analysisWe need to classify as performance or non-performance. The PR title: ""Make set_len a tokio-epoll-uring operation"". The body: adds ftruncate operation, updates set_len to use new operation. This seems like adding a new operation to use epoll-uring for set_len. This likely improves performance? The change is to use a new operation for set_len, which might be more efficient. But is it explicitly performance? The description: ""Make set_len a tokio-epoll-uring operation"". That implies using epoll-uring for set_len, which is likely a performance improvement. The body doesn't mention speed, latency, etc. But the change is to use a new operation that likely improves performance. The classification guidelines: performance-related PRs may contain keywords such as memory, optimization, leak, speed, perf, stuck, lag, fast, respons, slow, latency, performance, latenc, hang, tim, throughput, compile-time-hog, instant, responsive, react, perform, wait, minor, or any other terms referring to how faster and efficiently software runs. The PR doesn't contain those keywords. But the title ""Make set_len a tokio-epoll-uring operation"" implies a change to use epoll-uring, which is a performance improvement. But the guidelines say performance-related PRs may contain keywords. But not all performance PRs contain those keywords. The guidelines are not strict. We need to decide. The change is to use epoll-uring for set_len, which is likely a performance improvement. So classify as performance.assistantfinalperformance",2,0.33558316522704634,False
3225959128,1055,Avoid network downloads in tests,"## Summary
- add `local_gpt2_tokenizer` fixture to load GPT2 tokenizer from local files
- switch GPT2-using tests to rely on this fixture
- use passthrough tokenizer for validation-set test

## Testing
- `pre-commit run --all-files`
- `pytest tests/test_supervised.py tests/test_sft.py tests/test_text.py -m ""not entry and not slow and not ray""`

------
https://chatgpt.com/codex/tasks/task_e_68719c180a4883319e292ffc9037e739",OpenAI_Codex,9633,dlwh,closed,2025-07-13T00:17:21Z,2025-07-13T16:23:35Z,2025-07-13T16:23:35Z,496005961,https://api.github.com/repos/stanford-crfm/levanter,https://github.com/stanford-crfm/levanter/pull/1055,"analysisWe need to classify whether the primary purpose is performance improvement. The PR title: ""Avoid network downloads in tests"". Summary: add local tokenizer fixture, switch tests to rely on this fixture, use passthrough tokenizer for validation-set test. This is about avoiding network downloads, making tests faster? It's about test speed, but not necessarily performance of the library. It's about making tests faster by not downloading. The classification: performance? The guidelines: performance-related PRs may contain keywords like speed, perf, etc. This PR mentions ""Avoid network downloads in tests"" which reduces test runtime. That is a performance improvement in tests. But is that considered performance? The guidelines say performance-related PRs may contain keywords such as memory, optimization, leak, speed, perf, stuck, lag, fast, respons, slow, latency, performance, latenc, hang, tim, throughput, compile-time-hog, instant, responsive, react, perform, wait, minor, or any other terms referring to how faster and efficiently software runs. This PR is about making tests faster, but it's not about the library's performance. However, it's still a performance improvement. The primary purpose is to avoid network downloads in tests, making tests faster. That is a performance improvement. So classify as 'performance'.assistantfinalperformance",2,0.3342746099501353,False
3146857680,143,Switch DeepEquals stack to ArrayDeque,"## Summary
- switch stack in `deepEquals` and `deepHashCode` from `LinkedList` to `ArrayDeque`
- update imports
- add helper to insert collections into stack

## Testing
- `mvn -q test` *(fails: `mvn` not found)*

------
https://chatgpt.com/codex/tasks/task_b_684dfa8f5374832a98fa248a2db0ddf5",OpenAI_Codex,5373774,jdereg,closed,2025-06-14T22:48:02Z,2025-06-14T22:48:25Z,2025-06-14T22:48:25Z,12670630,https://api.github.com/repos/jdereg/java-util,https://github.com/jdereg/java-util/pull/143,"analysisWe need to classify the pull request as performance or non-performance. The PR title: ""Switch DeepEquals stack to ArrayDeque"". Summary: switch stack in deepEquals and deepHashCode from LinkedList to ArrayDeque, update imports, add helper to insert collections into stack. This is about using ArrayDeque instead of LinkedList. That likely improves performance because ArrayDeque is more efficient for stack operations. So it's a performance improvement. So classification: 'performance'.assistantfinalperformance",2,0.3007207668176187,False
3074502051,7794,Refactor verifyReplicationTasks,"## Summary
- simplify verifyReplicationTasks logic for clarity
- enable concurrent verification of replication tasks with configurable concurrency

## Testing
- `go test ./service/worker/migration/... -run Test_verifyReplicationTasks -count=1` *(fails: download go1.24.1 toolchain)*",OpenAI_Codex,3085410,meiliang86,open,2025-05-19T17:06:53Z,,,215654064,https://api.github.com/repos/temporalio/temporal,https://github.com/temporalio/temporal/pull/7794,"analysisWe need to classify the pull request as performance or non-performance. The PR title: ""Refactor verifyReplicationTasks"". Summary: simplify logic, enable concurrent verification with configurable concurrency. That suggests concurrency improvement, potentially performance. But is it primarily performance? The description: ""enable concurrent verification of replication tasks with configurable concurrency"". That is a performance improvement: concurrency to speed up verification. So likely performance. The keywords: concurrency, speed, performance. So classify as 'performance'.assistantfinalperformance",2,0.27733263925807894,False
3119357084,1429,Conditionally build seqlens tensors,"## Summary
- generate cumulative seqlen tensors only when flash attention is enabled
- ignore nested `mistral.rs` repo

## Testing
- `cargo fmt`
- `cargo test --workspace --quiet` *(fails: command was interrupted due to resource constraints)*

------
https://chatgpt.com/codex/tasks/task_e_6840d1743c4083229a4c1ea083fbdf90

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->

## Summary by CodeRabbit

- **Chores**
	- Updated ignored files to exclude the `mistral.rs` directory from version control.

- **Bug Fixes**
	- Improved handling of sequence length processing to ensure correct behavior when flash attention is enabled or disabled, preventing unnecessary computations when not needed.

<!-- end of auto-generated comment: release notes by coderabbit.ai -->",OpenAI_Codex,65165915,EricLBuehler,closed,2025-06-04T23:16:33Z,2025-06-04T23:21:46Z,2025-06-04T23:21:46Z,763774926,https://api.github.com/repos/EricLBuehler/mistral.rs,https://github.com/EricLBuehler/mistral.rs/pull/1429,"analysisWe need to classify as performance or non-performance. The PR title: ""Conditionally build seqlens tensors"". Summary: generate cumulative seqlen tensors only when flash attention is enabled, ignore nested repo. The body mentions improved handling of sequence length processing to ensure correct behavior when flash attention is enabled or disabled, preventing unnecessary computations when not needed. This is about performance: reducing unnecessary computations, conditional building, optimization. So classify as 'performance'.assistantfinalperformance",2,0.2712251369289674,False
3225389334,353,Implement chunked bucket ConcurrentList,"## Summary
- rewrite `ConcurrentList` to use chunked atomic buckets
- document the new approach in the user guide
- note the change in `changelog.md`

## Testing
- ‚ùå `mvn -q -Dtest=*ConcurrentTest* test` *(failed: mvn not installed)*

------
https://chatgpt.com/codex/tasks/task_b_687266251fa4832aaa60efb70188caf7",OpenAI_Codex,5373774,jdereg,closed,2025-07-12T13:54:39Z,2025-07-12T13:54:50Z,2025-07-12T13:54:50Z,12670630,https://api.github.com/repos/jdereg/java-util,https://github.com/jdereg/java-util/pull/353,"analysisWe need to classify the pull request as performance or non-performance. The PR title: ""Implement chunked bucket ConcurrentList"". Summary: rewrite ConcurrentList to use chunked atomic buckets, document new approach, note change in changelog. This seems like a performance improvement: using chunked atomic buckets likely improves concurrency/performance. The description mentions performance? It doesn't explicitly mention performance, but the change is about rewriting to use chunked atomic buckets, which is likely a performance improvement. The guidelines: performance-related PRs may contain keywords like optimization, speed, etc. This PR doesn't contain those keywords. But the content indicates a rewrite for performance. The guidelines say ""Performance-related pull requests may contain keywords such as memory, optimization, leak, speed, perf, stuck, lag, fast, respons, slow, latency, performance, latenc, hang, tim, throughput, compile-time-hog, instant, responsive, react, perform, wait, minor, or any other terms referring to how faster and efficiently software runs."" The PR doesn't contain those keywords. But the content is about rewriting to use chunked atomic buckets, which is likely a performance improvement. The question: ""determine whether the primary purpose of the pull request is to improve performance."" The guidelines: if it contains those keywords, classify as performance. If not, maybe non-performance. But the content indicates a performance improvement. The guidelines say ""Performance-related pull requests may contain keywords such as... or any other terms referring to how faster and efficiently software runs."" It doesn't say that if it doesn't contain those keywords, it's non-performance. But the guidelines are for classification. The PR is about rewriting to use chunked atomic buckets, which is likely a performance improvement. But the guidelines mention that performance-related PRs may contain keywords. It doesn't say that if they don't contain keywords, they are not performance. But the guidelines are for classification. We need to decide. The PR is about concurrency and chunked atomic buckets, which likely improves performance. So we should classify as 'performance'. The guidelines: ""Performance-related pull requests may contain keywords such as memory, optimization, leak, speed, perf, stuck, lag, fast, respons, slow, latency, performance, latenc, hang, tim, throughput, compile-time-hog, instant, responsive, react, perform, wait, minor, or any other terms referring to how faster and efficiently software runs."" The PR doesn't contain those keywords. But the content is about rewriting to use chunked atomic buckets. That is a performance improvement. The guidelines are not strict; they say ""may contain keywords"". So it's possible to classify as performance even if not containing keywords. The question: ""determine whether the primary purpose of the pull request is to improve performance."" The PR is about rewriting to use chunked atomic buckets, which is likely a performance improvement. So classification: performance.assistantfinalperformance",2,0.2709918376800494,False
3125825544,293,Fix buffer mismatch check,"## Summary
- optimize verification so buffer copies only occur when the read count is less than the buffer size

## Testing
- `./gradlew test --no-daemon` *(fails: unresolved Firebase references)*

------
https://chatgpt.com/codex/tasks/task_e_6843386496848320ba5de2416a851d8a",OpenAI_Codex,1151321,depau,closed,2025-06-06T20:44:10Z,2025-06-06T21:05:58Z,2025-06-06T21:05:58Z,147043230,https://api.github.com/repos/etchdroid/etchdroid,https://github.com/etchdroid/etchdroid/pull/293,"analysisWe need to classify the pull request as performance or non-performance. The title: ""Fix buffer mismatch check"". Summary: ""optimize verification so buffer copies only occur when the read count is less than the buffer size"". That is about optimization, reducing buffer copies, improving performance. Keywords: optimize, buffer copies, performance. So it's performance. So output: performance.assistantfinalperformance",2,0.2628633072615101,False
3105888579,28,Use bumpalo for temporary buffers,"## Summary
- add `bumpalo` dependency
- use bump allocation in `RMinHash::update`
- use bump allocation in `CMinHash::update`

## Testing
- `pytest -q` *(fails: ModuleNotFoundError: No module named 'rensa')*

------
https://chatgpt.com/codex/tasks/task_e_683b49da1b988332b03db8bb012b50f9",OpenAI_Codex,61982523,beowolx,closed,2025-05-31T18:33:27Z,2025-05-31T18:53:04Z,,746306477,https://api.github.com/repos/beowolx/rensa,https://github.com/beowolx/rensa/pull/28,"analysisWe need to classify whether the primary purpose is performance. The PR adds bumpalo for temporary buffers, uses bump allocation in RMinHash::update and CMinHash::update. This is about performance: bump allocation reduces allocations, improves speed. So classify as 'performance'.assistantfinalperformance",2,0.2592704989510354,False
3146399914,118,Improve sparse buffer multiplication,"## Summary
- add threaded buffer implementation with five-argument scaling
- allow benchmarking with a buffer that stores additional zero entries
- update benchmark output

## Testing
- `julia -t auto benchmark/sparse_buffer_mult.jl | tee benchmark/sparse_buffer_mult_output.txt` *(interrupted)*

------
https://chatgpt.com/codex/tasks/task_e_684b2fd24fd4832fb24bbb8a81955d53",OpenAI_Codex,13523097,thorek1,open,2025-06-14T17:31:20Z,,,571475096,https://api.github.com/repos/thorek1/MacroModelling.jl,https://github.com/thorek1/MacroModelling.jl/pull/118,"analysisWe need to classify as performance or non-performance. The PR title: ""Improve sparse buffer multiplication"". Summary: add threaded buffer implementation with five-argument scaling, allow benchmarking with a buffer that stores additional zero entries, update benchmark output. This is about performance: adding threaded implementation, scaling, benchmarking. So it's performance.assistantfinalperformance",2,0.23476739691723114,False
3213826108,4603,[Cursor] Fix some bugs,"## NOTE:

This PR was created by a Cursor agent. Proceed with scrutiny.

Fix router memory leaks by ensuring timeout cleanup and improve case sensitivity consistency in route path matching.
",Cursor,5580297,tannerlinsley,closed,2025-07-08T21:33:35Z,2025-07-15T22:15:25Z,,165670309,https://api.github.com/repos/TanStack/router,https://github.com/TanStack/router/pull/4603,"analysisWe need to classify as performance or non-performance. The PR title: ""[Cursor] Fix some bugs"". Body: ""Fix router memory leaks by ensuring timeout cleanup and improve case sensitivity consistency in route path matching."" The description mentions memory leaks, ensuring timeout cleanup. Memory leaks are performance-related. Also ""improve case sensitivity consistency"" is not performance. But memory leaks definitely performance. So classification: performance.assistantfinalperformance",2,0.20469672740789258,False
3250080019,24542,[fix][broker]Fix thread safety issues in BucketDelayedDeliveryTracker with StampedLock optimistic reads,"### Motivation

Fixes #23190

BucketDelayedDeliveryTracker had thread safety issues in frequently called methods like `containsMessage()` and `nextDeliveryTime()` that could lead to race conditions, incorrect duplicate detection, and scheduling inconsistencies under high concurrency loads.

The issue manifested as:
- Race conditions in `containsMessage()` leading to incorrect duplicate detection
- Concurrent access issues in `nextDeliveryTime()` causing scheduling inconsistencies
- Potential data corruption under high concurrency scenarios

### Modifications

- **Added StampedLock for high-performance concurrency control**
  - Implemented optimistic read pattern for frequently called read operations
  - Provides lock-free fast path when no concurrent writes are occurring
  - Falls back gracefully to read locks when validation fails

- **Applied optimistic reads to critical methods:**
  - `containsMessage()` - Used for duplicate message detection
  - `nextDeliveryTime()` - Called frequently for message scheduling

- **Maintained existing write operation synchronization**
  - Write operations continue to use `synchronized` for simplicity and safety
  - Mixed approach optimal for typical read-heavy delayed delivery workloads

- **Removed unused data structure**
  - Eliminated unused `immutableBucketsMap` field to reduce memory overhead
  - All bucket operations use the existing `immutableBuckets` RangeMap

### Performance Improvements

Benchmark results show excellent performance across various concurrency scenarios:
- **Single-threaded reads**: ~305 million ops/s
- **High concurrency (16 threads)**: ~2.6 billion ops/s  
- **Mixed read/write ratios**: Consistent performance from 10:90 to 90:10
- **Optimistic read success rate**: Very high under typical read-heavy workloads

### Thread Safety Strategy

- **Read operations**: Use StampedLock optimistic reads for maximum performance
- **Write operations**: Continue using synchronized for safety and simplicity
- **Data structures**: Leverage existing thread-safe collections (ConcurrentHashMap, etc.)

### Verifying this change

- **Added comprehensive thread safety test**: `BucketDelayedDeliveryTrackerThreadSafetyTest`
- **Created performance benchmark**: `BucketDelayedDeliveryTrackerSimpleBenchmark` 
- **All existing tests pass**
- **No functional changes** - maintains full backward compatibility

### Does this pull request potentially affect one of the following parts:

If the box was checked, please highlight the changes:

- [ ] Dependencies (add or upgrade a dependency)
- [ ] The public API
- [ ] The schema
- [ ] The default behavior
- [ ] The cluster topology
- [ ] The ARM (kafka compatibility, producer/consumer compatibility)

### Documentation

- [ ] `doc` <!-- Your PR contains doc changes -->
- [ ] `doc-required` <!-- Your PR changes impact docs and you will update later -->
- [x] `doc-not-needed` <!-- Your PR changes do not impact docs -->
- [ ] `doc-complete` <!-- Docs have been already added -->

### Matching PR in forked repository

PR in forked repository: [Link](https://github.com/Apurva007/pulsar/pull/7)

ü§ñ Generated with [Claude Code](https://claude.ai/code)

Co-Authored-By: Claude <noreply@anthropic.com>",Claude_Code,10327630,Apurva007,closed,2025-07-21T21:21:39Z,2025-07-22T06:17:01Z,2025-07-22T06:17:01Z,62117812,https://api.github.com/repos/apache/pulsar,https://github.com/apache/pulsar/pull/24542,"analysisWe need to classify as 'performance' or 'non-performance'. The PR title: ""[fix][broker]Fix thread safety issues in BucketDelayedDeliveryTracker with StampedLock optimistic reads"". The body mentions performance improvements: ""Added StampedLock for high-performance concurrency control"", ""Benchmark results show excellent performance"", ""Optimistic read success rate"". The primary purpose is to improve performance via optimistic reads. So classification: 'performance'.assistantfinalperformance",2,0.19735862109400856,True
3164861006,20535,experiment that probably won't merge: native yoga bindings,"Native Yoga bindings for Bun were implemented across several phases.

*   Core C   binding files for `Config`, `Node`, Prototypes, and Constructors were created in `src/bun.js/bindings/`, and `cmake/sources/CxxSources.txt` was updated.
*   JavaScriptCore's GC was integrated by defining IsoSubspaces and implementing `subspaceFor` in `JSYogaConfig.cpp` and `JSYogaNode.cpp` for native object memory management.
*   The `JSYogaConfig` class, its prototype, and constructor were fully implemented, providing methods like `setUseWebDefaults`.
*   The extensive `JSYogaNode` class was implemented, featuring flexible value parsing for setters (e.g., `setWidth`), hierarchy management (`insertChild`, `getChild`, `getParent`) by linking C   nodes to JS wrappers via `YGNodeSetContext`, and callback support (`setMeasureFunc`, `setDirtiedFunc`) with `JSC::Strong` references and `visitChildrenImpl` for GC tracing.
*   Yoga enums were exposed as JavaScript constants via `JSYogaConstants.h/.cpp`.
*   A central `JSYogaModule.h/.cpp` was created, combining `Config`, `Node`, and constants. `GlobalYoga.cpp` was added to expose `Yoga` on `globalThis`. `ZigGlobalObject.h/.cpp` was updated with lazy class structures for constructors, enabling `require('yoga-layout')` and `import Yoga from 'yoga-layout')`.
*   Unit tests were created in `test/js/bun/` for `yoga-config.test.js`, `yoga-node.test.js`, and `yoga-constants.test.js`.

This provides a native, API-compatible, and high-performance Yoga binding.",Cursor,709451,Jarred-Sumner,open,2025-06-21T07:27:15Z,,,357728969,https://api.github.com/repos/oven-sh/bun,https://github.com/oven-sh/bun/pull/20535,"analysisWe need to classify whether the primary purpose is to improve performance. The PR is about implementing native Yoga bindings for Bun. It mentions ""high-performance Yoga binding"". The description: ""Native Yoga bindings for Bun were implemented across several phases."" It says ""This provides a native, API-compatible, and high-performance Yoga binding."" The primary purpose is to provide native bindings, which likely improves performance. The PR is about adding native bindings, which is performance improvement. So classification: 'performance'.assistantfinalperformance",2,0.1882980405781564,False
3078729881,5209,Trace: Fixes thread safety issue in Trace class causing high CPU usage,"## Issue
When cross-region hedging is enabled in the Azure Cosmos DB .NET SDK, users experience high CPU usage due to thread safety issues when accessing dictionaries in the `Trace` class. The issue occurs in the `AddOrUpdateDatum` method where multiple parallel threads attempt to update the same dictionary without proper synchronization.

## Changes
- Added thread safety to the `Trace` class's dictionary operations by using locks
- Modified three methods to ensure thread-safe access:
  - `AddOrUpdateDatum(string, object)`
  - `AddDatum(string, object)`
  - `AddDatum(string, TraceDatum)`
- Added a unit test to verify thread safety with concurrent dictionary access
- Updated documentation in the `ITrace` interface to explicitly note thread-safety guarantees

## Testing
Created a multi-threaded test that simulates the concurrent access pattern that was causing the issue. The test creates 10 threads that all try to access the same dictionary simultaneously.

## Impact
These changes provide thread safety while having minimal performance impact since:
- The locks are only acquired briefly during dictionary operations
- The fix follows existing locking patterns in the codebase (like in the `AddChild` method)
- The changes only affect dictionary access in the `Trace` class and don't introduce broader changes

Fixes #5111.

> [!WARNING]
>
> <details>
> <summary>Firewall rules blocked me from connecting to one or more addresses</summary>
>
> #### I tried to connect to the following addresses, but was blocked by firewall rules:
>
> - `crl3.digicert.com`
>   - Triggering command: `dotnet build Microsoft.Azure.Cosmos/src/Microsoft.Azure.Cosmos.csproj ` (dns block)
>   - Triggering command: `/home/REDACTED/work/_temp/ghcca-node/node/bin/node /home/REDACTED/work/_temp/copilot-developer-action-main/dist/index.js ` (dns block)
>   - Triggering command: `dotnet test Microsoft.Azure.Cosmos/tests/Microsoft.Azure.Cosmos.Tests/Microsoft.Azure.Cosmos.Tests.csproj --filter FullyQualifiedName~Microsoft.Azure.Cosmos.Tests.Tracing.TraceTests.TestAddOrUpdateDatumThreadSafety ` (dns block)
> - `crl4.digicert.com`
>   - Triggering command: `dotnet build Microsoft.Azure.Cosmos/src/Microsoft.Azure.Cosmos.csproj ` (dns block)
>   - Triggering command: `dotnet test Microsoft.Azure.Cosmos/tests/Microsoft.Azure.Cosmos.Tests/Microsoft.Azure.Cosmos.Tests.csproj --filter FullyQualifiedName~Microsoft.Azure.Cosmos.Tests.Tracing.TraceTests.TestAddOrUpdateDatumThreadSafety ` (dns block)
> - `ocsp.digicert.com`
>   - Triggering command: `dotnet build Microsoft.Azure.Cosmos/src/Microsoft.Azure.Cosmos.csproj ` (dns block)
>   - Triggering command: `dotnet test Microsoft.Azure.Cosmos/tests/Microsoft.Azure.Cosmos.Tests/Microsoft.Azure.Cosmos.Tests.csproj --filter FullyQualifiedName~Microsoft.Azure.Cosmos.Tests.Tracing.TraceTests.TestAddOrUpdateDatumThreadSafety ` (dns block)
>   - Triggering command: `/home/REDACTED/work/_temp/ghcca-node/node/bin/node /home/REDACTED/work/_temp/copilot-developer-action-main/dist/index.js ` (dns block)
> - `s.symcb.com`
>   - Triggering command: `dotnet build Microsoft.Azure.Cosmos/src/Microsoft.Azure.Cosmos.csproj ` (dns block)
>   - Triggering command: `dotnet test Microsoft.Azure.Cosmos/tests/Microsoft.Azure.Cosmos.Tests/Microsoft.Azure.Cosmos.Tests.csproj --filter FullyQualifiedName~Microsoft.Azure.Cosmos.Tests.Tracing.TraceTests.TestAddOrUpdateDatumThreadSafety ` (dns block)
> - `s.symcd.com`
>   - Triggering command: `dotnet build Microsoft.Azure.Cosmos/src/Microsoft.Azure.Cosmos.csproj ` (dns block)
>   - Triggering command: `dotnet test Microsoft.Azure.Cosmos/tests/Microsoft.Azure.Cosmos.Tests/Microsoft.Azure.Cosmos.Tests.csproj --filter FullyQualifiedName~Microsoft.Azure.Cosmos.Tests.Tracing.TraceTests.TestAddOrUpdateDatumThreadSafety ` (dns block)
> - `ts-crl.ws.symantec.com`
>   - Triggering command: `dotnet build Microsoft.Azure.Cosmos/src/Microsoft.Azure.Cosmos.csproj ` (dns block)
>   - Triggering command: `/home/REDACTED/work/_temp/ghcca-node/node/bin/node /home/REDACTED/work/_temp/copilot-developer-action-main/dist/index.js ` (dns block)
>   - Triggering command: `dotnet test Microsoft.Azure.Cosmos/tests/Microsoft.Azure.Cosmos.Tests/Microsoft.Azure.Cosmos.Tests.csproj --filter FullyQualifiedName~Microsoft.Azure.Cosmos.Tests.Tracing.TraceTests.TestAddOrUpdateDatumThreadSafety ` (dns block)
> - `ts-ocsp.ws.symantec.com`
>   - Triggering command: `dotnet build Microsoft.Azure.Cosmos/src/Microsoft.Azure.Cosmos.csproj ` (dns block)
>   - Triggering command: `dotnet test Microsoft.Azure.Cosmos/tests/Microsoft.Azure.Cosmos.Tests/Microsoft.Azure.Cosmos.Tests.csproj --filter FullyQualifiedName~Microsoft.Azure.Cosmos.Tests.Tracing.TraceTests.TestAddOrUpdateDatumThreadSafety ` (dns block)
>   - Triggering command: `/home/REDACTED/work/_temp/ghcca-node/node/bin/node /home/REDACTED/work/_temp/copilot-developer-action-main/dist/index.js ` (dns block)
>
> If you need me to access, download, or install something from one of these locations, you can either:
>
> - Configure [Actions setup steps](https://gh.io/copilot/actions-setup-steps) to set up my environment, which run before the firewall is enabled
> - Add the appropriate URLs or hosts to my [firewall allow list](https://gh.io/copilot/firewall-config)
>
> </details>


---

üí° You can make Copilot smarter by setting up custom instructions, customizing its development environment and configuring Model Context Protocol (MCP) servers. Learn more [Copilot coding agent tips](https://gh.io/copilot-coding-agent-tips) in the docs.",Copilot,198982749,Copilot,open,2025-05-21T02:47:05Z,,,156260788,https://api.github.com/repos/Azure/azure-cosmos-dotnet-v3,https://github.com/Azure/azure-cosmos-dotnet-v3/pull/5209,"analysisWe need to classify as 'performance' or 'non-performance'. The PR is about thread safety issue causing high CPU usage. It adds locks to reduce CPU usage. It's about performance? The primary purpose is to fix thread safety issue causing high CPU usage. That is performance improvement? It's more about correctness, but also reduces CPU usage. The classification guidelines: performance-related PRs may contain keywords like latency, optimization, slow, memory, speed, etc. This PR mentions high CPU usage, thread safety, locks. It is about performance improvement? The primary purpose is to fix thread safety issue causing high CPU usage. That is a performance improvement. So classify as 'performance'.assistantfinalperformance",2,0.18048657332492335,False
3132195957,20297,Prevent over-counting Blob memory usage,"### What does this PR do?

<!-- **Please explain what your changes do**, example: -->

<!--

This adds a new flag --bail to bun test. When set, it will stop running tests after the first failure. This is useful for CI environments where you want to fail fast.

-->

- [ ] Documentation or TypeScript types (it's okay to leave the rest blank in this case)
- [ ] Code changes

### How did you verify your code works?

<!-- **For code changes, please include automated tests**. Feel free to uncomment the line below -->

<!-- I wrote automated tests -->

<!-- If JavaScript/TypeScript modules or builtins changed:

- [ ] I included a test for the new code, or existing tests cover it
- [ ] I ran my tests locally and they pass (`bun-debug test test-file-name.test`)

-->

<!-- If Zig files changed:

- [ ] I checked the lifetime of memory allocated to verify it's (1) freed and (2) only freed when it should be
- [ ] I included a test for the new code, or an existing test covers it
- [ ] JSValue used outside of the stack is either wrapped in a JSC.Strong or is JSValueProtect'ed
- [ ] I wrote TypeScript/JavaScript tests and they pass locally (`bun-debug test test-file-name.test`)
-->

<!-- If new methods, getters, or setters were added to a publicly exposed class:

- [ ] I added TypeScript types for the new methods, getters, or setters
-->

<!-- If dependencies in tests changed:

- [ ] I made sure that specific versions of dependencies are used instead of ranged or tagged versions
-->

<!-- If a new builtin ESM/CJS module was added:

- [ ] I updated Aliases in `module_loader.zig` to include the new module
- [ ] I added a test that imports the module
- [ ] I added a test that require() the module
-->
",Cursor,709451,Jarred-Sumner,open,2025-06-10T05:29:06Z,,,357728969,https://api.github.com/repos/oven-sh/bun,https://github.com/oven-sh/bun/pull/20297,"analysisWe need to classify whether the primary purpose is performance. The PR title: ""Prevent over-counting Blob memory usage"". The description: ""What does this PR do?"" and then a placeholder. The content: ""Prevent over-counting Blob memory usage"". That suggests it's about memory usage, maybe preventing memory overcounting. That is about memory usage, which is a performance-related issue. The guidelines: performance-related PRs may contain keywords such as memory, optimization, leak, speed, perf, stuck, lag, fast, respons, slow, latency, performance, latenc, hang, tim, throughput, compile-time-hog, instant, responsive, react, perform, wait, minor, or any other terms referring to how faster and efficiently software runs. The PR mentions memory usage. That is a performance issue. So classify as 'performance'.

Thus answer: performance.assistantfinalperformance",2,0.17937474413041446,True
3203264775,51,Fix unused buffer in floatWorker,"## Summary
- remove unused buffer allocation in `FileSourceModule::floatWorker`

## Testing
- `g++ -std=c++17 -c source_modules/file_source/src/main.cpp -Icore/src -Isource_modules/file_source/src -Icore/src/dsp -Icore/src/utils -Icore/src/signal_path -Icore/src/gui -Idecoder_modules/radio/src` *(fails: imgui.h: No such file or directory)*

------
https://chatgpt.com/codex/tasks/task_e_6868012e344883278f5e3566ba7757cc",OpenAI_Codex,952071,sannysanoff,closed,2025-07-04T16:34:10Z,2025-07-04T18:00:46Z,2025-07-04T18:00:46Z,418222999,https://api.github.com/repos/sannysanoff/SDRPlusPlusBrown,https://github.com/sannysanoff/SDRPlusPlusBrown/pull/51,"analysisWe need to classify whether the primary purpose is performance improvement. The PR title: ""Fix unused buffer in floatWorker"". Summary: remove unused buffer allocation in FileSourceModule::floatWorker. That is about removing unused buffer allocation, which is a memory optimization, but is it performance? Removing unused buffer allocation reduces memory usage, but also may improve performance slightly. The description: ""Fix unused buffer in floatWorker"". It's about removing unused buffer allocation. That is a memory optimization, but is it performance? The guidelines: performance-related PRs may contain keywords such as memory, optimization, leak, speed, perf, etc. This PR mentions ""unused buffer allocation"" which is memory usage. It might be considered performance improvement. The guidelines say ""memory"" is a keyword. So yes, it's performance. So classify as 'performance'.assistantfinalperformance",2,0.1600507262408561,False
3087489797,655,Reduce memory usage in ReentrantFileLockTest,"## Summary
- shrink number of subprocesses in `ReentrantFileLockTest`
- run fewer iterations in each locker process

The test occasionally failed with `os::commit_memory` errors when the
child VMs consumed too much memory. Running fewer processes with fewer
iterations keeps resource usage lower.

## Testing
- `mvn test` *(fails: `mvn: command not found`)*",OpenAI_Codex,109181,tgd,closed,2025-05-23T20:39:49Z,2025-05-23T20:45:56Z,,31261671,https://api.github.com/repos/OpenHFT/Chronicle-Bytes,https://github.com/OpenHFT/Chronicle-Bytes/pull/655,"analysisWe need to classify as performance or non-performance. The PR title: ""Reduce memory usage in ReentrantFileLockTest"". The body: shrink number of subprocesses, run fewer iterations, to reduce memory usage. This is about memory usage, which is performance-related? The instruction says performance-related PRs may contain keywords such as memory. Yes memory is included. So classify as 'performance'.assistantfinalperformance",2,0.14943017388701166,False
3212206965,900,feat: add buffer pool for audio conversion,"## Summary
This PR implements a float32 buffer pool for audio conversion operations, targeting the second memory allocation hotspot identified in our profiling analysis.

## Changes
- Implemented `Float32Pool` using `sync.Pool` for thread-safe buffer reuse
- Modified `convert16BitToFloat32` to use the pool for standard buffer sizes
- Added pool initialization during BirdNET startup
- Implemented proper buffer lifecycle management with return after prediction
- Added comprehensive unit tests, benchmarks, and fuzz tests
- Created documentation explaining the implementation

## Performance Impact
```
BenchmarkAudioConversionComparison/Original-16     4591    352197 ns/op    581647 B/op    1 allocs/op
BenchmarkAudioConversionComparison/WithPool-16    12958     92816 ns/op        69 B/op    1 allocs/op
```

### Improvements:
- **Memory allocation**: Reduced by 99.99% (581KB ‚Üí 69 bytes)
- **Performance**: 3.8x faster (352Œºs ‚Üí 93Œºs)
- **Hit rate**: 99.98% in steady state operation

## Testing
- ‚úÖ Unit tests for pool operations and concurrency
- ‚úÖ Fuzz tests for conversion correctness
- ‚úÖ Benchmarks showing significant improvements
- ‚úÖ All linter issues resolved

## Design Decisions
1. **Standard size only**: Pool only handles standard 3-second buffers (144,384 samples)
2. **Early return**: Buffers returned immediately after BirdNET prediction
3. **Graceful fallback**: Non-standard sizes allocate normally
4. **No clearing**: Audio data doesn't require security clearing

ü§ñ Generated with [Claude Code](https://claude.ai/code)

Co-Authored-By: Claude <noreply@anthropic.com>

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->
## Summary by CodeRabbit

* **New Features**
  * Introduced an optimized float32 buffer pool to improve memory efficiency during audio processing.
  * Added automatic reuse of float32 buffers for 16-bit audio conversions, reducing memory allocations and potential garbage collection pauses.

* **Documentation**
  * Added detailed documentation on the float32 buffer pool, including usage, performance benefits, and integration details.

* **Tests**
  * Added comprehensive unit, fuzz, and benchmark tests for audio conversion and buffer pool functionality, covering correctness, performance, and concurrency scenarios.
<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Claude_Code,7030001,tphakala,closed,2025-07-08T11:43:33Z,2025-07-08T12:26:54Z,2025-07-08T12:26:54Z,707764474,https://api.github.com/repos/tphakala/birdnet-go,https://github.com/tphakala/birdnet-go/pull/900,"analysisWe need to classify as 'performance' or 'non-performance'. The PR is about adding buffer pool for audio conversion, reducing memory allocation and improving speed. It clearly is performance improvement. So label 'performance'.assistantfinalperformance",2,0.1368337014598475,False
3215375260,2422,Fix bugs and consult AGENTS.md,"Fix multiple Unicode handling bugs and optimize string view creation for improved robustness and performance.

The Unicode fixes address critical issues in JSON parsing and string iteration, preventing crashes and incorrect behavior when encountering malformed UTF-16 sequences, particularly surrogate pairs. Performance is improved by removing unnecessary intermediate object creations in string view functions.",Cursor,747051,bobzhang,closed,2025-07-09T10:15:40Z,2025-07-21T09:25:27Z,,749681472,https://api.github.com/repos/moonbitlang/core,https://github.com/moonbitlang/core/pull/2422,"analysisWe need to classify as performance or non-performance. The PR title: ""Fix bugs and consult AGENTS.md"". Body: ""Fix multiple Unicode handling bugs and optimize string view creation for improved robustness and performance."" It mentions performance improvements. So classification: performance.assistantfinalperformance",2,0.13627234141574804,False
3124507979,20228,Cursor/fix next auth test timeouts and memory issues 43d5,"### What does this PR do?

<!-- **Please explain what your changes do**, example: -->

<!--

This adds a new flag --bail to bun test. When set, it will stop running tests after the first failure. This is useful for CI environments where you want to fail fast.

-->

- [ ] Documentation or TypeScript types (it's okay to leave the rest blank in this case)
- [ ] Code changes

### How did you verify your code works?

<!-- **For code changes, please include automated tests**. Feel free to uncomment the line below -->

<!-- I wrote automated tests -->

<!-- If JavaScript/TypeScript modules or builtins changed:

- [ ] I included a test for the new code, or existing tests cover it
- [ ] I ran my tests locally and they pass (`bun-debug test test-file-name.test`)

-->

<!-- If Zig files changed:

- [ ] I checked the lifetime of memory allocated to verify it's (1) freed and (2) only freed when it should be
- [ ] I included a test for the new code, or an existing test covers it
- [ ] JSValue used outside of the stack is either wrapped in a JSC.Strong or is JSValueProtect'ed
- [ ] I wrote TypeScript/JavaScript tests and they pass locally (`bun-debug test test-file-name.test`)
-->

<!-- If new methods, getters, or setters were added to a publicly exposed class:

- [ ] I added TypeScript types for the new methods, getters, or setters
-->

<!-- If dependencies in tests changed:

- [ ] I made sure that specific versions of dependencies are used instead of ranged or tagged versions
-->

<!-- If a new builtin ESM/CJS module was added:

- [ ] I updated Aliases in `module_loader.zig` to include the new module
- [ ] I added a test that imports the module
- [ ] I added a test that require() the module
-->
",Cursor,709451,Jarred-Sumner,closed,2025-06-06T11:30:33Z,2025-06-06T11:30:43Z,2025-06-06T11:30:43Z,357728969,https://api.github.com/repos/oven-sh/bun,https://github.com/oven-sh/bun/pull/20228,"analysisWe need to classify as performance or non-performance. The PR title: ""Cursor/fix next auth test timeouts and memory issues"". The description mentions test timeouts and memory issues. That suggests performance? Memory issues could be performance. But the primary purpose: fix test timeouts and memory issues. That is about tests failing due to timeouts and memory leaks. That is performance-related? The guidelines: performance-related PRs may contain keywords such as memory, optimization, leak, speed, perf, stuck, lag, fast, respons, slow, latency, performance, latenc, hang, tim, throughput, compile-time-hog, instant, responsive, react, perform, wait, minor, or any other terms referring to how faster and efficiently software runs.

The PR mentions memory issues, timeouts. That is performance. So classify as 'performance'.assistantfinalperformance",2,0.12558951696480558,True
3125979396,2245,Reduce LAN game timeout from 15 to 5 seconds for faster testing,"This PR addresses the issue where users get kicked when trying to open a new LAN lobby shortly after closing a game. The problem occurs because the server waits too long before shutting down after all players disconnect.

## Problem
When users close a LAN game and quickly try to open a new lobby (within ~25 seconds), they encounter these error messages:
- `You have been kicked: This game is already started and is no longer accepting new players.`
- `You have been disconnected from server.`

This is especially problematic during testing, which often involves repeatedly starting games.

## Root Cause
The server waits 15 seconds after all players disconnect before shutting down (to allow reconnections). If users try to create a new lobby during this window, the old server is still running and rejects new connections.

## Solution
Reduce the local game timeout from 15 seconds to 5 seconds in `PlayerCollection.cs`. This change:

- ‚úÖ Reduces waiting time by 66% (from 15s to 5s)
- ‚úÖ Still provides reasonable reconnection window for genuine network issues
- ‚úÖ Greatly improves testing workflow for developers
- ‚úÖ Maintains backward compatibility
- ‚úÖ Zero risk to production/online games (only affects local games)

## Changes Made
- Modified `PlayerCollection.cs` line 158: Changed `TimeSpan.FromSeconds(15)` to `TimeSpan.FromSeconds(5)` for local games
- Added comprehensive unit tests in `PlayerCollectionTimeoutTests.cs` to validate timeout behavior
- Non-local games remain unchanged at 2 minutes for network stability

## Testing
Created unit tests that verify:
- Local games timeout after exactly 5 seconds
- Non-local games still timeout after 2 minutes
- New timeout is significantly faster than the previous 15-second delay

Fixes #2175.

---

üí° You can make Copilot smarter by setting up custom instructions, customizing its development environment and configuring Model Context Protocol (MCP) servers. Learn more [Copilot coding agent tips](https://gh.io/copilot-coding-agent-tips) in the docs.",Copilot,198982749,Copilot,closed,2025-06-06T22:11:27Z,2025-06-06T23:31:58Z,2025-06-06T23:31:58Z,3222538,https://api.github.com/repos/octgn/OCTGN,https://github.com/octgn/OCTGN/pull/2245,"analysisWe need to classify as 'performance' or 'non-performance'. The PR title: ""Reduce LAN game timeout from 15 to 5 seconds for faster testing"". The body describes reducing timeout to speed up testing. This is a performance improvement: reducing wait time. Keywords: faster, performance, speed, latency. So classify as 'performance'.assistantfinalperformance",2,0.11225001487493434,False
3074482141,19766,Add BUN_OPTIONS env var,"## Summary
- add a `parseOptionsEnv` helper for splitting environment option strings
- support a new `BUN_OPTIONS` variable in `initArgv`
- copy argv slices with `@memcpy` and avoid unnecessary allocations

## Testing
- `bun agent test test/internal/ban-words.test.ts` *(fails: file rename error during WebKit setup)*",OpenAI_Codex,709451,Jarred-Sumner,closed,2025-05-19T16:58:54Z,2025-05-24T20:36:52Z,2025-05-24T20:36:52Z,357728969,https://api.github.com/repos/oven-sh/bun,https://github.com/oven-sh/bun/pull/19766,"analysisWe need to classify whether the primary purpose is performance. The PR adds BUN_OPTIONS env var, parseOptionsEnv helper, support new env var in initArgv, copy argv slices with @memcpy and avoid unnecessary allocations. That seems performance improvement: copying argv slices with @memcpy to avoid allocations. Also parseOptionsEnv helper maybe for performance? The mention of avoiding unnecessary allocations is performance. So classify as 'performance'.assistantfinalperformance",2,0.10296759833658461,False
3191369762,19,Fix bugs in frontend blocks,"Fix various bugs in frontend blocks, including import inconsistencies, a memory leak, and accessibility issues.",Cursor,3749759,alexpate,closed,2025-07-01T08:32:50Z,2025-07-01T08:33:20Z,2025-07-01T08:33:20Z,744656234,https://api.github.com/repos/trylinky/linky,https://github.com/trylinky/linky/pull/19,"analysisWe need to classify as performance or non-performance. The PR title: ""Fix bugs in frontend blocks"". Body: ""Fix various bugs in frontend blocks, including import inconsistencies, a memory leak, and accessibility issues."" It mentions a memory leak. That is a performance issue? Memory leak is a performance issue. The guidelines: performance-related PRs may contain keywords such as memory, optimization, leak, speed, perf, etc. So memory leak qualifies. So classification: performance.assistantfinalperformance",2,0.10074768931159453,False
3233770846,984,"fix: security, performance, and reliability issues in API","Fixes 3 critical bugs: insecure cookie parsing, `GlobalScope` memory leaks, and potential infinite loops in API pagination.",Cursor,80542861,mostafaalagamy,closed,2025-07-15T21:37:11Z,2025-07-16T17:08:53Z,2025-07-16T17:08:53Z,677376114,https://api.github.com/repos/mostafaalagamy/Metrolist,https://github.com/mostafaalagamy/Metrolist/pull/984,"analysisWe need to classify as performance or non-performance. The PR title: ""fix: security, performance, and reliability issues in API"". Body: ""Fixes 3 critical bugs: insecure cookie parsing, `GlobalScope` memory leaks, and potential infinite loops in API pagination.""

We need to determine if primary purpose is to improve performance. The PR addresses security, performance, reliability. It fixes memory leaks (performance), infinite loops (performance). So yes, performance-related. So classification: 'performance'.assistantfinalperformance",2,0.09844438182427853,False
3166367062,20578,Improve Headers construction,"## Summary
- optimize Headers constructor when called without arguments
- expose and use `toJSNewlyConstructed`

## Testing
- `bun bd test test/js/bun/http/bun-serve-headers.test.ts` *(fails: unable to download WebKit build)*

------
https://chatgpt.com/codex/tasks/task_e_6858904715488328921daf5016807c78",OpenAI_Codex,709451,Jarred-Sumner,closed,2025-06-22T23:35:41Z,2025-06-24T00:31:46Z,,357728969,https://api.github.com/repos/oven-sh/bun,https://github.com/oven-sh/bun/pull/20578,"analysisWe need to classify the pull request as performance or non-performance. The title: ""Improve Headers construction"". Summary: ""optimize Headers constructor when called without arguments"" and ""expose and use `toJSNewlyConstructed`"". This is about optimizing the Headers constructor, likely performance improvement. Keywords: optimize, performance. So classification: performance.assistantfinalperformance",2,0.09699552810429149,False
3078237999,19800,Add Zstd decompression to HTTP client,"## Summary
- support `zstd` in HTTP Accept-Encoding and decoding
- implement `ZstdReaderArrayList` for streaming decompression
- update Decompressor and encoding logic to handle zstd

## Testing
- `bun agent test test/js/web/fetch/client-fetch.test.ts` *(fails: CMake error downloading WebKit)*",OpenAI_Codex,709451,Jarred-Sumner,closed,2025-05-20T21:07:07Z,2025-05-21T06:26:48Z,2025-05-21T06:26:48Z,357728969,https://api.github.com/repos/oven-sh/bun,https://github.com/oven-sh/bun/pull/19800,"analysisWe need to classify whether the primary purpose is performance. The PR adds Zstd decompression to HTTP client. Zstd is a compression algorithm that is faster and more efficient. The PR adds support for zstd in Accept-Encoding and decoding, and implements streaming decompression. This is about compression, which can improve performance (network bandwidth, speed). The description: ""Add Zstd decompression to HTTP client"". This is about adding a new compression algorithm, which can improve performance by reducing data size and decompressing faster. So it's performance-related. The classification: 'performance'.assistantfinalperformance",2,0.08855782670259035,False
3106804055,814,Improve Wget builder defaults and reading efficiency,"## Summary
- use char buffer for better efficiency in `Wget.fetch`
- create default connection provider lazily in `Wget.Builder`
- document default value for `maxResponseBytes`
- update appendable exception test to match new fetch behaviour

## Testing
- `mvn -q test` *(fails: `mvn: command not found`)*

------
https://chatgpt.com/codex/tasks/task_e_683bffad4210832994604d39bc13f86a",OpenAI_Codex,1070321,peter-lawrey,closed,2025-06-01T07:46:49Z,2025-06-01T07:47:17Z,2025-06-01T07:47:17Z,31261793,https://api.github.com/repos/OpenHFT/Chronicle-Core,https://github.com/OpenHFT/Chronicle-Core/pull/814,"analysisWe need to classify as performance or non-performance. The PR title: ""Improve Wget builder defaults and reading efficiency"". Summary: use char buffer for better efficiency in Wget.fetch, create default connection provider lazily, document default value, update test. This is about reading efficiency, char buffer, performance. So it's performance. So output 'performance'.assistantfinalperformance",2,0.07445191423777812,False
3106780046,813,Improve Wget builder defaults and reading efficiency,"## Summary
- use char buffer for better efficiency in `Wget.fetch`
- create default connection provider lazily in `Wget.Builder`
- document default value for `maxResponseBytes`

## Testing
- `mvn -q test` *(fails: `mvn: command not found`)*

------
https://chatgpt.com/codex/tasks/task_e_683bffad4210832994604d39bc13f86a",OpenAI_Codex,1070321,peter-lawrey,closed,2025-06-01T07:25:10Z,2025-06-01T07:25:24Z,2025-06-01T07:25:24Z,31261793,https://api.github.com/repos/OpenHFT/Chronicle-Core,https://github.com/OpenHFT/Chronicle-Core/pull/813,"analysisWe need to classify as performance or non-performance. The PR title: ""Improve Wget builder defaults and reading efficiency"". Summary: use char buffer for better efficiency in Wget.fetch, create default connection provider lazily, document default value for maxResponseBytes. This is about reading efficiency, char buffer, lazy creation. That is performance improvement. So label 'performance'.assistantfinalperformance",2,0.0737427312314545,False
