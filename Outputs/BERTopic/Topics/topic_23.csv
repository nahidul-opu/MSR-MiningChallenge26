id,number,title,body,agent,user_id,user,state,created_at,closed_at,merged_at,repo_id,repo_url,html_url,Topic,Probability,Representative_document
3127630140,1253,Compute point index map lazily,"This PR avoids storing `alt->point-idxs` in `alt-table`, instead computing it directly (basically `invert-index`) when needed. This is good because we only actually need this index in one place. Behavior should be unchanged.

https://chatgpt.com/codex/tasks/task_e_6844b88d56048331a0349cec4e0720da",OpenAI_Codex,30707,pavpanchekha,closed,2025-06-07T22:56:40Z,2025-06-09T16:34:41Z,,13683952,https://api.github.com/repos/herbie-fp/herbie,https://github.com/herbie-fp/herbie/pull/1253,23,1.0,False
3093387507,1782,Add benchmark for deserializing large added vocab +  optimizations,,OpenAI_Codex,48595927,ArthurZucker,open,2025-05-27T10:09:25Z,,,219035799,https://api.github.com/repos/huggingface/tokenizers,https://github.com/huggingface/tokenizers/pull/1782,23,1.0,False
3240593081,9,Codex/integrate tygent module for performance,,OpenAI_Codex,212389004,tygent0,open,2025-07-17T19:00:42Z,,,1020430400,https://api.github.com/repos/Doriandarko/make-it-heavy,https://github.com/Doriandarko/make-it-heavy/pull/9,23,1.0,False
3217675934,246,Cursor/analyze and optimize code performance 4130,,Cursor,5333537,gmathi,open,2025-07-10T02:36:33Z,,,92864511,https://api.github.com/repos/gmathi/NovelLibrary,https://github.com/gmathi/NovelLibrary/pull/246,23,1.0,False
3239263606,6472,Optimize draw shape pressure property,"This PR refactors how draw shape point pressure is stored and handled, leading to storage optimization and improved clarity.

Previously, pressure was stored as a float `0-1` in the `z` property of a point, with `0` or `0.5` indicating no pressure. This update changes the storage to:
1.  **Omit the `z` property entirely** when no pressure information is available (e.g., mouse input).
2.  Store pressure as an **integer between `0` and `100`** when pressure is provided (e.g., pen/stylus input).

A new migration (`OptimizePressure: 3`) is included to safely convert existing draw shapes to the new format, ensuring backward and forward compatibility. This change reduces the data size of draw shapes and makes the pressure values more intuitive.

### Change type

- [ ] `bugfix`
- [x] `improvement`
- [ ] `feature`
- [ ] `api`
- [ ] `other`

### Test plan

1.  **Draw with mouse:**
    *   Select the draw tool.
    *   Draw a shape using a mouse.
    *   Verify the shape is drawn correctly. (Internally, points should *not* have a `z` property).
2.  **Draw with pen/stylus (if applicable):**
    *   Select the draw tool.
    *   Draw a shape using a pen or stylus with varying pressure.
    *   Verify the shape's stroke width changes with pressure. (Internally, points *should* have a `z` property with an integer value between 0-100).
3.  **Load old files:**
    *   Open a file created before this PR with draw shapes.
    *   Verify the draw shapes load and display correctly, and their pressure values are migrated.

- [x] Unit tests
- [x] End to end tests

### Release notes

- Optimized draw shapes by storing pressure as an integer (0-100) only when provided, reducing file size and improving data clarity. Non-pressure inputs (e.g., mouse) no longer store a pressure value. Existing draw shapes will be automatically migrated.",Cursor,23072548,steveruizok,closed,2025-07-17T11:42:48Z,2025-07-28T13:50:13Z,,365739812,https://api.github.com/repos/tldraw/tldraw,https://github.com/tldraw/tldraw/pull/6472,23,1.0,True
3111841315,492,Add Git web worker for background isomorphic-git operations,,Cursor,4608155,colebemis,closed,2025-06-02T22:42:30Z,2025-06-02T22:57:29Z,,517126881,https://api.github.com/repos/lumen-notes/lumen,https://github.com/lumen-notes/lumen/pull/492,23,1.0,False
3158165385,415,Add autovacuum optimization CLI,,OpenAI_Codex,16663421,janbjorge,closed,2025-06-18T21:06:51Z,2025-06-22T11:05:49Z,2025-06-22T11:05:49Z,788904329,https://api.github.com/repos/janbjorge/pgqueuer,https://github.com/janbjorge/pgqueuer/pull/415,23,1.0,False
3087728875,94,Cursor/identify additional shallow store optimizations d0c6,,Cursor,2830514,ryokun6,closed,2025-05-23T23:42:19Z,2025-05-23T23:42:27Z,2025-05-23T23:42:27Z,923332984,https://api.github.com/repos/ryokun6/ryos,https://github.com/ryokun6/ryos/pull/94,23,1.0,False
3212315483,672,Add speed index to models,Still need to feed the model values with relevant ones,Cursor,8428699,yannbu,closed,2025-07-08T12:16:04Z,2025-07-09T15:07:00Z,2025-07-09T15:07:00Z,899127822,https://api.github.com/repos/WorkflowAI/WorkflowAI,https://github.com/WorkflowAI/WorkflowAI/pull/672,23,1.0,False
3196341284,423,Add drain benchmark strategy,,OpenAI_Codex,16663421,janbjorge,closed,2025-07-02T15:38:33Z,2025-07-03T12:04:57Z,2025-07-03T12:04:57Z,788904329,https://api.github.com/repos/janbjorge/pgqueuer,https://github.com/janbjorge/pgqueuer/pull/423,23,1.0,False
3250477735,397,Optimize nancorrmatrix and nancovmatrix for cache locality,"Refactor `nancorrmatrix` and `nancovmatrix` to process observations sequentially. This improves cache locality by reducing random memory access patterns, leading to better performance.

The previous implementation iterated over variable pairs, then observations, resulting in scattered memory access. The new approach iterates over observations first, loading an entire observation into cache, then processing all variable pairs for that observation. This reduces cache misses significantly.

Also adds new benchmark parameters to test these functions with larger inputs.

ü§ñ Generated with [Claude Code](https://claude.ai/code)

Co-Authored-By: Claude <noreply@anthropic.com>
",Claude_Code,5635139,max-sixty,closed,2025-07-22T00:19:55Z,2025-07-22T00:28:17Z,2025-07-22T00:28:17Z,25501620,https://api.github.com/repos/numbagg/numbagg,https://github.com/numbagg/numbagg/pull/397,23,0.3068786012603992,False
3153767204,2516,alpaka kernels for matrix multiplication and addition,,OpenAI_Codex,115539225,Akasxh,open,2025-06-17T14:42:57Z,,,26217535,https://api.github.com/repos/alpaka-group/alpaka,https://github.com/alpaka-group/alpaka/pull/2516,23,0.19710019033222675,False
3159898280,393,Optimize HitTester with point lookup,"## Summary
- add `intersects_point` in `SceneCache` using rstar
- update `HitTester` to query layers with new method
- keep cursor state and screen space conversion helpers
- clarify docs about deep-first hit testing order
- log hit test result each time the cursor moves

## Testing
- `cargo test -p cg -- --nocapture`


------
https://chatgpt.com/codex/tasks/task_e_6853d59cb434832aba7ff41ddf4d86de",OpenAI_Codex,16307013,softmarshmallow,closed,2025-06-19T11:00:51Z,2025-06-19T11:04:58Z,2025-06-19T11:04:58Z,338529997,https://api.github.com/repos/gridaco/grida,https://github.com/gridaco/grida/pull/393,23,0.18561860838904284,False
3132536542,385,Implement scene caching in canvas,"## Summary
- introduce `SceneCache` helper for storing pre-recorded pictures
- cache and reuse scenes in `Renderer`
- update window example to cache scenes
- document scene caching in README

## Testing
- `find crates/grida-canvas -name '*.rs' -print0 | xargs -0 rustfmt --edition 2021`
- `cargo test -p cg`
- `cargo build -p cg`


------
https://chatgpt.com/codex/tasks/task_e_6847dfabb644832a8622090ac98f820f",OpenAI_Codex,16307013,softmarshmallow,closed,2025-06-10T07:53:49Z,2025-06-10T15:11:33Z,2025-06-10T15:11:33Z,338529997,https://api.github.com/repos/gridaco/grida,https://github.com/gridaco/grida/pull/385,23,0.15608855539595648,False
2892658334,493,Add Memory Bucket Optimizer Tool,"# Memory Bucket Optimizer Tool for wolfSSL

This PR adds a memory bucket optimizer tool for wolfSSL's static memory feature. The tool analyzes malloc sizes from the wolfSSL example client and suggests optimal bucket configurations to minimize wasted memory overhead.

## Features
- Analyzes malloc sizes from the wolfSSL example client
- Suggests optimal bucket configurations to minimize wasted memory overhead
- Orders bucket sizes from smallest to largest
- Works with different TLS operations (TLS 1.2, TLS 1.3, different servers)
- Includes visualization with gnuplot charts
- Supports testing with WOLFSSL_NO_MALLOC for embedded systems

## IDE Parsing Issues
During development, we noticed that some IDEs report syntax errors in wolfSSL header files (memory.h, ssl.h) and implementation files (memory.c). These errors are **NOT** related to our memory bucket optimizer changes but are rather due to the IDE's inability to properly parse the complex conditional compilation directives in the wolfSSL codebase.

These syntax errors are purely IDE parsing issues and do not indicate actual code problems. They are a result of the complex conditional compilation structure of the wolfSSL codebase, which is designed to support multiple platforms, configurations, and feature sets. The code compiles and runs correctly despite these IDE errors.

## Link to Devin run
https://app.devin.ai/sessions/41da69b84f0b4df8aeabf89a731efcb1

## Requested by
jacob@wolfssl.com",Devin,158243242,devin-ai-integration[bot],closed,2025-03-04T01:05:18Z,2025-03-13T14:51:55Z,,21290295,https://api.github.com/repos/wolfSSL/wolfssl-examples,https://github.com/wolfSSL/wolfssl-examples/pull/493,23,0.0861701732599301,True
3215138589,4579,‰ºòÂåñÂÜÖÂ≠òÂàÜÊûêÂäüËÉΩÊÄßËÉΩ,"Optimize memory analysis functions to prevent high CPU usage and freezing.

The previous implementation could lead to 100% CPU utilization and system freezes due to processing a large number of objects without limits. This PR introduces timeouts, limits on the number of objects analyzed, and simplifies complex deep analysis to significantly improve performance and stability.",Cursor,51039935,jxxghp,closed,2025-07-09T09:10:19Z,2025-07-09T09:43:46Z,2025-07-09T09:43:46Z,647509652,https://api.github.com/repos/jxxghp/MoviePilot,https://github.com/jxxghp/MoviePilot/pull/4579,23,0.0715664861754653,False
3226522393,126,Fix memory allocation issue for large files by capping StaticSparseDAG capacity,"Thanks for asking me to work on this. I will get started on it and keep this PR's description up to date as I form a plan and make progress.

Original description:

> Reproduce this issue and try to fix it



<!-- START COPILOT CODING AGENT TIPS -->
---

üí¨ Share your feedback on Copilot coding agent for the chance to win a $200 gift card! Click [here](https://survey.alchemer.com/s3/8343779/Copilot-Coding-agent) to start the survey.",Copilot,198982749,Copilot,closed,2025-07-13T13:52:51Z,2025-07-13T14:30:53Z,2025-07-13T14:30:53Z,132325183,https://api.github.com/repos/messense/jieba-rs,https://github.com/messense/jieba-rs/pull/126,23,0.04821202432211936,False
3125710825,1936,feat: update parts channel capacity from 1000 to 2500,"Updates the parts channel capacity from 1000 to 2500 to prevent blocking during processing of large blocks (128MB).

## Changes

- Increased `partsChan` buffer capacity from 1000 to 2500 in all relevant files:
  - `node/node.go` - Main production code where the parts channel is created
  - `consensus/reactor_test.go` - Test files using the parts channel
  - `consensus/propagation/reactor_test.go`
  - `consensus/propagation/commitment_test.go`
  - `consensus/wal_generator.go`
  - `consensus/byzantine_test.go`
  - `consensus/replay_file.go`
  - `consensus/common_test.go`

## Problem

The current capacity of 1000 can cause blocking when processing large 128MB blocks, which adds unnecessary complexity to the consensus mechanism and can impact performance.

## Solution

Increasing the channel capacity to 2500 provides sufficient buffer space to handle large block processing without blocking, simplifying the consensus flow and improving performance for large blocks.

Fixes #1923.

---

üí° You can make Copilot smarter by setting up custom instructions, customizing its development environment and configuring Model Context Protocol (MCP) servers. Learn more [Copilot coding agent tips](https://gh.io/copilot-coding-agent-tips) in the docs.",Copilot,198982749,Copilot,closed,2025-06-06T19:42:55Z,2025-06-11T14:42:14Z,2025-06-11T14:42:14Z,221931137,https://api.github.com/repos/celestiaorg/celestia-core,https://github.com/celestiaorg/celestia-core/pull/1936,23,0.04743371764044783,False
3181043136,2404,feat: Implement configurable batch queue throttling to prevent resource exhaustion,"## Problem

The `BatchQueue` implementation in `sequencers/single/queue.go` had **no size limits**, leading to potential resource exhaustion and system instability. During DA congestion or connection issues, batches would accumulate faster than they could be processed, causing:

- **Unbounded memory growth** from unlimited slice expansion
- **Unbounded disk usage** from persistent storage without cleanup
- **No backpressure mechanism** to signal upstream components when overwhelmed
- **Performance degradation** during restart recovery with large persistent queues

## Solution

Added configurable queue limits with graceful error handling while maintaining full backward compatibility:

### Key Changes

1. **New Error Type for Backpressure**
   ```go
   var ErrQueueFull = errors.New(""batch queue is full"")
   ```

2. **Enhanced BatchQueue Structure**
   ```go
   type BatchQueue struct {
       queue        []coresequencer.Batch
       maxQueueSize int // 0 = unlimited for backward compatibility
       mu           sync.Mutex
       db           ds.Batching
   }
   ```

3. **Throttling Logic in AddBatch**
   ```go
   // Check if queue is full (maxQueueSize of 0 means unlimited)
   if bq.maxQueueSize > 0 && len(bq.queue) >= bq.maxQueueSize {
       return ErrQueueFull
   }
   ```

4. **Production-Ready Defaults**
   - Set default limit of 1000 batches in single sequencer
   - Enhanced error logging when queue reaches capacity
   - Graceful error propagation with informative messages

### Backward Compatibility

- Existing tests use `maxSize: 0` (unlimited) to maintain current behavior
- All existing functionality preserved
- No breaking changes to public APIs

### Test Coverage

Added comprehensive test suites covering:
- Various queue size limits (unlimited, within limit, at limit, exceeding limit)
- Queue behavior after batch processing (demonstrates backpressure relief)
- Thread safety under concurrent load (100 workers, 10 queue limit)
- End-to-end integration testing with sequencer

**Coverage increased from 76.7% to 78.0%**

### Example Behavior

```go
// During normal operation
queue := NewBatchQueue(db, ""batches"", 1000)
err := queue.AddBatch(ctx, batch) // ‚úÖ Success

// During DA congestion (queue full)
err := queue.AddBatch(ctx, batch) // ‚ùå Returns ErrQueueFull

// After DA processes batches
batch, _ := queue.Next(ctx)       // Frees space
err = queue.AddBatch(ctx, batch)  // ‚úÖ Success again
```

This prevents the resource exhaustion scenarios while allowing normal operation and providing clear backpressure signals to upstream components.

Fixes #2252.

<!-- START COPILOT CODING AGENT TIPS -->
---

üí¨ Share your feedback on Copilot coding agent for the chance to win a $200 gift card! Click [here](https://survey.alchemer.com/s3/8343779/Copilot-Coding-agent) to start the survey.",Copilot,198982749,Copilot,closed,2025-06-27T01:50:31Z,2025-06-27T07:29:09Z,2025-06-27T07:29:08Z,335571729,https://api.github.com/repos/evstack/ev-node,https://github.com/evstack/ev-node/pull/2404,23,0.04407222111212924,True
