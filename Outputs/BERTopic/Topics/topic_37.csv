id,number,title,body,agent,user_id,user,state,created_at,closed_at,merged_at,repo_id,repo_url,html_url,Topic,Probability,Representative_document
3235395709,34529,Fix file save blocking on entry refresh for improved hot reload performance,"## Summary

Fixes file save operations blocking on filesystem entry refresh, which was causing hot reload systems to detect file changes later than other editors like VS Code or Sublime Text.

## Changes

Modified `LocalWorktree::write_file` in `crates/worktree/src/worktree.rs` to make the `refresh_entry` call non-blocking by moving it to a background task. The save operation now completes immediately after the file write, with filesystem state refresh happening asynchronously.

## Root Cause

The issue was in the save flow where:
1. File gets written to disk (`write.await?`)
2. Save completion waits for `refresh_entry().await?` to finish
3. `refresh_entry` sends a scan request to background thread and waits
4. Only then does the save operation return success

This blocking behavior delayed the save completion signal that hot reload tools rely on.

## Solution

- Start refresh task without awaiting it
- Spawn refresh in background with `.detach()`
- Return success immediately after file write
- Use existing entry state when available

## Testing

User reported that the fix resolved the hot reload delay issue after building and testing the changes.

## Related Issue

Closes #34527

ðŸ¤– Generated with [Claude Code](https://claude.ai/code)

Co-Authored-By: Claude <noreply@anthropic.com>",Claude_Code,27272,sausaw,closed,2025-07-16T10:36:19Z,2025-07-16T10:40:17Z,,340547520,https://api.github.com/repos/zed-industries/zed,https://github.com/zed-industries/zed/pull/34529,37,1.0,False
3168705723,1636,Implement retry-after header handling for improved throttling in fetch requests,"Currently, genaiscript handles throttling situations but does not respect the `retry-after` header returned by services. This leads to unnecessary load on throttled services and suboptimal user experience with exponential backoff delays that may be longer than needed.

## Changes Made

This PR implements proper `retry-after` header handling in the fetch retry logic:

### 1. Added `parseRetryAfter()` function
- Supports both integer seconds format: `retry-after: 120` 
- Supports HTTP date format: `retry-after: Fri, 31 Dec 1999 23:59:59 GMT`
- Handles edge cases: negative values, invalid inputs, past dates, whitespace
- Returns `null` for invalid inputs to fallback to exponential backoff

### 2. Enhanced retry logic in `createFetch()`
- Checks for `retry-after` header in HTTP responses during retries
- Uses server-specified delay instead of calculated exponential backoff when available
- Falls back to existing exponential backoff when no `retry-after` header is present
- Respects `maxDelay` configuration to prevent excessive waits

### 3. Improved user transparency
- Displays retry-after information in log messages
- Format: `retry #1 in 120.0s (retry-after: 120s)` vs `retry #1 in 2.0s`
- Users can now see the actual reason and duration for delays

## Example Behavior

**Before:**
```
retry #1 in 2.0s 429 Too Many Requests
retry #2 in 3.0s 429 Too Many Requests  
retry #3 in 4.5s 429 Too Many Requests
```

**After (with retry-after: 60):**
```
retry #1 in 60.0s (retry-after: 60s) 429 Too Many Requests
```

## Benefits

- âœ… Reduces unnecessary load on throttled services by respecting server timing
- âœ… Provides transparency to users about wait times and reasons
- âœ… Maintains full backward compatibility with existing retry behavior
- âœ… Handles common throttling scenarios (HTTP 429, 503, etc.)
- âœ… RFC 7231 compliant retry-after header parsing

## Testing

Added comprehensive unit tests covering:
- Integer seconds parsing (`""120""` â†’ 120 seconds)
- HTTP date parsing (future dates with proper time calculation)
- Edge cases (negative values, invalid strings, past dates)
- Whitespace handling and input validation

Fixes #1629.

> [!WARNING]
>
> <details>
> <summary>Firewall rules blocked me from connecting to one or more addresses</summary>
>
> #### I tried to connect to the following addresses, but was blocked by firewall rules:
>
> - `cdn.sheetjs.com`
>   - Triggering command: `node /usr/local/bin/yarn install ` (dns block)
>
> If you need me to access, download, or install something from one of these locations, you can either:
>
> - Configure [Actions setup steps](https://gh.io/copilot/actions-setup-steps) to set up my environment, which run before the firewall is enabled
> - Add the appropriate URLs or hosts to my [firewall allow list](https://gh.io/copilot/firewall-config)
>
> </details>



<!-- START COPILOT CODING AGENT TIPS -->
---

ðŸ’¡ You can make Copilot smarter by setting up custom instructions, customizing its development environment and configuring Model Context Protocol (MCP) servers. Learn more [Copilot coding agent tips](https://gh.io/copilot-coding-agent-tips) in the docs.",Copilot,198982749,Copilot,closed,2025-06-23T16:02:38Z,2025-06-23T16:41:07Z,2025-06-23T16:41:07Z,679784368,https://api.github.com/repos/microsoft/genaiscript,https://github.com/microsoft/genaiscript/pull/1636,37,1.0,True
3242174192,6617,feat: implement async timeout for segments with cache support,"## Overview

This PR implements asynchronous segment execution with caching support to address performance issues in large repositories, specifically targeting issue #6094. The feature allows segments to execute with configurable timeouts, using cached data when operations exceed the threshold while updating the cache in the background.

## Problem

Users working with large monorepos experience slow prompt rendering due to expensive operations like `git status` that can take several seconds. Current workarounds involve disabling status checks, which sacrifices important information in the prompt.

## Solution

### Core Features

1. **Async Timeout Configuration**: New `async_timeout` field for segments
2. **Intelligent Caching**: Cached segment data with background refresh
3. **Graceful Fallback**: Use cached data when operations exceed timeout
4. **Background Updates**: Non-blocking cache refresh for next prompt

### Implementation Details

- **Configuration**: Added `async_timeout` field to segment configuration (in nanoseconds)
- **Cache Management**: New `AsyncSegmentCache` system for persistent segment data
- **Background Processes**: Automatic cache refresh without blocking prompt rendering
- **CLI Support**: Cache management commands for debugging and maintenance

### Usage Example

```json
{
  ""type"": ""git"",
  ""style"": ""plain"",
  ""async_timeout"": 100,
  ""properties"": {
    ""fetch_status"": true
  }
}
```

## How It Works

1. **First Run**: Segment executes normally, result is cached
2. **Subsequent Runs**: 
   - If execution completes within timeout â†’ use fresh data, update cache
   - If execution exceeds timeout â†’ use cached data, start background refresh
3. **Background Refresh**: Cache is updated asynchronously for next prompt

## Benefits

- **Instant Prompts**: Cached data provides immediate rendering
- **Fresh Data**: Background updates ensure information stays current
- **Configurable**: Per-segment timeout configuration
- **Backward Compatible**: Existing configurations work unchanged

## Testing

- âœ… Unit tests for async timeout configuration
- âœ… Cache management functionality tests
- âœ… Integration tests with git segments
- âœ… CLI command validation
- âœ… Configuration loading from JSON/YAML

## Files Changed

- `src/config/segment.go` - Core async execution logic
- `src/cache/async.go` - Async cache management system
- `src/cli/cache.go` - CLI cache refresh commands
- `src/config/async_test.go` - Comprehensive test suite
- `src/config/async_config_test.go` - Configuration loading tests

## Migration

No breaking changes. The feature is opt-in via the new `async_timeout` configuration field.

## Future Enhancements

This implementation provides the foundation for the full async prompt system mentioned in #6094. Future work could include:
- Shell-specific async prompt integration
- Advanced cache invalidation strategies
- Performance monitoring and metrics

Closes #6094

> [!WARNING]
>
> <details>
> <summary>Firewall rules blocked me from connecting to one or more addresses</summary>
>
> #### I tried to connect to the following addresses, but was blocked by firewall rules:
>
> - `ohmyposh.dev`
>
> If you need me to access, download, or install something from one of these locations, you can either:
>
> - Configure [Actions setup steps](https://gh.io/copilot/actions-setup-steps) to set up my environment, which run before the firewall is enabled
> - Add the appropriate URLs or hosts to my [firewall allow list](https://gh.io/copilot/firewall-config)
>
> </details>



<!-- START COPILOT CODING AGENT TIPS -->
---

ðŸ’¡ You can make Copilot smarter by setting up custom instructions, customizing its development environment and configuring Model Context Protocol (MCP) servers. Learn more [Copilot coding agent tips](https://gh.io/copilot-coding-agent-tips) in the docs.",Copilot,198982749,Copilot,open,2025-07-18T07:09:18Z,,,175405157,https://api.github.com/repos/JanDeDobbeleer/oh-my-posh,https://github.com/JanDeDobbeleer/oh-my-posh/pull/6617,37,1.0,True
3228266614,7288,Implement exponential backoff for Copilot refresh after commenting on Padawan PRs,"When commenting on Copilot-assigned PRs, the timeline refresh now uses exponential backoff with early exit detection to show newly launched Copilot sessions more responsively.

## Problem

Previously, when a user commented on a Copilot-assigned PR, the extension would wait a fixed 1 second before refreshing the timeline to show the newly launched Copilot session. This caused delays since it takes time to determine and launch the session on GitHub.com.

## Solution

Implemented exponential backoff with retry intervals of 500ms, 1s, 2s, and 5s that:

- **Captures initial timeline state** before starting retries
- **Polls for new `CopilotStarted` events** on each retry
- **Exits early** when a new Copilot session is detected
- **Gracefully handles errors** during timeline fetching
- **Falls back to single refresh** if no new events are found

## Key Benefits

- **Faster response** when Copilot sessions start quickly (500ms vs 1s minimum)
- **Better user experience** by showing sessions immediately when available
- **Server-friendly** with reasonable retry intervals that avoid excessive load
- **Maintains reliability** with fallback behavior
- **Backwards compatible** with existing functionality

## Implementation Details

The `tryScheduleCopilotRefresh` method in `src/github/issueOverview.ts` now:

```typescript
// Before: Fixed 1-second wait
await new Promise(resolve => setTimeout(resolve, 1000));
if (!this._isDisposed) {
    this.refreshPanel();
}

// After: Exponential backoff with early exit
const delays = [500, 1000, 2000, 5000];
for (const delay of delays) {
    await new Promise(resolve => setTimeout(resolve, delay));
    
    const currentTimeline = await this._getTimeline();
    const currentCopilotStartedEvents = currentTimeline.filter(event => 
        event.event === EventType.CopilotStarted);
    
    if (currentCopilotStartedEvents.length > initialCopilotStartedEvents.length) {
        this.refreshPanel();
        return; // Early exit when new event found
    }
}
```

## Testing

- Added comprehensive unit tests covering edge cases
- Verified exponential backoff timing and early exit logic
- Confirmed backwards compatibility with existing behavior
- All existing tests continue to pass

Fixes #7231.

<!-- START COPILOT CODING AGENT TIPS -->
---

ðŸ’¡ You can make Copilot smarter by setting up custom instructions, customizing its development environment and configuring Model Context Protocol (MCP) servers. Learn more [Copilot coding agent tips](https://gh.io/copilot-coding-agent-tips) in the docs.",Copilot,198982749,Copilot,closed,2025-07-14T10:36:18Z,2025-07-14T18:47:52Z,2025-07-14T18:47:52Z,134777298,https://api.github.com/repos/microsoft/vscode-pull-request-github,https://github.com/microsoft/vscode-pull-request-github/pull/7288,37,1.0,True
3088454350,694,[alpha_factory] add LRU cache size limit,"## Summary
- implement size-capped OrderedDict cache in `llm_provider`
- document `AF_LLM_CACHE_SIZE` in code and README
- verify LRU eviction via new unit test

## Testing
- `python check_env.py --auto-install`
- `pytest -q`",OpenAI_Codex,24208299,MontrealAI,closed,2025-05-24T13:17:35Z,2025-05-24T13:17:45Z,2025-05-24T13:17:45Z,922805069,https://api.github.com/repos/MontrealAI/AGI-Alpha-Agent-v0,https://github.com/MontrealAI/AGI-Alpha-Agent-v0/pull/694,37,1.0,False
3104683212,1245,[alpha_factory] optimize rate limiter,"## Summary
- use TTLCache+deque for SimpleRateLimiter
- adjust demos and tests
- add regression test for throttling

## Testing
- `python check_env.py --auto-install`
- `pre-commit run --files src/interface/api_server.py alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py tests/test_rate_limiter_eviction.py tests/test_api_server_static.py` *(fails: could not fetch black)*
- `mypy --config-file mypy.ini src/interface/api_server.py alpha_factory_v1/demos/alpha_agi_insight_v1/src/interface/api_server.py tests/test_rate_limiter_eviction.py tests/test_api_server_static.py` *(fails: many errors)*
- `pytest -q tests/test_rate_limiter_eviction.py tests/test_api_server_static.py`

------
https://chatgpt.com/codex/tasks/task_e_683a659fc3748333bf8a6dbe3a466de4",OpenAI_Codex,24208299,MontrealAI,closed,2025-05-31T02:23:30Z,2025-05-31T02:23:59Z,2025-05-31T02:23:59Z,922805069,https://api.github.com/repos/MontrealAI/AGI-Alpha-Agent-v0,https://github.com/MontrealAI/AGI-Alpha-Agent-v0/pull/1245,37,1.0,False
3253128073,95,Modify exit to throttle on bandwidth exhaustion,"## Summary
- add fallback mode without exposing governor types
- create fallback limiter via `with_fallback`
- throttle using fallback rate when paid bandwidth is exhausted

## Testing
- `cargo check -p geph5-exit`
- `cargo test -p geph5-exit -- --help`


------
https://chatgpt.com/codex/tasks/task_b_687fa69770b8833381b6d4afa2c42b46",OpenAI_Codex,20490891,nullchinchilla,closed,2025-07-22T15:22:31Z,2025-07-22T15:23:40Z,2025-07-22T15:23:40Z,758681331,https://api.github.com/repos/geph-official/geph5,https://github.com/geph-official/geph5/pull/95,37,1.0,False
3243098553,300,Add cache size limit support,"## Summary
- implement `cache_size_limit` and `replacement_policy` parameters
- default to LRU policy and enforce limit in memory core
- document cache size limiting in README
- test LRU eviction behaviour
- update default params test

## Testing
- `ruff check .`
- `mypy src/cachier`
- `pytest -m ""memory or pickle"" --cov=cachier --cov-report=term`
- `./scripts/test-local.sh all` *(fails: Docker is not installed)*

------
https://chatgpt.com/codex/tasks/task_e_687a30b9d6dc8323931ef5c545e0c3bb",OpenAI_Codex,917954,shaypal5,open,2025-07-18T12:37:46Z,,,66466789,https://api.github.com/repos/python-cachier/cachier,https://github.com/python-cachier/cachier/pull/300,37,1.0,False
3274990408,410,Add runtime rate target config,"## Summary
- add `RuntimeConfig` with `rate_target_hz`
- propagate runtime config in `CuConfig`
- store runtime config in `CuRuntime`
- enforce rate target in generated `run` loop
- showcase rate target in new `cu_rate_target` example

## Testing
- `cargo check -p cu-rate-target`
- `cargo test -p cu29-runtime --no-run`


------
https://chatgpt.com/codex/tasks/task_e_688933dc67cc8330b6bb6971073465c6",OpenAI_Codex,975564,gbin,closed,2025-07-29T21:06:22Z,2025-07-30T02:12:42Z,2025-07-30T02:12:42Z,290790298,https://api.github.com/repos/copper-project/copper-rs,https://github.com/copper-project/copper-rs/pull/410,37,1.0,False
3089612967,892,Use LRU cache for preprocess,"## Summary
- speed up token preprocessing with `functools.lru_cache`
- add `PREPROCESS_CACHE_SIZE` setting

## Testing
- `pytest -q`",OpenAI_Codex,1231232,jdesboeufs,closed,2025-05-25T22:20:49Z,2025-06-01T21:24:58Z,,28606501,https://api.github.com/repos/addok/addok,https://github.com/addok/addok/pull/892,37,1.0,False
2927184629,1634,Add 1-second delay between restore changes and refresh webviews,"This PR adds a 1-second delay between restoring changes and refreshing webviews to ensure the restored files are properly loaded before refreshing the UI.

Link to Devin run: https://app.devin.ai/sessions/6cb56d14685b4640a1afb33e284ad503
Requested by: kiet@onlook.dev",Devin,158243242,devin-ai-integration[bot],closed,2025-03-18T05:22:54Z,2025-03-18T05:47:25Z,2025-03-18T05:47:25Z,820087727,https://api.github.com/repos/onlook-dev/onlook,https://github.com/onlook-dev/onlook/pull/1634,37,1.0,False
2778508344,1004,refactor: move processNode to main process,"Move processNode function from renderer to main process to prevent UI blocking.

Changes:
- Move processNode function from renderer to main process
- Add WebviewUpdate interface for type-safe IPC communication
- Update existing getTemplateNodeById calls to use direct IPC
- Batch webview updates to prevent UI blocking

Link to Devin run: https://app.devin.ai/sessions/707a1409d32244efb22e49a164a0a3bc",Devin,158243242,devin-ai-integration[bot],closed,2025-01-09T18:29:31Z,2025-01-09T18:45:26Z,,820087727,https://api.github.com/repos/onlook-dev/onlook,https://github.com/onlook-dev/onlook/pull/1004,37,0.04488503256933396,False
2776940475,1002,refactor(studio): make processNode non-blocking,"# Non-blocking processNode Implementation

This PR implements a non-blocking version of the `processNode` function in the AST manager to prevent UI blocking in the renderer process.

## Changes
- Replace synchronous DFS with chunked processing using requestIdleCallback
- Add batching system for DOM updates to reduce UI blocking
- Implement error handling for executeJavaScript calls
- Add timeout-based batching with configurable batch sizes

## Testing
The changes have been tested locally to ensure:
- UI remains responsive during tree traversal
- DOM updates are properly batched and executed
- Error handling works as expected
- No regression in existing functionality

Link to Devin run: https://app.devin.ai/sessions/750e15190ec34ad7bfdd90b903d85413
",Devin,158243242,devin-ai-integration[bot],closed,2025-01-09T06:30:25Z,2025-01-09T19:13:24Z,,820087727,https://api.github.com/repos/onlook-dev/onlook,https://github.com/onlook-dev/onlook/pull/1002,37,0.03439706023446577,False
