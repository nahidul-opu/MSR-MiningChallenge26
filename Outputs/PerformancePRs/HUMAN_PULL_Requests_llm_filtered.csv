id,number,title,user,user_id,state,created_at,closed_at,merged_at,repo_url,html_url,body,agent,llm_output
2419106029,87963,ref(span-buffer): Move from sets to arrays,untitaker,837573,closed,2025-03-26T11:30:27Z,2025-04-04T10:20:54Z,,https://api.github.com/repos/getsentry/sentry,https://github.com/getsentry/sentry/pull/87963,"Arrays might be faster as they might not run comparisons on payloads to
determine whether they are unique. However, they might not be as each
item has to be copied individually in Lua.
",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""ref(span-buffer): Move from sets to arrays"". Body: ""Arrays might be faster as they might not run comparisons on payloads to determine whether they are unique. However, they might not be as each item has to be copied individually in Lua.""

This is about performance: moving from sets to arrays to potentially improve speed. Contains ""faster"". So classification: performance.assistantfinalperformance"
2258009014,14887,feat: Avoid class fields all-together,mydea,2411343,closed,2025-01-02T15:00:27Z,2025-01-03T08:07:49Z,2025-01-03T08:07:49Z,https://api.github.com/repos/getsentry/sentry-javascript,https://github.com/getsentry/sentry-javascript/pull/14887,"We already have an eslint rule to avoid class fields, but had exceptions for static fields as well as for arrow functions.

This also leads to bundle size increases, so removing the exceptions and handling the (few) exceptions we have there should save some bytes.

Additionally, this has additional challenges if we want to avoid/reduce polyfills, as class fields need to be polyfilled for ES2020, sadly.

Found as part of https://github.com/getsentry/sentry-javascript/pull/14882",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""feat: Avoid class fields all-together"". Body: mentions eslint rule to avoid class fields, exceptions for static fields and arrow functions. Also mentions bundle size increases, removing exceptions and handling few exceptions should save some bytes. Also mentions polyfills for ES2020. So it's about reducing bundle size, removing polyfills, performance improvement. The keywords: ""bundle size increases"", ""save some bytes"", ""polyfills"", ""performance"". So it's performance. So classification: performance.assistantfinalperformance"
2623753154,7541,chore: skip s3 LIST for projects configured in env,maxdeichmann,17686849,closed,2025-06-27T09:24:33Z,2025-06-27T11:23:38Z,2025-06-27T11:23:38Z,https://api.github.com/repos/langfuse/langfuse,https://github.com/langfuse/langfuse/pull/7541,"
<!-- ELLIPSIS_HIDDEN -->



> [!IMPORTANT]
> Add environment variable to skip S3 list for specified project IDs in `processEventBatch`.
> 
>   - **Environment**:
>     - Add `LANGFUSE_SKIP_S3_LIST_FOR_OBSERVATIONS_PROJECT_IDS` to `env.ts` as an optional string.
>   - **Ingestion Logic**:
>     - In `processEventBatch` in `processEventBatch.ts`, parse `LANGFUSE_SKIP_S3_LIST_FOR_OBSERVATIONS_PROJECT_IDS` to determine `projectIdsToSkipS3List`.
>     - Modify `shouldSkipS3List` logic to skip S3 list if project ID is in `projectIdsToSkipS3List` or source is 'otel'.
> 
> <sup>This description was created by </sup>[<img alt=""Ellipsis"" src=""https://img.shields.io/badge/Ellipsis-blue?color=175173"">](https://www.ellipsis.dev?ref=langfuse%2Flangfuse&utm_source=github&utm_medium=referral)<sup> for e6bdf0a275ea26d5def8c4123b977d160af9610c. You can [customize](https://app.ellipsis.dev/langfuse/settings/summaries) this summary. It will automatically update as commits are pushed.</sup>

<!-- ELLIPSIS_HIDDEN -->",Human,"analysisWe need to classify as performance or non-performance. The PR is about skipping S3 LIST for projects configured in env. It's a change to skip listing S3 for certain projects. This is likely a performance improvement: skipping S3 list reduces latency, reduces API calls, improves performance. The description mentions ""skip S3 list"" which is a performance optimization. So classification: performance.assistantfinalperformance"
2576376243,1911,[rollout] feat: Support Multi-stage Awake for SGLang,hebiao064,11166516,closed,2025-06-08T06:51:40Z,2025-06-23T21:03:35Z,2025-06-23T21:03:35Z,https://api.github.com/repos/volcengine/verl,https://github.com/volcengine/verl/pull/1911,"Co-authored with: MrAta (immrata@gmail.com) and @zhaochenyang20 

### Checklist Before Starting

- [x] Search for similar PR(s).

### What does this PR do?

### Motivation

In RL Ecosystem which use colocate design like [verl](https://github.com/volcengine/verl/tree/main), we need to offload training model and load serving model & KV Cache frequently.


#### Background
- Currently SGLang is using [torch_memory_saver](https://github.com/fzyzcjy/torch_memory_saver) to pause and resume.
- [torch_memory_saver](https://github.com/fzyzcjy/torch_memory_saver) is a open source repo that provided easy to use api to hack **cudaMalloc** and **cudaFree** to make sure the virtual address could be consistent after pause and resume, which is critical to ensure CUDA Graph work.
- CUDA Graph is critical to make sure SGLang runs faster in decoding phases.


#### Here is the current behavior of VERL + SGLang

![Image](https://github.com/user-attachments/assets/e87e7dd6-f223-4de6-8f07-915eb2030ea8)

1. During Training, we have training model and optimizer state in the GPU Memory, and once training is done, we will offload optimizer state to cpu and keep the model weights in GPU, which is needed in Update Weight.
2. During Update Weight, we awake the SGLang engine, so those paused memory of Model Weights and KV Cache will come back. Then we update model from training model to serving model on the fly using the api: `update_weights_in_tensor`
3. After Model being updated, we delete the training model from GPU Memory.


Above design works pretty well so far, however, this would waste a big chunk of GPU Memory during rollout, which could cause a few issues we've seen so far:
- **Small KV Cache**: We need to use relative lower number of mem fraction ratio (e.g: 0.6), hence our KV Cache has less tokens. Given KV Cache has less tokens, we will hit `RuntimeError: Prefill out of memory. Try to lower your batch size.` when we try prefill large number of requests.
- **Out of Memory**: If we use mem fraction ratio 0.8 and run RL for 32B model on 8 H100, it will OOM during update weight


#### Challenge
- `torch_memory_saver` currently only supports Singleton, hence SGLang will pause and resume KV Cache + Weights together, they are treated as the same group of memory controlled by the singleton `torch_memory_saver` instance

#### Proposal
![Image](https://github.com/user-attachments/assets/7fda9638-0dc2-4c14-bc64-cd20616f350f)

1. During Training, we do the same
2. During Update Weight Stage 1, we awake the model weights from SGLang and then update weights
3. During Update Weight Stage 2, we delete the training model weights from GPU Memory
4. Awake the SGLang's KV Cache


![Image](https://github.com/user-attachments/assets/f3dab327-dc2e-4ed8-88d7-15e383f77d25)


### Benefit
With above feature, we can train larger model with same GPU, we can also make training/rollout more efficient given we can allocate larger KV Cache

### Solution: Keep using Singleton and provide tag based pause/resume

- [x] Support tag based resume/pause: https://github.com/fzyzcjy/torch_memory_saver/pull/20
- [x] Support Multiple Stage Awake in SGLang: https://github.com/sgl-project/sglang/pull/7099
- [ ] Support Multiple Stage Awake in verl: https://github.com/volcengine/verl/pull/1911

### High-Level Design

> Demonstrate the high-level design if this PR is complex.

### Specific Changes

> List the specific changes.

### API

> Demonstrate how the API changes if any.

### Usage Example

> Provide usage example(s) for easier usage.

```python
# Add code snippet or script demonstrating how to use this 
```

### Test

![Screenshot 2025-06-19 at 12 16 19‚ÄØPM](https://github.com/user-attachments/assets/a95dd57e-43e1-4f28-8a84-003ec5c043fc)
![Screenshot 2025-06-19 at 12 13 14‚ÄØPM](https://github.com/user-attachments/assets/f1f4a8a8-1845-4fad-9424-5526d4154dd0)


### Additional Info.

- **Issue Number**: Fixes issue # or discussion # if any.
- **Training**: [Note which backend this PR will affect: FSDP, Megatron, both, or none]
- **Inference**: [Note which backend this PR will affect: vLLM, SGLang, both, or none]

### Checklist Before Submitting

- [ ] Read the [Contribute Guide](https://github.com/volcengine/verl?tab=readme-ov-file#contribution-guide).
- [ ] Apply [pre-commit checks](https://github.com/volcengine/verl?tab=readme-ov-file#code-linting-and-formatting).
- [ ] Add `[BREAKING]` to the PR title if it breaks any API.
- [ ] Update the documentation about your changes in the [docs](https://github.com/volcengine/verl/tree/main/docs).
- [ ] New CI unit test(s) are added to cover the code path.
- [ ] Rely on existing unit tests on CI that covers the code path.
",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""[rollout] feat: Support Multi-stage Awake for SGLang"". Body describes memory management, GPU memory usage, training/rollout, OOM, etc. It mentions performance: ""CUDA Graph is critical to make sure SGLang runs faster in decoding phases."" Also ""Benefit: With above feature, we can train larger model with same GPU, we can also make training/rollout more efficient given we can allocate larger KV Cache"". So it's about improving performance, memory usage, efficiency. Keywords: performance, optimization, memory, OOM, efficient. So classify as 'performance'.assistantfinalperformance"
2324586321,17207,feat: Support codesigning macOS executables in `bun build --compile`,Pranav2612000,20909078,closed,2025-02-09T15:27:56Z,2025-02-25T09:34:11Z,2025-02-25T09:34:11Z,https://api.github.com/repos/oven-sh/bun,https://github.com/oven-sh/bun/pull/17207,"### What does this PR do?

Based off #15525, this PR adds support for code signing macos executables. This is done with 2 changes:

1. Support codesigning macOS executables in bun build --compile
2. Deletes the hack where we read the last 4096 bytes of the currently running executables itself to see if it ends with ""-- Bun --\n"" . We now add a __BUN,__bun section in the macho executable so all we need to do is a single pointer lookup to find out if the current executable is a standalone executable and to get the data. This will also reduce memory usage because we no longer need to do another memory allocation to read through the standalone executable's code.

### How did you verify your code works?
- Ensured that bun is able to generate binaries which are codesignable.

<!-- **For code changes, please include automated tests**. Feel free to uncomment the line below -->

<!-- I wrote automated tests -->

<!-- If JavaScript/TypeScript modules or builtins changed:

- [ ] I included a test for the new code, or existing tests cover it
- [ ] I ran my tests locally and they pass (`bun-debug test test-file-name.test`)

-->

<!-- If Zig files changed:

- [ ] I checked the lifetime of memory allocated to verify it's (1) freed and (2) only freed when it should be
- [ ] I included a test for the new code, or an existing test covers it
- [ ] JSValue used outside outside of the stack is either wrapped in a JSC.Strong or is JSValueProtect'ed
- [ ] I wrote TypeScript/JavaScript tests and they pass locally (`bun-debug test test-file-name.test`)
-->

<!-- If new methods, getters, or setters were added to a publicly exposed class:

- [ ] I added TypeScript types for the new methods, getters, or setters
-->

<!-- If dependencies in tests changed:

- [ ] I made sure that specific versions of dependencies are used instead of ranged or tagged versions
-->

<!-- If a new builtin ESM/CJS module was added:

- [ ] I updated Aliases in `module_loader.zig` to include the new module
- [ ] I added a test that imports the module
- [ ] I added a test that require() the module
-->


Fixes #7208 
",Human,"analysisWe need to classify as performance or non-performance. The PR adds code signing support and removes hack reading last 4096 bytes, adding a section to Mach-O. It reduces memory usage and uses single pointer lookup. That is a performance improvement: reduces memory usage, faster lookup. The keywords: ""reduce memory usage"", ""single pointer lookup"". It's performance. So label 'performance'.assistantfinalperformance"
2301234632,16824,Glob Match Rewrite,probably-neb,17836474,closed,2025-01-28T02:49:57Z,2025-02-14T14:32:57Z,2025-02-14T14:32:57Z,https://api.github.com/repos/oven-sh/bun,https://github.com/oven-sh/bun/pull/16824,"### What does this PR do?

<!-- **Please explain what your changes do**, example: -->

Rewrites the glob match implementation (both ascii and non-ascii) by porting [the fast-glob crate](https://github.com/shulaoda/fast-glob)

This fixes #14934 as well as a few other issues I found along the way.
Additionally, based on my local testing it results in a large perf win.

The perf win I am referring to was measured by running each test in `test/js/bun/glob/match.test.ts` 10,000 times each and comparing the total runtime for the current and my new implementation. On my machine I get an average of a little over 200x faster than the current implementation. I'd be happy to upload the code for this comparison to github and share if it is of interest.

I didn't do very much more in depth perf testing because my main focus with the PR was to fix the aforementioned issues with the current implementation.

The high-level overview of the change is instead of keeping a track of a `BraceStack` to deal with braces `{foo,bar}`, it recursively creates new globs for each branch (i.e. `1{foo,bar}2` -> [`1foo2`,`1bar2`]). 
This does mean, however, that `matchImpl` must now take in an allocator parameter in order to create the sub globs. For better performance the implementation basically uses a `std.stackFallback` to stack allocate sub globs less than a predetermined length defined by the `GLOB_STACK_BUF_SIZE` variable in both `src/glob/ascii.zig` and `src/glob/GlobWalker.zig` (currently 64 in both places).

You will notice in the PR that I updated all callsites of `matchImpl` to pass in an instance of `Allocator`. I would appreciate if someone more familiar with the codebase than I would double check each place that I did so to make sure I'm using the allocator that makes the most sense in each context. Please note that in places where an allocator was not available in the context I used `bun.default_allocator` rather than change more function signatures, but I'm happy to change this if desired.

- [x] Code changes

### How did you verify your code works?

<!-- **For code changes, please include automated tests**. Feel free to uncomment the line below -->

I wrote automated tests 

<!-- If JavaScript/TypeScript modules or builtins changed:

- [x] I included a test for the new code, or existing tests cover it
- [x] I ran my tests locally and they pass (`bun-debug test test-file-name.test`)

-->



- [x] I checked the lifetime of memory allocated to verify it's (1) freed and (2) only freed when it should be
- [x] I included a test for the new code, or an existing test covers it
- [x] I wrote TypeScript/JavaScript tests and they pass locally (`bun-debug test test-file-name.test`)
",Human,"analysisWe need to classify as 'performance' or 'non-performance'. The PR title: ""Glob Match Rewrite"". Body mentions rewriting glob match implementation, porting fast-glob crate, fixes issues, and ""Additionally, based on my local testing it results in a large perf win."" They mention 200x faster. So it's performance improvement. So classification: 'performance'.assistantfinalperformance"
2441390805,18804,fix(node:http) resume when reading and avoid unnecessary pause/resumes calls,cirospaciari,6379399,closed,2025-04-06T00:07:32Z,2025-04-08T00:30:16Z,2025-04-08T00:30:16Z,https://api.github.com/repos/oven-sh/bun,https://github.com/oven-sh/bun/pull/18804,"### What does this PR do?
Fix: https://github.com/oven-sh/bun/issues/18799
<!-- **Please explain what your changes do**, example: -->

<!--

This adds a new flag --bail to bun test. When set, it will stop running tests after the first failure. This is useful for CI environments where you want to fail fast.

-->

- [ ] Documentation or TypeScript types (it's okay to leave the rest blank in this case)
- [x] Code changes

### How did you verify your code works?
Tests
<!-- **For code changes, please include automated tests**. Feel free to uncomment the line below -->

<!-- I wrote automated tests -->

<!-- If JavaScript/TypeScript modules or builtins changed:

- [ ] I included a test for the new code, or existing tests cover it
- [ ] I ran my tests locally and they pass (`bun-debug test test-file-name.test`)

-->

<!-- If Zig files changed:

- [ ] I checked the lifetime of memory allocated to verify it's (1) freed and (2) only freed when it should be
- [ ] I included a test for the new code, or an existing test covers it
- [ ] JSValue used outside of the stack is either wrapped in a JSC.Strong or is JSValueProtect'ed
- [ ] I wrote TypeScript/JavaScript tests and they pass locally (`bun-debug test test-file-name.test`)
-->

<!-- If new methods, getters, or setters were added to a publicly exposed class:

- [ ] I added TypeScript types for the new methods, getters, or setters
-->

<!-- If dependencies in tests changed:

- [ ] I made sure that specific versions of dependencies are used instead of ranged or tagged versions
-->

<!-- If a new builtin ESM/CJS module was added:

- [ ] I updated Aliases in `module_loader.zig` to include the new module
- [ ] I added a test that imports the module
- [ ] I added a test that require() the module
-->
",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""fix(node:http) resume when reading and avoid unnecessary pause/resumes calls"". The description: ""Fix: https://github.com/oven-sh/bun/issues/18799"". It seems to fix node:http resume behavior, avoid unnecessary pause/resumes calls. That is about performance? It might be about improving performance by reducing unnecessary pause/resume calls. The keywords: ""resume"", ""pause"", ""unnecessary"". The description says ""avoid unnecessary pause/resumes calls"". That is a performance improvement. The issue link might be about performance. The PR is about node:http. The description: ""Fix: https://github.com/oven-sh/bun/issues/18799"". The issue likely about performance. The PR is about avoiding unnecessary pause/resume calls, which is a performance improvement. So classify as 'performance'.

Check if any other classification: The PR is about node:http, but the change is to avoid unnecessary pause/resume calls, which is a performance improvement. So classification: performance.assistantfinalperformance"
2425248848,18585,avoid encoding as double in `napi_create_double` if possible,dylan-conway,35280289,closed,2025-03-28T13:01:23Z,2025-03-28T22:16:32Z,2025-03-28T22:16:32Z,https://api.github.com/repos/oven-sh/bun,https://github.com/oven-sh/bun/pull/18585,"### What does this PR do?
Arithmetic on numbers encoded as doubles in JSC seems to hit more slow paths compared to `NumberTag` numbers.

Fixes #9218

We might want to do this in other places. With this change in a debug build, fps goes from ~1 to ~100 on M4 max

<img width=""339"" alt=""Screenshot 2025-03-28 at 5 54 06‚ÄØAM"" src=""https://github.com/user-attachments/assets/2f4817a4-af4c-4d9e-a293-d98d478871be"" />
<!-- **Please explain what your changes do**, example: -->

<!--

This adds a new flag --bail to bun test. When set, it will stop running tests after the first failure. This is useful for CI environments where you want to fail fast.

-->

### How did you verify your code works?

<!-- **For code changes, please include automated tests**. Feel free to uncomment the line below -->

<!-- I wrote automated tests -->

<!-- If JavaScript/TypeScript modules or builtins changed:

- [ ] I included a test for the new code, or existing tests cover it
- [ ] I ran my tests locally and they pass (`bun-debug test test-file-name.test`)

-->

<!-- If Zig files changed:

- [ ] I checked the lifetime of memory allocated to verify it's (1) freed and (2) only freed when it should be
- [ ] I included a test for the new code, or an existing test covers it
- [ ] JSValue used outside of the stack is either wrapped in a JSC.Strong or is JSValueProtect'ed
- [ ] I wrote TypeScript/JavaScript tests and they pass locally (`bun-debug test test-file-name.test`)
-->

<!-- If new methods, getters, or setters were added to a publicly exposed class:

- [ ] I added TypeScript types for the new methods, getters, or setters
-->

<!-- If dependencies in tests changed:

- [ ] I made sure that specific versions of dependencies are used instead of ranged or tagged versions
-->

<!-- If a new builtin ESM/CJS module was added:

- [ ] I updated Aliases in `module_loader.zig` to include the new module
- [ ] I added a test that imports the module
- [ ] I added a test that require() the module
-->
",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""avoid encoding as double in `napi_create_double` if possible"". Body: mentions arithmetic on numbers encoded as doubles in JSC hits slow paths compared to NumberTag numbers. Fixes #9218. With change in debug build, fps goes from ~1 to ~100 on M4 max. So it's clearly performance improvement. So label 'performance'.assistantfinalperformance"
2435440515,18728,Remove JSC.Strong from NodeHTTPResponse,Jarred-Sumner,709451,open,2025-04-02T23:26:17Z,,,https://api.github.com/repos/oven-sh/bun,https://github.com/oven-sh/bun/pull/18728,"### What does this PR do?

Remove the `JSC.Strong` for the `Promise` from `NodeHTTPResponse`. This means that the only strong reference to the NodeHTTPResponse object comes from the socket.

Note that this won't impact promise rejections because those are kept as strong inside the JSGlobalObject.

### How did you verify your code works?

CI",Human,"analysisWe need to classify whether the primary purpose is performance. The PR removes JSC.Strong from NodeHTTPResponse. That seems like a memory/performance optimization? It says only strong reference from socket, no impact on promise rejections. This is about memory management, possibly performance. The description: ""Remove the JSC.Strong for the Promise from NodeHTTPResponse. This means that the only strong reference to the NodeHTTPResponse object comes from the socket."" That is about memory usage, maybe performance. The keywords: memory, performance, optimization. So likely performance. The classification: 'performance'.assistantfinalperformance"
2256956124,16095,Report memory cost of sourcemaps to GC,Jarred-Sumner,709451,closed,2025-01-01T09:32:16Z,2025-03-08T04:09:24Z,,https://api.github.com/repos/oven-sh/bun,https://github.com/oven-sh/bun/pull/16095,"### What does this PR do?

<!-- **Please explain what your changes do**, example: -->

<!--

This adds a new flag --bail to bun test. When set, it will stop running tests after the first failure. This is useful for CI environments where you want to fail fast.

-->

- [ ] Documentation or TypeScript types (it's okay to leave the rest blank in this case)
- [ ] Code changes

### How did you verify your code works?

<!-- **For code changes, please include automated tests**. Feel free to uncomment the line below -->

<!-- I wrote automated tests -->

<!-- If JavaScript/TypeScript modules or builtins changed:

- [ ] I included a test for the new code, or existing tests cover it
- [ ] I ran my tests locally and they pass (`bun-debug test test-file-name.test`)

-->

<!-- If Zig files changed:

- [ ] I checked the lifetime of memory allocated to verify it's (1) freed and (2) only freed when it should be
- [ ] I included a test for the new code, or an existing test covers it
- [ ] JSValue used outside outside of the stack is either wrapped in a JSC.Strong or is JSValueProtect'ed
- [ ] I wrote TypeScript/JavaScript tests and they pass locally (`bun-debug test test-file-name.test`)
-->

<!-- If new methods, getters, or setters were added to a publicly exposed class:

- [ ] I added TypeScript types for the new methods, getters, or setters
-->

<!-- If dependencies in tests changed:

- [ ] I made sure that specific versions of dependencies are used instead of ranged or tagged versions
-->

<!-- If a new builtin ESM/CJS module was added:

- [ ] I updated Aliases in `module_loader.zig` to include the new module
- [ ] I added a test that imports the module
- [ ] I added a test that require() the module
-->
",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Report memory cost of sourcemaps to GC"". The description: It's about reporting memory cost of sourcemaps to GC. That seems like memory usage, maybe performance. The keywords: memory, cost, GC. The instruction says performance-related PR may contain keywords like memory. So likely performance. The primary purpose: to report memory cost of sourcemaps to GC. That is about memory usage, which is performance. So classify as 'performance'.assistantfinalperformance"
2613893429,20612,Optimize  `napi_get_value_string_utf8` `napi_get_value_string_latin1`  `napi_get_value_string_utf16`,Jarred-Sumner,709451,closed,2025-06-24T07:24:20Z,2025-06-25T00:39:33Z,2025-06-25T00:39:33Z,https://api.github.com/repos/oven-sh/bun,https://github.com/oven-sh/bun/pull/20612,"

### What does this PR do?

Avoid resolving string slices

Check for exceptions when resolving rope strings

### How did you verify your code works?

It compiled",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Optimize `napi_get_value_string_utf8` `napi_get_value_string_latin1` `napi_get_value_string_utf16`"". The body: ""Avoid resolving string slices"", ""Check for exceptions when resolving rope strings"". The description: ""Optimize"" suggests performance improvement. The keywords: ""Optimize"" is a performance keyword. Also ""Avoid resolving string slices"" likely improves performance. So classification: performance.assistantfinalperformance"
2556590402,20102,fix memory leak when pipe Bun.spawn stdio is never read repeatedly,nektro,5464072,closed,2025-05-31T10:46:27Z,2025-05-31T18:16:49Z,2025-05-31T18:16:49Z,https://api.github.com/repos/oven-sh/bun,https://github.com/oven-sh/bun/pull/20102,"calls to `.ensureUnusedCapacity()` elsewhere were not previously accounted for upon the closing of the PipeReader.

Closes: https://github.com/oven-sh/bun/issues/18265

----

results of the new test:

before:

```
test/js/bun/spawn/spawn-noread-leak.test.ts:
before 36.266 MB
after 369.531 MB
```

after:

```
test/js/bun/spawn/spawn-noread-leak.test.ts:
before 35.344 MB
after 58.313 MB
```

----

additionally using the `spawn` function from the linked issue:

```js
async function spawn() {
  const proc = Bun.spawn([""sed"", ""20q"", ""/etc/passwd""], {
    stdio: [""ignore"", ""pipe"", ""pipe""],
  });
  await proc.exited;
}

setTimeout(() => process.exit(0), 30_000);

while (true) {
  Bun.gc(true);
  console.clear();
  console.log(`RSS: ${(process.memoryUsage().rss / 1024 / 1024).toFixed(3)} MB`);
  await spawn();
}
```

before:
`RSS: 310.938 MB`
after:
`RSS: 38.328 MB`

and increasing the simulation to 2 mins:

before:
`RSS: 1090.328 MB`
after:
`RSS: 38.266 MB`
",Human,"analysisWe need to classify the pull request as performance or non-performance. The title: ""fix memory leak when pipe Bun.spawn stdio is never read repeatedly"". Body describes memory leak, tests show memory usage before and after. It's about memory leak, which is a performance issue. The description includes memory usage, leak, etc. So it's performance. The classification: 'performance'.assistantfinalperformance"
2256920866,2958,fix: slow lastalert query,shahargl,12069200,closed,2025-01-01T08:01:35Z,2025-01-01T08:13:06Z,2025-01-01T08:13:06Z,https://api.github.com/repos/keephq/keep,https://github.com/keephq/keep/pull/2958,close https://github.com/keephq/keep/issues/2957,Human,"analysisWe need to classify whether the primary purpose is performance. The title: ""fix: slow lastalert query"". That indicates performance issue: slow query. So classification: performance.assistantfinalperformance"
2602215850,18,Add comprehensive VibeTunnel protocol benchmark tool,hjanuschka,2891702,closed,2025-06-18T15:53:42Z,2025-06-18T21:38:11Z,2025-06-18T21:38:11Z,https://api.github.com/repos/amantus-ai/vibetunnel,https://github.com/amantus-ai/vibetunnel/pull/18,"Features:
- Complete HTTP API client implementation for VibeTunnel protocol
- Session management benchmarks (create/get/list/delete operations)
- SSE streaming performance testing with latency measurements
- Concurrent user load testing with realistic simulation
- Support for custom hostname/port configuration
- Detailed performance statistics and reporting

Commands:
- session: Test session lifecycle performance
- stream: Benchmark SSE streaming latency/throughput
- load: Concurrent user load testing

Tested against both Go (port 4031) and Rust (port 4044) servers. Tool successfully creates sessions and measures performance metrics.

ü§ñ Generated with [Claude Code](https://claude.ai/code)",Human,"analysisWe need to classify as performance or non-performance. The PR adds a benchmark tool for VibeTunnel protocol, with performance metrics, latency, throughput, load testing. It's clearly performance. So label 'performance'.assistantfinalperformance"
2606867877,33,feat: POSIX-first cross-platform process checking,hjanuschka,2891702,closed,2025-06-20T09:51:55Z,2025-06-20T09:52:31Z,2025-06-20T09:52:31Z,https://api.github.com/repos/amantus-ai/vibetunnel,https://github.com/amantus-ai/vibetunnel/pull/33,"## Summary
Add cross-platform process checking that prioritizes efficient `kill(pid, 0)` on POSIX systems and only uses gopsutil on Windows.

## Platform Strategy
### üêß **POSIX Systems (Linux, macOS, FreeBSD)**
- **Method**: `kill(pid, 0)` via `os.FindProcess().Signal(syscall.Signal(0))`
- **Why**: Most efficient - direct system call, no external dependencies
- **Performance**: Fastest possible process existence check

### ü™ü **Windows Only** 
- **Method**: `gopsutil.PidExists()`
- **Why**: Windows lacks `kill()` and signal support
- **Necessity**: Only cross-platform option available

## Implementation Logic
```go
if runtime.GOOS == ""windows"" {
    // Windows: use gopsutil (only option)
    return process.PidExists(int32(pid))
}

// POSIX: use efficient kill(pid, 0) 
return osProcess.Signal(syscall.Signal(0)) == nil
```

## Benefits
- ‚úÖ **Maximum Efficiency**: POSIX systems use native `kill(pid, 0)`
- ‚úÖ **Windows Compatible**: gopsutil handles Windows process checking
- ‚úÖ **Platform Optimal**: Each OS uses its most efficient method  
- ‚úÖ **Minimal Overhead**: No unnecessary dependencies on POSIX
- ‚úÖ **Proven Approach**: `kill(pid, 0)` is the standard Unix method

## Testing
- [x] Builds successfully on POSIX systems
- [x] Preserves efficient `kill(pid, 0)` as primary method
- [x] Adds Windows support via gopsutil fallback
- [ ] Windows integration testing needed

ü§ñ Generated with [Claude Code](https://claude.ai/code)",Human,"analysisWe need to classify as performance or non-performance. The PR is about improving performance by using kill(pid,0) instead of gopsutil on POSIX. It explicitly mentions ""Most efficient"", ""Fastest possible process existence check"", ""Maximum Efficiency"". So it's performance. So output 'performance'.assistantfinalperformance"
2609611207,1213,perf(engine): Disable worker eager execution to try distribute load,daryllimyt,5508348,closed,2025-06-22T09:46:03Z,2025-06-22T09:47:52Z,2025-06-22T09:47:52Z,https://api.github.com/repos/TracecatHQ/tracecat,https://github.com/TracecatHQ/tracecat/pull/1213,"
    
<!-- This is an auto-generated description by cubic. -->

## Summary by cubic
Disabled eager activity execution for Temporal workers to help distribute load more evenly.

- **Config**
  - Added TEMPORAL__DISABLE_EAGER_ACTIVITY_EXECUTION environment variable to control this behavior.

<!-- End of auto-generated description by cubic. -->

",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""perf(engine): Disable worker eager execution to try distribute load"". The body: ""Disabled eager activity execution for Temporal workers to help distribute load more evenly."" This is about performance: distributing load, disabling eager execution to improve performance. So classification: performance.assistantfinalperformance"
2533141282,3777,Refactor: Optimize React component rendering and improve robustness,hannesrudolph,49103247,closed,2025-05-21T04:00:12Z,2025-05-31T00:56:01Z,,https://api.github.com/repos/RooCodeInc/Roo-Code,https://github.com/RooCodeInc/Roo-Code/pull/3777,"Grey Screen Fix, jules experiment

This commit introduces several changes aimed at improving UI rendering performance and overall robustness, potentially addressing ""grey screen"" or unresponsiveness issues.

1.  **Optimized Memoization for Chat Rows:**
    *   Replaced generic `deepEqual` with custom comparison functions for `React.memo` in `ChatRow.tsx` and `BrowserSessionRow.tsx`. These custom functions perform more targeted comparisons of props, focusing only on fields relevant to rendering, which should reduce the overhead of memoization and prevent unnecessary re-renders.
    *   The internal `ChatRowContentComponent` in `ChatRow.tsx` was also wrapped with `React.memo`.

2.  **Increased Robustness:**
    *   Added `try-catch` blocks around `JSON.parse` calls within `BrowserSessionRow.tsx` to prevent runtime errors from malformed JSON in message text.

3.  **Code Analysis Confirmations:**
    *   Analysis of `ChatView.tsx` indicated that its `useEffect` dependency arrays were already in a reasonably optimized state.
    *   Review of `ClineProvider.ts` confirmed that its `dispose` method is comprehensive and correctly wired to the `onDidDispose` event of webview panels, ensuring cleanup of tab-specific provider instances.
    *   Review of `ShadowCheckpointService.ts` confirmed that the `renameNestedGitRepos` method and its usage in `stageAll` include appropriate `try...catch` and `try...finally` blocks for robust handling of file system operations.

These changes collectively aim to make the UI more efficient and the extension more stable.

<!--
Thank you for contributing to Roo Code!

Before submitting your PR, please ensure:
- It's linked to an approved GitHub Issue.
- You've reviewed our [Contributing Guidelines](../CONTRIBUTING.md).
-->

### Related GitHub Issue

<!-- Every PR MUST be linked to an approved issue. -->

Closes: # <!-- Replace with the issue number, e.g., Closes: #123 -->

### Description

<!--
Briefly summarize the changes in this PR and how they address the linked issue.
The issue should cover the ""what"" and ""why""; this section should focus on:
- The ""how"": key implementation details, design choices, or trade-offs made.
- Anything specific reviewers should pay attention to in this PR.
-->

### Test Procedure

<!--
Detail the steps to test your changes. This helps reviewers verify your work.
- How did you test this specific implementation? (e.g., unit tests, manual testing steps)
- How can reviewers reproduce your tests or verify the fix/feature?
- Include relevant testing environment details if applicable.
-->

### Type of Change

<!-- Mark all applicable boxes with an 'x'. -->

- [ ] üêõ **Bug Fix**: Non-breaking change that fixes an issue.
- [ ] ‚ú® **New Feature**: Non-breaking change that adds functionality.
- [ ] üí• **Breaking Change**: Fix or feature that would cause existing functionality to not work as expected.
- [ ] ‚ôªÔ∏è **Refactor**: Code change that neither fixes a bug nor adds a feature.
- [ ] üíÖ **Style**: Changes that do not affect the meaning of the code (white-space, formatting, etc.).
- [ ] üìö **Documentation**: Updates to documentation files.
- [ ] ‚öôÔ∏è **Build/CI**: Changes to the build process or CI configuration.
- [ ] üßπ **Chore**: Other changes that don't modify `src` or test files.

### Pre-Submission Checklist

<!-- Go through this checklist before marking your PR as ready for review. -->

- [ ] **Issue Linked**: This PR is linked to an approved GitHub Issue (see ""Related GitHub Issue"" above).
- [ ] **Scope**: My changes are focused on the linked issue (one major feature/fix per PR).
- [ ] **Self-Review**: I have performed a thorough self-review of my code.
- [ ] **Code Quality**:
    - [ ] My code adheres to the project's style guidelines.
    - [ ] There are no new linting errors or warnings (`npm run lint`).
    - [ ] All debug code (e.g., `console.log`) has been removed.
- [ ] **Testing**:
    - [ ] New and/or updated tests have been added to cover my changes.
    - [ ] All tests pass locally (`npm test`).
    - [ ] The application builds successfully with my changes.
- [ ] **Branch Hygiene**: My branch is up-to-date (rebased) with the `main` branch.
- [ ] **Documentation Impact**: I have considered if my changes require documentation updates (see ""Documentation Updates"" section below).
- [ ] **Changeset**: A changeset has been created using `npm run changeset` if this PR includes user-facing changes or dependency updates.
- [ ] **Contribution Guidelines**: I have read and agree to the [Contributor Guidelines](../CONTRIBUTING.md).

### Screenshots / Videos

<!--
For UI changes, please provide before-and-after screenshots or a short video of the *actual results*.
This greatly helps in understanding the visual impact of your changes.
-->

### Documentation Updates

<!--
Does this PR necessitate updates to user-facing documentation?
- [ ] No documentation updates are required.
- [ ] Yes, documentation updates are required. (Please describe what needs to be updated or link to a PR in the docs repository).
-->

### Additional Notes

<!-- Add any other context, questions, or information for reviewers here. -->

### Get in Touch

<!--
Please provide your Discord username for reviewers or maintainers to reach you if they have questions about your PR
-->
",Human,"analysisWe need to classify as 'performance' or 'non-performance'. The PR title: ""Refactor: Optimize React component rendering and improve robustness"". Body mentions optimizing memoization, reducing overhead, improving UI rendering performance, addressing grey screen/unresponsiveness. Keywords: optimize, performance, rendering, memoization, reduce overhead, unresponsiveness. So it's performance. So output 'performance'.assistantfinalperformance"
2329360113,1146,feat: `UTApi.generateSignedURL`,juliusmarminge,51714798,closed,2025-02-11T17:46:33Z,2025-02-11T18:12:03Z,2025-02-11T18:12:03Z,https://api.github.com/repos/pingdotgg/uploadthing,https://github.com/pingdotgg/uploadthing/pull/1146,"

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->
## Summary by CodeRabbit

- **Documentation**
  - Updated API references with clearer guidelines for generating secure, presigned URLs.
  - Simplified explanations regarding the usage of the new `generateSignedURL` method.

- **New Features**
  - Introduced an optimized approach for generating secure URLs for private files that reduces latency.
  - Deprecated the legacy URL generation approach in favor of the new efficient solution.
  - Enhanced API responses with additional file access details, including a new `ufsUrl`.
  - Added configuration options for customizing file service host URLs.

- **Tests**
  - Added a new test suite to validate the functionality of the `generateSignedURL` method, including various scenarios for the `expiresIn` parameter.
<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,"analysisWe need to classify as performance or non-performance. The PR introduces a new method generateSignedURL, optimized approach reduces latency. That is performance improvement. So label 'performance'.assistantfinalperformance"
2259056356,379,Parallel tests,terzioglub,101180533,closed,2025-01-03T10:42:40Z,2025-01-03T11:08:51Z,2025-01-03T11:08:51Z,https://api.github.com/repos/bruin-data/bruin,https://github.com/bruin-data/bruin/pull/379,,Human,"analysisWe need to classify the pull request. Title: ""Parallel tests"". Body? Not provided. The title suggests tests run in parallel, which could improve performance. But is the primary purpose to improve performance? Running tests in parallel can speed up test suite, but the PR might be about test parallelization. That is a performance improvement. So classify as 'performance'. There's no other context. So answer: performance.assistantfinalperformance"
2356985296,470,Patch/reduce response size for internal parse commands,karakanb,16530606,closed,2025-02-25T15:05:26Z,2025-02-25T15:17:19Z,2025-02-25T15:17:19Z,https://api.github.com/repos/bruin-data/bruin,https://github.com/bruin-data/bruin/pull/470,"This PR removes unused elements from two internal commands, which were getting problematic in larger pipelines.

<img width=""406"" alt=""image"" src=""https://github.com/user-attachments/assets/54fd43c8-ede6-400a-8fa8-ce63544ebd24"" />
",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Patch/reduce response size for internal parse commands"". Body: ""This PR removes unused elements from two internal commands, which were getting problematic in larger pipelines."" The description: removing unused elements to reduce response size. That is about reducing response size, which is a performance improvement (memory usage, network). The keywords: ""response size"" is about performance. So classify as 'performance'.assistantfinalperformance"
2518037766,180,feat(os): memoize exec from file patterns,john-royal,34844819,closed,2025-05-14T03:01:11Z,2025-05-14T04:39:32Z,2025-05-14T04:39:32Z,https://api.github.com/repos/sam-goodwin/alchemy,https://github.com/sam-goodwin/alchemy/pull/180,"## Changes

1. Enhanced `ExecProps.memoize` to support file-based memoization:
   - Previously: `memoize?: boolean`
   - Now: `memoize?: boolean | { patterns: string[] }`
   - Patterns can be an array of file names or globs, relative to the given working directory if provided or `process.cwd()` otherwise (uses `glob` from `fs/promises`, compatible with Bun and Node.js >= 22)
2. Added tests verifying the new file pattern functionality:
   - Commands are memoized when files haven't changed
   - Commands are re-executed when files are modified

## Example

```typescript
const result = await Exec(""build"", {
  command: ""npm run build"",
  memoize: { patterns: [""src/**/*.ts"", ""package.json""] }
});
```",Human,"analysisWe need to classify whether the primary purpose of the pull request is to improve performance. The PR title: ""feat(os): memoize exec from file patterns"". The body describes adding memoization support for file patterns. Memoization is a performance optimization: caching results to avoid recomputation. The PR adds ability to memoize based on file patterns. This is a performance improvement. The keywords: memoize, performance, optimization. So classification: 'performance'.assistantfinalperformance"
2612511272,4944,DRAFT: e2e rn0.80.0 perf test fix,antonis,304044,closed,2025-06-23T18:44:56Z,2025-06-24T05:49:14Z,,https://api.github.com/repos/getsentry/sentry-react-native,https://github.com/getsentry/sentry-react-native/pull/4944,"## :loudspeaker: Type of change
<!--- Put an `x` in the boxes that apply -->
- [ ] Bugfix
- [ ] New feature
- [ ] Enhancement
- [ ] Refactoring


## :scroll: Description
<!--- Describe your changes in detail -->


## :bulb: Motivation and Context
<!--- Why is this change required? What problem does it solve? -->
<!--- If it fixes an open issue, please link to the issue here. -->


## :green_heart: How did you test it?


## :pencil: Checklist
<!--- Put an `x` in the boxes that apply -->
- [ ] I added tests to verify changes
- [ ] No new PII added or SDK only sends newly added PII if `sendDefaultPII` is enabled
- [ ] I updated the docs if needed.
- [ ] I updated the wizard if needed.
- [ ] All tests passing
- [ ] No breaking changes

## :crystal_ball: Next steps

#skip-changelog",Human,"analysisWe need to classify as performance or non-performance. The title: ""DRAFT: e2e rn0.80.0 perf test fix"". The body is empty. The title contains ""perf test fix"". That indicates performance. So classification: performance.assistantfinalperformance"
2482501128,3333,Chunking,six7,4548309,closed,2025-04-25T21:46:08Z,2025-05-10T10:01:24Z,,https://api.github.com/repos/tokens-studio/figma-plugin,https://github.com/tokens-studio/figma-plugin/pull/3333,wip PR to introduce chunking if > 100kb,Human,"analysisWe need to classify whether the primary purpose is to improve performance. The PR title: ""Chunking wip PR to introduce chunking if > 100kb"". Body: ""wip PR to introduce chunking if > 100kb"". This seems to be about chunking data if >100kb. That likely is to improve performance by chunking large payloads. The description: ""introduce chunking if > 100kb"". That is a performance improvement. So classify as 'performance'.assistantfinalperformance"
2564432253,3402,Github Sync Optimization,akshay-gupta7,9948167,open,2025-06-03T15:36:53Z,,,https://api.github.com/repos/tokens-studio/figma-plugin,https://github.com/tokens-studio/figma-plugin/pull/3402,"<!--
  Notes for authors:
  - Provide context with minimal words, keep it concise
  - Mark as a draft for work in progress PRs
  - Once ready for review, notify others in #code-reviews
  - Remember, the review process is a learning opportunity for both reviewers and authors, it's a way for us to share knowledge and avoid silos.
-->

### Why does this PR exist?

Resolves #3392 

<!--
  Describe the problem you're addressing and the rationale behind this PR.
-->

### What does this pull request do?

Currently, when syncing to GitHub(in multi file sync), we push all JSON files regardless of whether they've changed or not. This results in unnecessary writes, longer sync times, and potentially bloated commit histories.

This PR addresses it by creating a filtered Changeset when pushing to github, scanning for files only with a change, or potentially being deleted, and pushes only those changes in the commit API request. 
<!--
  Detailed summary of the changes, including any visual or interactive updates.
  For UI changes, add before/after screenshots. For interactive elements, consider including a video or an animated gif.
  Explain some of the choices you've made in the PR, if they're not obvious.
-->

### Testing this change

There is no direct way for a user to test this, but what can be done is that they can push a file with a lot of token sets, then make a small change and see how much time is it taking for them to push even a small change.

<!--
  Describe how this change can be tested. Are there steps required to get there? Explain what's required so a reviewer can test these changes locally.

  If you have a review link available, add it here.
-->

### Additional Notes (if any)

<!--
  Add any other context or screenshots about the pull request
-->
",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Github Sync Optimization"". Body: mentions unnecessary writes, longer sync times, commit histories. It addresses filtering changeset to push only changed files. This is about performance optimization. Keywords: ""performance"", ""optimization"", ""longer sync times"". So classification: performance.assistantfinalperformance"
2597381928,2223,compress image before upload,spartan-vutrannguyen,134599728,closed,2025-06-17T02:48:02Z,2025-06-17T17:18:07Z,2025-06-17T17:18:07Z,https://api.github.com/repos/onlook-dev/onlook,https://github.com/onlook-dev/onlook/pull/2223,"## Description

<!-- Provide a clear and concise description of your changes -->
- Compress images before uploading a local project
## Related Issues

<!-- Link any related issues using GitHub keywords (e.g., ""closes #123"", ""fixes #456"", ""related to #789"") -->

## Type of Change

<!-- Put an `x` in the boxes that apply -->

- [x] Bug fix
- [ ] New feature
- [ ] Documentation update
- [ ] Release
- [ ] Refactor
- [ ] Other (please describe):

## Testing

<!-- Describe the tests you ran or the steps to verify your changes -->

## Screenshots (if applicable)

<!-- Add screenshots to help explain your changes -->

## Additional Notes

<!-- Add any other context about the PR here -->
",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""compress image before upload"". Body: description: ""Compress images before uploading a local project"". This is about image compression before upload. That is a performance improvement? It reduces upload size, improves performance. But is it a performance-related PR? The guidelines: performance-related PR may contain keywords like performance, optimization, etc. This PR does not contain those keywords. But the content is about compressing images before upload, which is a performance improvement. The question: ""determine whether the primary purpose of the pull request is to improve performance."" The PR is about compressing images before upload, which is a performance improvement. So classification: performance. The guidelines mention that performance-related PR may contain keywords, but not required. The content indicates performance improvement. So answer: performance.assistantfinalperformance"
2590522777,2179,bug: fixed Click-and-drag frames from their top-bar is extremely slow,PradyumnChauhan,140681043,closed,2025-06-13T14:59:16Z,2025-06-13T22:10:10Z,,https://api.github.com/repos/onlook-dev/onlook,https://github.com/onlook-dev/onlook/pull/2179,"## Description
Fixed frame dragging performance and smoothness issues. The changes include:

- Optimized drag handling with requestAnimationFrame
- Added proper TypeScript types for refs
- Improved animation smoothness with CSS transitions
- Added cleanup for event listeners
- Implemented proper state management during drag operations

## Related Issues

Fixes #2151 

## Type of Change
- [x] Bug fix
- [ ] New feature
- [ ] Documentation update
- [ ] Release
- [ ] Refactor
- [ ] Other (please describe):


## Recording

https://github.com/user-attachments/assets/279b79fd-d318-40fa-a2bf-44c291ad760b



## Additional Notes

Key technical improvements:

- Used requestAnimationFrame for smooth animations
- Added proper cleanup for event listeners
- Implemented throttling for drag updates (60fps)
- Added willChange hint for better browser optimization
- Fixed TypeScript type issues with refs
- Added smooth transitions for better UX
- Performance metrics before and after the change:

- Reduced frame dropping during drag operations
- Smoother animation transitions
- More responsive UI feedback
- Better memory management with proper cleanup

<!-- ELLIPSIS_HIDDEN -->

----

> [!IMPORTANT]
> Optimize frame dragging performance and smoothness by using `requestAnimationFrame`, adding CSS transitions, and improving event handling in `index.tsx` and `top-bar.tsx`.
> 
>   - **Performance Improvements**:
>     - Use `requestAnimationFrame` in `top-bar.tsx` for smoother drag animations.
>     - Add CSS transitions in `index.tsx` for smoother frame movement.
>     - Implement throttling for drag updates to 60fps.
>   - **Event Handling**:
>     - Add cleanup for event listeners in `top-bar.tsx` using `useEffect`.
>     - Use `useCallback` for `handleMove`, `endMove`, and `handleMouseDown` in `top-bar.tsx`.
>   - **TypeScript and State Management**:
>     - Add TypeScript types for refs in `index.tsx`.
>     - Implement proper state management during drag operations in `top-bar.tsx`.
> 
> <sup>This description was created by </sup>[<img alt=""Ellipsis"" src=""https://img.shields.io/badge/Ellipsis-blue?color=175173"">](https://www.ellipsis.dev?ref=onlook-dev%2Fonlook&utm_source=github&utm_medium=referral)<sup> for 005fb445e2c847744c76fb518223421ad9d12022. You can [customize](https://app.ellipsis.dev/onlook-dev/settings/summaries) this summary. It will automatically update as commits are pushed.</sup>

<!-- ELLIPSIS_HIDDEN -->",Human,"analysisWe need to classify as 'performance' or 'non-performance'. The PR title: ""bug: fixed Click-and-drag frames from their top-bar is extremely slow"". Body mentions performance improvements: requestAnimationFrame, CSS transitions, throttling, etc. So it's performance. So output 'performance'.assistantfinalperformance"
2520828481,1839,syncing dev tab content,spartan-vutrannguyen,134599728,closed,2025-05-15T04:07:56Z,2025-05-15T06:55:12Z,2025-05-15T06:55:12Z,https://api.github.com/repos/onlook-dev/onlook,https://github.com/onlook-dev/onlook/pull/1839,"## Description

<!-- Provide a clear and concise description of your changes -->
- When updating the file content, the dev tools tab synchronizes this change immediately.
## Related Issues

<!-- Link any related issues using GitHub keywords (e.g., ""closes #123"", ""fixes #456"", ""related to #789"") -->

## Type of Change

<!-- Put an `x` in the boxes that apply -->

- [ ] Bug fix
- [ ] New feature
- [ ] Documentation update
- [ ] Release
- [ ] Refactor
- [ ] Other (please describe):

## Testing

<!-- Describe the tests you ran or the steps to verify your changes -->

## Screenshots (if applicable)

<!-- Add screenshots to help explain your changes -->

## Additional Notes

<!-- Add any other context about the PR here -->

<!-- ELLIPSIS_HIDDEN -->


----

> [!IMPORTANT]
> Synchronize dev tools tab with file content updates, handle file events, and update dependencies.
> 
>   - **Behavior**:
>     - Dev tools tab now synchronizes immediately with file content updates in `file-tree.tsx` and `index.tsx`.
>     - Handles file events for add, remove, and change to update file list and content.
>   - **File Handling**:
>     - `FileTree` component in `file-tree.tsx` now accepts `files` prop and uses `ResizeObserver` for width.
>     - `buildFileTree` function memoized to optimize performance.
>     - `loadNewContent` function added to `index.tsx` to update file content on change events.
>   - **Styling**:
>     - Removed `z-10` class from `PopoverContent` in `color-background.tsx`, `input-color.tsx`, and `text-selected.tsx`.
>   - **Dependencies**:
>     - Updated various dependencies in `bun.lock`, including `eslint`, `react`, and `next`.
> 
> <sup>This description was created by </sup>[<img alt=""Ellipsis"" src=""https://img.shields.io/badge/Ellipsis-blue?color=175173"">](https://www.ellipsis.dev?ref=onlook-dev%2Fonlook&utm_source=github&utm_medium=referral)<sup> for 854099927a7f871a82a4838914fe58163f71f466. You can [customize](https://app.ellipsis.dev/onlook-dev/settings/summaries) this summary. It will automatically update as commits are pushed.</sup>


<!-- ELLIPSIS_HIDDEN -->",Human,"analysisWe need to classify as performance or non-performance. The PR description: ""syncing dev tab content"" and mentions ""buildFileTree function memoized to optimize performance."" Also mentions ""immediately synchronizes"" but not necessarily performance improvement. The keywords: ""optimize performance"" is present. The description includes ""memoized to optimize performance."" That indicates performance improvement. So classify as 'performance'.assistantfinalperformance"
2530271776,1883,update font loader,spartan-vutrannguyen,134599728,closed,2025-05-20T04:10:38Z,2025-05-28T09:49:51Z,,https://api.github.com/repos/onlook-dev/onlook,https://github.com/onlook-dev/onlook/pull/1883,"## Description

<!-- Provide a clear and concise description of your changes -->

## Related Issues

<!-- Link any related issues using GitHub keywords (e.g., ""closes #123"", ""fixes #456"", ""related to #789"") -->

## Type of Change

<!-- Put an `x` in the boxes that apply -->

- [ ] Bug fix
- [ ] New feature
- [ ] Documentation update
- [ ] Release
- [ ] Refactor
- [ ] Other (please describe):

## Testing

<!-- Describe the tests you ran or the steps to verify your changes -->

## Screenshots (if applicable)

<!-- Add screenshots to help explain your changes -->

## Additional Notes

<!-- Add any other context about the PR here -->

<!-- ELLIPSIS_HIDDEN -->

----

> [!IMPORTANT]
> Update font loading to use Next.js optimization and add utilities for Google and local font loading.
> 
>   - **Font Loading**:
>     - Replaces `WebFont` loader with Next.js font optimization in `FontManager`.
>     - Introduces `ensureGoogleFontLoaded` in `fontPreview.ts` and `ensureLocalFontLoaded` in `localFontPreview.ts` to manage font loading.
>   - **Components**:
>     - Updates `FontFamily` and `FontVariant` components in `font-family.tsx` to use new font loading utilities.
>     - Adds `useEffect` hooks to check and set font loading status.
>   - **Misc**:
>     - Removes `WebFont` import from `font/index.ts`.
>     - Adjusts opacity and transition styles in `font-family.tsx` to reflect font loading status.
> 
> <sup>This description was created by </sup>[<img alt=""Ellipsis"" src=""https://img.shields.io/badge/Ellipsis-blue?color=175173"">](https://www.ellipsis.dev?ref=onlook-dev%2Fonlook&utm_source=github&utm_medium=referral)<sup> for 5c09f48f3c603f84ae0deefd09bd712cef742a0b. You can [customize](https://app.ellipsis.dev/onlook-dev/settings/summaries) this summary. It will automatically update as commits are pushed.</sup>

<!-- ELLIPSIS_HIDDEN -->",Human,"analysisWe need to classify as performance or non-performance. The PR updates font loader to use Next.js optimization and adds utilities for Google and local font loading. This is about font loading optimization, which is performance improvement. Keywords: optimization, Next.js optimization, font loading. So it's performance.assistantfinalperformance"
2439890840,1729,refactor: display font as their style,spartan-vutrannguyen,134599728,closed,2025-04-04T15:39:55Z,2025-04-04T20:59:24Z,2025-04-04T20:59:24Z,https://api.github.com/repos/onlook-dev/onlook,https://github.com/onlook-dev/onlook/pull/1729,"## Description

<!-- Provide a clear and concise description of your changes -->
- Display font as their style in the Font Panel and Font Input
- Refactor Font Panel for better performance.
## Related Issues

<!-- Link any related issues using GitHub keywords (e.g., ""closes #123"", ""fixes #456"", ""related to #789"") -->

## Type of Change

<!-- Put an `x` in the boxes that apply -->

- [ ] Bug fix
- [ ] New feature
- [ ] Documentation update
- [ ] Release
- [x] Refactor
- [ ] Other (please describe):

## Testing

<!-- Describe the tests you ran or the steps to verify your changes -->

## Screenshots (if applicable)

<!-- Add screenshots to help explain your changes -->
<img width=""1715"" alt=""image"" src=""https://github.com/user-attachments/assets/3afb7898-b73e-4423-869f-ac68e319dc32"" />

## Additional Notes

<!-- Add any other context about the PR here -->

<!-- ELLIPSIS_HIDDEN -->


----

> [!IMPORTANT]
> Refactor font management to display fonts in their styles, improve performance with batch loading, and add search functionality.
> 
>   - **Behavior**:
>     - Fonts are now displayed in their respective styles in `FontInput.tsx` and `FontFamily.tsx`.
>     - `FontManager` in `index.ts` refactored to load fonts in batches and support font searching.
>     - Added tooltips for font names in `FontInput.tsx` and `FontFamily.tsx`.
>   - **Performance**:
>     - Batch loading of fonts implemented in `FontManager` to improve performance.
>     - Debounced search functionality added in `FontPanel/index.tsx`.
>   - **Dependencies**:
>     - Added `webfontloader` and `@types/webfontloader` to `package.json`.
>   - **Testing**:
>     - Updated tests in `fonts.test.ts` to cover new font utilities and behaviors.
> 
> <sup>This description was created by </sup>[<img alt=""Ellipsis"" src=""https://img.shields.io/badge/Ellipsis-blue?color=175173"">](https://www.ellipsis.dev?ref=onlook-dev%2Fonlook&utm_source=github&utm_medium=referral)<sup> for b9dd50c631bbade18dc20babccb89570a3a8391c. It will automatically update as commits are pushed.</sup>


<!-- ELLIPSIS_HIDDEN -->",Human,"analysisWe need to classify as performance or non-performance. The PR description includes ""Refactor Font Panel for better performance."" Also mentions ""Batch loading of fonts implemented in FontManager to improve performance."" So it's performance-related. So label 'performance'.assistantfinalperformance"
2421225050,1681,update searching brand color picker,spartan-vutrannguyen,134599728,closed,2025-03-27T05:16:55Z,2025-03-27T22:41:30Z,2025-03-27T22:41:30Z,https://api.github.com/repos/onlook-dev/onlook,https://github.com/onlook-dev/onlook/pull/1681,"## Description

<!-- Provide a clear and concise description of your changes -->
- Allow search by color group name
## Related Issues

<!-- Link any related issues using GitHub keywords (e.g., ""closes #123"", ""fixes #456"", ""related to #789"") -->

## Type of Change

<!-- Put an `x` in the boxes that apply -->

- [ ] Bug fix
- [ ] New feature
- [ ] Documentation update
- [ ] Release
- [x] Refactor
- [ ] Other (please describe):

## Testing

<!-- Describe the tests you ran or the steps to verify your changes -->

## Screenshots (if applicable)

<!-- Add screenshots to help explain your changes -->

https://github.com/user-attachments/assets/ba432eeb-16cc-4a2f-bdb2-98702b6ecd11


## Additional Notes

<!-- Add any other context about the PR here -->

<!-- ELLIPSIS_HIDDEN -->

----

> [!IMPORTANT]
> Update color picker to allow searching by color group name and optimize theme refresh by reloading webviews.
> 
>   - **Behavior**:
>     - Allow search by color group name in `ColorBrandPicker.tsx`.
>     - Expand color groups if search query is present in `ColorBrandPicker.tsx`.
>     - Optimize theme refresh by reloading webviews in `index.ts`.
>   - **Functions**:
>     - `filteredColorGroups` and `filteredColorDefaults` in `ColorBrandPicker.tsx` now include group name in search.
>     - `reloadWebviews()` replaces individual frame refresh in `ThemeManager` in `index.ts`.
> 
> <sup>This description was created by </sup>[<img alt=""Ellipsis"" src=""https://img.shields.io/badge/Ellipsis-blue?color=175173"">](https://www.ellipsis.dev?ref=onlook-dev%2Fonlook&utm_source=github&utm_medium=referral)<sup> for ddf9507d9e237b83a7dbf9377af0364912e8d512. It will automatically update as commits are pushed.</sup>

<!-- ELLIPSIS_HIDDEN -->",Human,"analysisWe need to classify as performance or non-performance. The PR description: ""update searching brand color picker"" and includes ""optimize theme refresh by reloading webviews"". That is performance optimization. Keywords: optimize, performance, reload, theme refresh. So classification: performance.assistantfinalperformance"
2607093477,2249,refactor: update publish function,spartan-vutrannguyen,134599728,closed,2025-06-20T11:00:02Z,2025-06-20T17:46:25Z,2025-06-20T17:46:25Z,https://api.github.com/repos/onlook-dev/onlook,https://github.com/onlook-dev/onlook/pull/2249,"## Description

<!-- Provide a clear and concise description of your changes -->

## Related Issues

<!-- Link any related issues using GitHub keywords (e.g., ""closes #123"", ""fixes #456"", ""related to #789"") -->

## Type of Change

<!-- Put an `x` in the boxes that apply -->

- [ ] Bug fix
- [ ] New feature
- [ ] Documentation update
- [ ] Release
- [ ] Refactor
- [ ] Other (please describe):

## Testing

<!-- Describe the tests you ran or the steps to verify your changes -->

## Screenshots (if applicable)

<!-- Add screenshots to help explain your changes -->

## Additional Notes

<!-- Add any other context about the PR here -->

<!-- ELLIPSIS_HIDDEN -->


----

> [!IMPORTANT]
> Refactors `serializeFiles` in `index.ts` for parallel processing and updates directory exclusion constants in `files.ts`.
> 
>   - **Refactor `serializeFiles` in `index.ts`**:
>     - Uses parallel processing and batch handling for file serialization.
>     - Introduces `getAllFilePathsFlat`, `shouldSkipFile`, `categorizeFiles`, `processTextFilesBatch`, and `processBinaryFilesBatch` for improved file handling.
>     - Adds error handling and logging for file processing.
>   - **Constants Update in `files.ts`**:
>     - Adds `BASE_EXCLUDED_DIRECTORIES` for reuse in other constants.
>     - Updates `EXCLUDED_SYNC_DIRECTORIES`, `IGNORED_UPLOAD_DIRECTORIES`, and `EXCLUDED_PUBLISH_DIRECTORIES` to use `BASE_EXCLUDED_DIRECTORIES`.
>   - **Miscellaneous**:
>     - Adds `skipBuild` flag in `preview-domain-section.tsx`.
> 
> <sup>This description was created by </sup>[<img alt=""Ellipsis"" src=""https://img.shields.io/badge/Ellipsis-blue?color=175173"">](https://www.ellipsis.dev?ref=onlook-dev%2Fonlook&utm_source=github&utm_medium=referral)<sup> for ecaca752946fc7583755b0830acabc70aeab9194. You can [customize](https://app.ellipsis.dev/onlook-dev/settings/summaries) this summary. It will automatically update as commits are pushed.</sup>


<!-- ELLIPSIS_HIDDEN -->",Human,"analysisWe need to classify as performance or non-performance. The PR is a refactor: update publish function, refactor serializeFiles for parallel processing and batch handling. That is performance improvement: parallel processing, batch handling, improved file handling, error handling. So it's performance. The classification: 'performance'.assistantfinalperformance"
2392368251,1600,Make saving non-block for deployment,Kitenite,31864905,closed,2025-03-14T01:35:15Z,2025-03-14T01:50:43Z,2025-03-14T01:50:43Z,https://api.github.com/repos/onlook-dev/onlook,https://github.com/onlook-dev/onlook/pull/1600,"## Description

<!-- Provide a clear and concise description of your changes -->

## Related Issues

<!-- Link any related issues using GitHub keywords (e.g., ""closes #123"", ""fixes #456"", ""related to #789"") -->

## Type of Change

<!-- Put an `x` in the boxes that apply -->

- [X] Bug fix
- [ ] New feature
- [ ] Documentation update
- [ ] Release
- [ ] Refactor
- [ ] Other (please describe):

## Testing

<!-- Describe the tests you ran or the steps to verify your changes -->

## Screenshots (if applicable)

<!-- Add screenshots to help explain your changes -->

## Additional Notes

<!-- Add any other context about the PR here -->

<!-- ELLIPSIS_HIDDEN -->


----

> [!IMPORTANT]
> Make `createCommit` non-blocking in `hosting.ts` and remove `isSaving` flag in `versions.ts` to improve deployment process.
> 
>   - **Behavior**:
>     - `createCommit` in `HostingManager.publish()` in `hosting.ts` is now non-blocking by removing `await`.
>     - Removed `isSaving` flag from `VersionsManager.createCommit()` in `versions.ts` to prevent blocking behavior.
>   - **Misc**:
>     - Minor code cleanup in `versions.ts` by removing redundant `isSaving` assignments.
> 
> <sup>This description was created by </sup>[<img alt=""Ellipsis"" src=""https://img.shields.io/badge/Ellipsis-blue?color=175173"">](https://www.ellipsis.dev?ref=onlook-dev%2Fonlook&utm_source=github&utm_medium=referral)<sup> for 50d8aa0afcf3a0e75e6ca5d05ee6a0523f1c6a34. It will automatically update as commits are pushed.</sup>


<!-- ELLIPSIS_HIDDEN -->",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Make saving non-block for deployment"". The description mentions making createCommit non-blocking, removing isSaving flag to improve deployment process. This is about performance improvement. Keywords: non-blocking, improve deployment process. So classify as 'performance'.assistantfinalperformance"
2608978890,294,Improve workflow system with bug fixes and optimizations,ericproulx,8418467,closed,2025-06-21T11:53:37Z,2025-06-23T17:13:30Z,2025-06-23T17:13:30Z,https://api.github.com/repos/Shopify/roast,https://github.com/Shopify/roast/pull/294,"This PR improves the workflow system with several bug fixes and optimizations:

Overall, fixed mutliple warnings when running tests.

## Changes Made
- **Fix variable getter method redefinition**: Prevent redefining getter methods that already exist in each_step.rb
- **Optimize SQLite state repository operations**: Improve database operations and state management
- **Enhance step executor reporting**: Better error handling and reporting functionality
- **Refactor test suites**: Simplify and improve test maintainability across multiple test files
- **Update interpolator test**: Use simpler expectation syntax for better readability

## Files Modified
- `lib/roast/workflow/each_step.rb`
- `lib/roast/workflow/sqlite_state_repository.rb`
- `lib/roast/workflow/step_executor_with_reporting.rb`
- `test/roast/workflow/interpolator_test.rb`
- `test/roast/workflow/iteration_steps_test.rb`
- `test/roast/workflow/step_executor_with_reporting_test.rb`
- `test/roast/workflow/step_loader_test.rb`

## Testing
All existing tests pass, and the refactored test suites are more maintainable and readable.

## Impact
These changes improve the robustness and maintainability of the workflow system while fixing potential bugs with method redefinition.",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Improve workflow system with bug fixes and optimizations"". Body mentions bug fixes, optimizations, but not specifically performance. It mentions ""Optimize SQLite state repository operations: Improve database operations and state management"". That could be performance. But the description says ""bug fixes and optimizations"" but not explicitly performance. The guidelines: performance-related PRs may contain keywords such as respons, react, wait, responsive, throughput, slow, performance, optimization, latency, compile-time-hog, fast, memory, stuck, leak, hang, perf, minor, speed, perform, lag, latenc, tim, instant, or any other terms referring to how faster and efficiently software runs.

The PR includes ""Optimize SQLite state repository operations"" which is an optimization. But is that performance? It could be. But the guidelines say ""performance-related pull requests may contain keywords such as... optimization"". So ""optimization"" is a keyword. So this PR likely is performance. But also bug fixes. The question: ""determine whether the primary purpose of the pull request is to improve performance."" The title says ""Improve workflow system with bug fixes and optimizations"". The body: ""This PR improves the workflow system with several bug fixes and optimizations:... Optimize SQLite state repository operations: Improve database operations and state management"". That is performance improvement. So classification: performance.assistantfinalperformance"
2527349758,1698,mh4gf poc rag,MH4GF,31152321,closed,2025-05-19T02:05:42Z,2025-05-26T06:32:58Z,,https://api.github.com/repos/liam-hq/liam,https://github.com/liam-hq/liam/pull/1698,"- **feat: Implement RAG for database schema queries using pgvector**
- **remove try-catch**
- **approve mastra/rag**
- **fix connect url setting**
- **Refactor PgVector initialization to use lazy loading for improved performance**
- **Add SSL options to PgVector initialization for secure database connections**
- **Update PgVector connection string to use non-pooling option**
",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""mh4gf poc rag"". Body: features: Implement RAG for database schema queries using pgvector. remove try-catch, approve mastra/rag, fix connect url setting, Refactor PgVector initialization to use lazy loading for improved performance, Add SSL options, Update connection string to use non-pooling option.

The key phrase: ""Refactor PgVector initialization to use lazy loading for improved performance"". That indicates performance improvement. Also ""non-pooling option"" might affect performance. So classification: performance.assistantfinalperformance"
2588963649,1994,üîß(turbo): Configure outputLogs errors-only to reduce AI agent token usage,MH4GF,31152321,closed,2025-06-13T03:05:58Z,2025-06-13T07:44:18Z,2025-06-13T07:44:18Z,https://api.github.com/repos/liam-hq/liam,https://github.com/liam-hq/liam/pull/1994,"## Issue

- resolve: Reduce token usage when AI agents run lint commands while maintaining error visibility

## Why is this change needed?

When AI agents execute build tools like lint, gen, and fmt through Turborepo, the verbose output consumes significant tokens. By configuring `outputLogs: ""errors-only""`, we maintain error visibility for debugging while dramatically reducing unnecessary output that agents don't need to process.

## What would you like reviewers to focus on?

- Verify that error output is still properly displayed for debugging
- Confirm that the configuration covers all relevant build tasks
- Check that this doesn't break any existing CI/CD workflows

## Testing Verification

### Success
```sh
‚ùØ pnpm lint:turbo --force

> liam-frontend@0.0.1 lint:turbo /Users/mh4gf/ghq/github.com/liam-hq/liam
> turbo lint --force

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

                  Update available v2.5.3 ‚â´ v2.5.4
 Changelog: https://github.com/vercel/turborepo/releases/tag/v2.5.4
       Run ""pnpm dlx @turbo/codemod@latest update"" to update

       Follow @turborepo for updates: https://x.com/turborepo
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
turbo 2.5.3

‚Ä¢ Packages in scope: @liam-hq/agent, @liam-hq/app, @liam-hq/cli, @liam-hq/configs, @liam-hq/db, @liam-hq/db-structure, @liam-hq/docs, @liam-hq/e2e, @liam-hq/erd-core, @liam-hq/erd-sample, @liam-hq/figma-to-css-variables, @liam-hq/github, @liam-hq/jobs, @liam-hq/mcp-server, @liam-hq/pglite-server, @liam-hq/storybook, @liam-hq/ui
‚Ä¢ Running lint in 17 packages
‚Ä¢ Remote caching disabled

 Tasks:    24 successful, 24 total
Cached:    0 cached, 24 total
  Time:    19.754s
```

### Failure

```sh
‚ùØ pnpm lint:turbo

> liam-frontend@0.0.1 lint:turbo /Users/mh4gf/ghq/github.com/liam-hq/liam
> turbo lint

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

                  Update available v2.5.3 ‚â´ v2.5.4
 Changelog: https://github.com/vercel/turborepo/releases/tag/v2.5.4
       Run ""pnpm dlx @turbo/codemod@latest update"" to update

       Follow @turborepo for updates: https://x.com/turborepo
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
turbo 2.5.3

‚Ä¢ Packages in scope: @liam-hq/agent, @liam-hq/app, @liam-hq/cli, @liam-hq/configs, @liam-hq/db, @liam-hq/db-structure, @liam-hq/docs, @liam-hq/e2e, @liam-hq/erd-core, @liam-hq/erd-sample, @liam-hq/figma-to-css-variables, @liam-hq/github, @liam-hq/jobs, @liam-hq/mcp-server, @liam-hq/pglite-server, @liam-hq/storybook, @liam-hq/ui
‚Ä¢ Running lint in 17 packages
‚Ä¢ Remote caching disabled
@liam-hq/agent:lint: cache miss, executing 74fc4e76a3a16063
@liam-hq/agent:lint:
@liam-hq/agent:lint:
@liam-hq/agent:lint: > @liam-hq/agent@0.1.0 lint /Users/mh4gf/ghq/github.com/liam-hq/liam/frontend/internal-packages/agent
@liam-hq/agent:lint: > concurrently ""pnpm:lint:*""
@liam-hq/agent:lint:
@liam-hq/agent:lint: [eslint]
@liam-hq/agent:lint: [eslint] > @liam-hq/agent@0.1.0 lint:eslint /Users/mh4gf/ghq/github.com/liam-hq/liam/frontend/internal-packages/agent
@liam-hq/agent:lint: [eslint] > eslint .
@liam-hq/agent:lint: [eslint]
@liam-hq/agent:lint: [biome]
@liam-hq/agent:lint: [biome] > @liam-hq/agent@0.1.0 lint:biome /Users/mh4gf/ghq/github.com/liam-hq/liam/frontend/internal-packages/agent
@liam-hq/agent:lint: [biome] > biome check .
@liam-hq/agent:lint: [biome]
@liam-hq/agent:lint: [tsc]
@liam-hq/agent:lint: [tsc] > @liam-hq/agent@0.1.0 lint:tsc /Users/mh4gf/ghq/github.com/liam-hq/liam/frontend/internal-packages/agent
@liam-hq/agent:lint: [tsc] > tsc --noEmit
@liam-hq/agent:lint: [tsc]
@liam-hq/agent:lint: [biome] ./src/chat/workflow/shared/stateManager.ts organizeImports ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
@liam-hq/agent:lint: [biome]
@liam-hq/agent:lint: [biome]   √ó Import statements could be sorted:
@liam-hq/agent:lint: [biome]
@liam-hq/agent:lint: [biome]       1   1 ‚îÇ   import { schemaSchema } from '@liam-hq/db-structure'
@liam-hq/agent:lint: [biome]       2     ‚îÇ - import¬∑type¬∑{¬∑WorkflowState¬∑}¬∑from¬∑'../types'
@liam-hq/agent:lint: [biome]       3     ‚îÇ - import¬∑*¬∑as¬∑v¬∑from¬∑'valibot'
@liam-hq/agent:lint: [biome]           2 ‚îÇ + import¬∑*¬∑as¬∑v¬∑from¬∑'valibot'
@liam-hq/agent:lint: [biome]           3 ‚îÇ + import¬∑type¬∑{¬∑WorkflowState¬∑}¬∑from¬∑'../types'
@liam-hq/agent:lint: [biome]       4   4 ‚îÇ
@liam-hq/agent:lint: [biome]       5   5 ‚îÇ   /**
@liam-hq/agent:lint: [biome]
@liam-hq/agent:lint: [biome]
@liam-hq/agent:lint: [biome] check ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
@liam-hq/agent:lint: [biome]
@liam-hq/agent:lint: [biome]   √ó Some errors were emitted while running checks.
@liam-hq/agent:lint: [biome]
@liam-hq/agent:lint: [biome]
@liam-hq/agent:lint: [biome] Skipped 1 suggested fixes.
@liam-hq/agent:lint: [biome] If you wish to apply the suggested (unsafe) fixes, use the command biome check --fix --unsafe
@liam-hq/agent:lint: [biome]
@liam-hq/agent:lint: [biome] Checked 35 files in 8ms. No fixes applied.
@liam-hq/agent:lint: [biome] Found 1 error.
@liam-hq/agent:lint: [biome] ‚ÄâELIFECYCLE‚Äâ Command failed with exit code 1.
@liam-hq/agent:lint: [biome] pnpm run lint:biome exited with code 1
@liam-hq/agent:lint: [tsc] pnpm run lint:tsc exited with code 0
@liam-hq/agent:lint: [eslint] pnpm run lint:eslint exited with code 0
@liam-hq/agent:lint: ‚ÄâELIFECYCLE‚Äâ Command failed with exit code 1.
@liam-hq/agent:lint: ERROR: command finished with error: command (/Users/mh4gf/ghq/github.com/liam-hq/liam/frontend/internal-packages/agent) /Users/mh4gf/.asdf/installs/nodejs/22.16.0/bin/pnpm run lint exited (1)
@liam-hq/agent#lint: command (/Users/mh4gf/ghq/github.com/liam-hq/liam/frontend/internal-packages/agent) /Users/mh4gf/.asdf/installs/nodejs/22.16.0/bin/pnpm run lint exited (1)

 Tasks:    22 successful, 23 total
Cached:    21 cached, 23 total
  Time:    2.847s
Failed:    @liam-hq/agent#lint

 ERROR  run failed: command  exited (1)
‚ÄâELIFECYCLE‚Äâ Command failed with exit code 1.
```

## What was done

### ü§ñ Generated by PR Agent at 4049ba47d9858d737e085712f2f6f03dae52795f

‚Ä¢ Configure `outputLogs: ""errors-only""` for build tools to reduce AI agent token usage
‚Ä¢ Apply setting to build, gen, lint, and fmt tasks in turbo.json
‚Ä¢ Maintain error visibility while minimizing verbose output


## Detailed Changes

<table><thead><tr><th></th><th align=""left"">Relevant files</th></tr></thead><tbody><tr><td><strong>Configuration changes</strong></td><td><table>
<tr>
  <td>
    <details>
      <summary><strong>turbo.json</strong><dd><code>Configure outputLogs errors-only for build tasks</code>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </dd></summary>
<hr>

turbo.json

‚Ä¢ Added <code>""outputLogs"": ""errors-only""</code> to build, gen, lint, and fmt tasks<br> <br>‚Ä¢ Modified fmt task from empty object to explicit configuration<br> ‚Ä¢ <br>Maintained all existing dependencies and configurations


</details>


  </td>
  <td><a href=""https://github.com/liam-hq/liam/pull/1994/files#diff-f8de965273949793edc0fbfe249bb458c0becde39b2e141db087bcbf5d4ad5e3"">+9/-4</a>&nbsp; &nbsp; &nbsp; </td>

</tr>
</table></td></tr></tr></tbody></table>

## Additional Notes

This change specifically targets AI agent efficiency while preserving developer experience for error handling.

___

> <details> <summary>  Need help?</summary><li>Type <code>/help how to ...</code> in the comments thread for any questions about Qodo Merge usage.</li><li>Check out the <a href=""https://qodo-merge-docs.qodo.ai/usage-guide/"">documentation</a> for more information.</li></details>",Human,"analysisWe need to classify as performance or non-performance. The PR is about configuring outputLogs to errors-only to reduce AI agent token usage. This is about reducing token usage, which is a performance improvement? It's about reducing token usage, which is a performance optimization. The keywords: ""performance"", ""optimization"", ""reduce"", ""token usage"". The description: ""Reduce token usage when AI agents run lint commands while maintaining error visibility"". That is a performance improvement. So classify as 'performance'.assistantfinalperformance"
2593064254,2012,chore(deps): update dependency @vitejs/plugin-react to v4.5.1,renovate[bot],29139614,closed,2025-06-14T22:09:31Z,2025-06-17T04:04:31Z,2025-06-17T04:04:31Z,https://api.github.com/repos/liam-hq/liam,https://github.com/liam-hq/liam/pull/2012,"This PR contains the following updates:

| Package | Change | Age | Adoption | Passing | Confidence |
|---|---|---|---|---|---|
| [@vitejs/plugin-react](https://redirect.github.com/vitejs/vite-plugin-react/tree/main/packages/plugin-react#readme) ([source](https://redirect.github.com/vitejs/vite-plugin-react/tree/HEAD/packages/plugin-react)) | [`4.4.1` -> `4.5.1`](https://renovatebot.com/diffs/npm/@vitejs%2fplugin-react/4.4.1/4.5.1) | [![age](https://developer.mend.io/api/mc/badges/age/npm/@vitejs%2fplugin-react/4.5.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://developer.mend.io/api/mc/badges/adoption/npm/@vitejs%2fplugin-react/4.5.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://developer.mend.io/api/mc/badges/compatibility/npm/@vitejs%2fplugin-react/4.4.1/4.5.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://developer.mend.io/api/mc/badges/confidence/npm/@vitejs%2fplugin-react/4.4.1/4.5.1?slim=true)](https://docs.renovatebot.com/merge-confidence/) |

---

### Release Notes

<details>
<summary>vitejs/vite-plugin-react (@&#8203;vitejs/plugin-react)</summary>

### [`v4.5.1`](https://redirect.github.com/vitejs/vite-plugin-react/blob/HEAD/packages/plugin-react/CHANGELOG.md#451-2025-06-03)

[Compare Source](https://redirect.github.com/vitejs/vite-plugin-react/compare/476e705375ef618458918580beb63f43799d12e4...2f3205265904ff7770021700689a0d6fe17b1f03)

##### Add explicit semicolon in preambleCode [#&#8203;485](https://redirect.github.com/vitejs/vite-plugin-react/pull/485)

This fixes an edge case when using HTML minifiers that strips line breaks aggressively.

### [`v4.5.0`](https://redirect.github.com/vitejs/vite-plugin-react/blob/HEAD/packages/plugin-react/CHANGELOG.md#450-2025-05-23)

[Compare Source](https://redirect.github.com/vitejs/vite-plugin-react/compare/57cc39869c319b842dac348b62c882a7bb963f7b...476e705375ef618458918580beb63f43799d12e4)

##### Add `filter` for rolldown-vite [#&#8203;470](https://redirect.github.com/vitejs/vite-plugin-react/pull/470)

Added `filter` so that it is more performant when running this plugin with rolldown-powered version of Vite.

##### Skip HMR for JSX files with hooks [#&#8203;480](https://redirect.github.com/vitejs/vite-plugin-react/pull/480)

This removes the HMR warning for hooks with JSX.

</details>

---

### Configuration

üìÖ **Schedule**: Branch creation - Only on Sunday and Saturday ( * * * * 0,6 ) (UTC), Automerge - At any time (no schedule defined).

üö¶ **Automerge**: Disabled by config. Please merge this manually once you are satisfied.

‚ôª **Rebasing**: Whenever PR becomes conflicted, or you tick the rebase/retry checkbox.

üîï **Ignore**: Close this PR and you won't be reminded about this update again.

---

 - [ ] <!-- rebase-check -->If you want to rebase/retry this PR, check this box

---

This PR was generated by [Mend Renovate](https://mend.io/renovate/). View the [repository job log](https://developer.mend.io/github/liam-hq/liam).
<!--renovate-debug:eyJjcmVhdGVkSW5WZXIiOiI0MC41MC4wIiwidXBkYXRlZEluVmVyIjoiNDAuNTAuMCIsInRhcmdldEJyYW5jaCI6Im1haW4iLCJsYWJlbHMiOltdfQ==-->
",Human,"analysisWe need to classify as performance or non-performance. The PR is a dependency update to @vitejs/plugin-react. The changelog mentions ""Add `filter` for rolldown-vite"" and ""more performant when running this plugin with rolldown-powered version of Vite."" Also ""Skip HMR for JSX files with hooks"" but that's not performance. The main change is adding filter for rolldown-vite to be more performant. That is a performance improvement. The PR is a dependency update, but the update includes performance improvements. The question: ""Given a text combining the title and body of a pull request, determine whether the primary purpose of the pull request is to improve performance."" The PR is a dependency update, but the update includes performance improvements. The primary purpose is to update dependency, but the update includes performance improvements. The classification likely should be 'performance' because it mentions performance improvements. The guidelines: performance-related PRs may contain keywords such as... ""performance"" etc. The body contains ""more performant"". So yes, performance. So output 'performance'.assistantfinalperformance"
2580753509,1970,„ÄêBugFix„ÄëError when buildAgent takes more than 15 seconds to respond.,FunamaYukina,23717638,closed,2025-06-10T09:58:46Z,2025-06-10T11:52:00Z,,https://api.github.com/repos/liam-hq/liam,https://github.com/liam-hq/liam/pull/1970,"## Why is this change needed?
<!-- Please explain briefly why this change is necessary -->

Add a temporary keep-alive mechanism to prevent Vercel's 15-second timeout limit during long-running AI processing, particularly with the o3 model in Build mode.

- **API Route (`/api/chat/route.ts`)**:
  - Added `maxDuration = 15` to stay within free tier limits
  - Implemented heartbeat mechanism that sends keep-alive messages every 5 seconds when no activity for 8+ seconds
  - Added proper cleanup with `finally` block

- **Frontend (`frontend/apps/app/components/Chat/services/aiMessageService.ts)**:
  - Added handling for `heartbeat` message type
  - Updates progress messages to show processing status with elapsed time
  - Filters duplicate ""Processing..."" messages

## What would you like reviewers to focus on?
<!-- What specific aspects are you requesting review for? -->

## Testing Verification
<!-- Please describe how you verified these changes in your local environment using text/images/video -->

## What was done
<!-- This section will be filled by PR-Agent when the Pull Request is opened -->

pr_agent:summary

## Detailed Changes
<!-- This section will be filled by PR-Agent when the Pull Request is opened -->

pr_agent:walkthrough

## Additional Notes
<!-- Any additional information for reviewers -->
",Human,"analysisWe need to classify as performance or non-performance. The PR adds keep-alive mechanism to prevent 15-second timeout, heartbeat, etc. It's about improving performance? It's about preventing timeout, ensuring responsiveness. It's performance improvement? It's more about reliability and responsiveness. But keywords: ""performance"", ""latency"", ""slow"", ""timeout"". The description: ""Error when buildAgent takes more than 15 seconds to respond."" They added keep-alive to prevent timeout. That is performance improvement? It's more about reliability. But the guidelines: performance-related PRs may contain keywords such as respons, react, wait, responsive, throughput, slow, performance, optimization, latency, compile-time-hog, fast, memory, stuck, leak, hang, perf, minor, speed, perform, lag, latenc, tim, instant, or any other terms referring to how faster and efficiently software runs. This PR mentions ""timeout"", ""keep-alive"", ""heartbeat"", ""processing status"". It is about preventing timeout, which is a performance issue. It might be considered performance. The title: ""BugFix Error when buildAgent takes more than 15 seconds to respond."" It's a bug fix, but the change is to improve performance by adding keep-alive. The guidelines: classify as performance if primary purpose is to improve performance. This is to prevent timeout, which is a performance issue. So classify as performance.assistantfinalperformance"
2260295346,23811,[fix][fn] Record Pulsar Function processing time properly for asynchronous functions,walkinggo,40360529,closed,2025-01-04T09:51:57Z,2025-01-28T16:03:11Z,2025-01-28T16:03:11Z,https://api.github.com/repos/apache/pulsar,https://github.com/apache/pulsar/pull/23811,"<!--
### Contribution Checklist
  
  - PR title format should be *[type][component] summary*. For details, see *[Guideline - Pulsar PR Naming Convention](https://pulsar.apache.org/contribute/develop-semantic-title/)*. 

  - Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.
  
  - Each pull request should address only one issue, not mix up code from multiple issues.
  
  - Each commit in the pull request has a meaningful commit message

  - Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.
-->

<!-- Either this PR fixes an issue, -->


<!-- or this PR is one task of an issue -->

Fixes #23705 

<!-- If the PR belongs to a PIP, please add the PIP link here -->


<!-- Details of when a PIP is required and how the PIP process work, please see: https://github.com/apache/pulsar/blob/master/pip/README.md -->

### Motivation

See #23705 
In the realm of asynchronous processing, precise timing and performance metrics are essential for effective monitoring and optimization. Apache Pulsar Functions, as a distributed compute platform, relies heavily on asynchronous operations to process and transform data streams. However, the current implementation lacks a robust mechanism for accurately capturing and reporting the execution times of these asynchronous functions.


### Modifications
This Pull Request introduces several modifications to the Apache Pulsar Functions project aimed at improving the accuracy and reliability of asynchronous function execution time statistics. The primary goal is to enhance monitoring and analysis capabilities for function performance. Here‚Äôs a breakdown of the key changes:

1. Removal of processTimeStart Method: The processTimeStart method in ComponentStatsManager has been removed. This method was previously used to record the start time of asynchronous function execution. The start time is now recorded in the JavaExecutionResult object, providing a more centralized and consistent approach.

2. Modification of JavaInstanceRunnable: The run method in JavaInstanceRunnable has been updated to reflect the removal of processTimeStart. The stats.processTimeStart() call has been deleted, and the stats.processTimeEnd() method now accepts the start time as a parameter to calculate the total execution time.

3. Update to FunctionStatsManager: The processTimeEnd method in FunctionStatsManager has been modified to remove the processTimeStart member variable and accept the start time as a parameter. This allows for accurate calculation of the function‚Äôs execution time.

4. Changes to SinkStatsManager and SourceStatsManager: The processTimeEnd methods in SinkStatsManager and SourceStatsManager have been updated to accept the start time as a parameter. However, since these classes do not record processing time, the method bodies remain empty.

5. Addition of startTime in JavaExecutionResult: The JavaExecutionResult class now includes a startTime member variable to store the start time of asynchronous function execution. This allows for accurate calculation of execution time within the handleResult method of JavaInstanceRunnable.

6. Modification of AsyncFuncRequest: The AsyncFuncRequest class in JavaInstance now includes a result member variable of type JavaExecutionResult. This change ensures that the processAsyncResultsInInputOrder method uses the existing JavaExecutionResult object instead of creating a new one, maintaining consistency and avoiding duplication.

7. Use of Same ExecutionResult in Non-asyncPreserveInputOrder Mode: In scenarios where asyncPreserveInputOrderForOutputMessages is disabled, the same executionResult object is now used to avoid unnecessary object creation and potential issues with result assignment.

8. Fix for Result and Exception Handling: Two patches address potential bugs related to result and exception handling in JavaInstance. The processAsyncResultsInInputOrder method now ensures that the result and userException fields of JavaExecutionResult are properly set, improving the reliability of the execution result.

9. Addition of Test Case: A new test case, testAsyncFunctionTime, has been added to verify the accuracy of asynchronous function execution time recording and calculation. This test ensures that the start time recorded in JavaExecutionResult is within an acceptable range of the actual start time.


<!-- Describe the modifications you've done. -->
### Documentation

<!-- DO NOT REMOVE THIS SECTION. CHECK THE PROPER BOX ONLY. -->

- [ ] `doc` <!-- Your PR contains doc changes. -->
- [ ] `doc-required` <!-- Your PR changes impact docs and you will update later -->
- [x] `doc-not-needed` <!-- Your PR changes do not impact docs -->
- [ ] `doc-complete` <!-- Docs have been already added -->

### Matching PR in forked repository

PR in forked repository: <!-- ENTER URL HERE -->https://github.com/walkinggo/pulsar/pull/7

<!--
After opening this PR, the build in apache/pulsar will fail and instructions will
be provided for opening a PR in the PR author's forked repository.

apache/pulsar pull requests should be first tested in your own fork since the 
apache/pulsar CI based on GitHub Actions has constrained resources and quota.
GitHub Actions provides separate quota for pull requests that are executed in 
a forked repository.

The tests will be run in the forked repository until all PR review comments have
been handled, the tests pass and the PR is approved by a reviewer.
-->
",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""[fix][fn] Record Pulsar Function processing time properly for asynchronous functions"". The body describes modifications to record execution times, improve monitoring and analysis capabilities for function performance. It mentions accurate timing, performance metrics, etc. This is clearly performance-related. So classification: performance.assistantfinalperformance"
2452623588,16440,[V1][Perf] Avoid mem duplication when aggregating MM tensors,njhill,16958488,open,2025-04-11T01:12:56Z,,,https://api.github.com/repos/vllm-project/vllm,https://github.com/vllm-project/vllm/pull/16440,"When stacking or concatenating, replace the original tensors with slices into the larger one.

cc @ywang96 @DarkLight1337 ",Human,"analysisWe need to classify as performance or non-performance. The title: ""[V1][Perf] Avoid mem duplication when aggregating MM tensors"". The body: ""When stacking or concatenating, replace the original tensors with slices into the larger one."" This is about memory duplication, performance improvement. So it's performance.assistantfinalperformance"
2381033827,62,Implement parallel file processing for github repo analysis,danieldavidson,5460596,closed,2025-03-10T02:00:03Z,2025-03-10T02:00:38Z,,https://api.github.com/repos/mrwadams/stride-gpt,https://github.com/mrwadams/stride-gpt/pull/62,,Human,"analysisWe need to classify the PR as performance or non-performance. The title: ""Implement parallel file processing for github repo analysis"". Body likely describes adding parallel processing. That is performance improvement. Keywords: parallel, performance, speed. So classify as 'performance'.assistantfinalperformance"
2377661814,77,fix: Update batch size handling for transcription and improve documentation on GPU memory issues,SpirusNox,78000963,closed,2025-03-07T03:39:39Z,2025-03-07T03:39:43Z,2025-03-07T03:39:43Z,https://api.github.com/repos/rishikanthc/Scriberr,https://github.com/rishikanthc/Scriberr/pull/77,,Human,"analysisWe need to classify as performance or non-performance. The title: ""fix: Update batch size handling for transcription and improve documentation on GPU memory issues"". This seems about batch size handling for transcription, and documentation on GPU memory issues. That likely relates to performance? GPU memory issues can affect performance. But is it a performance improvement? The title says ""fix: Update batch size handling for transcription and improve documentation on GPU memory issues"". The content likely addresses memory usage and batch size, which can affect performance. But the classification: performance-related PRs contain keywords like performance, optimization, etc. This title doesn't contain those keywords. But the content might. But we only have title and body? The body is not provided. We only have title. The title mentions GPU memory issues, which is performance. But the instruction: ""Given a text combining the title and body of a pull request, determine whether the primary purpose of the pull request is to improve performance."" We only have title. The title indicates memory issues, which is performance. But the title also says ""improve documentation"". That might be non-performance. But the primary purpose? It says ""Update batch size handling for transcription"" which might be performance. But also ""improve documentation on GPU memory issues"" is documentation. The primary purpose might be to fix batch size handling, which could be performance. But the title doesn't mention performance explicitly. But the content might. We need to decide. The guidelines: performance-related PRs may contain keywords such as... or any other terms referring to how faster and efficiently software runs. The title doesn't contain those. But GPU memory issues is performance. But the guidelines say ""or any other terms referring to how faster and efficiently software runs."" GPU memory issues is a performance concern. So likely performance. But the title also mentions documentation. But the primary purpose might be to fix batch size handling, which could be performance. I'd classify as performance.assistantfinalperformance"
2433598142,30580,feat(webpack): set default sassImplementation to sass-embedded,Coly010,12140467,closed,2025-04-02T09:03:08Z,2025-04-16T10:06:58Z,2025-04-16T10:06:58Z,https://api.github.com/repos/nrwl/nx,https://github.com/nrwl/nx/pull/30580,"## Current Behavior
The `sassImplementation` option for Webpack and Rspack is set to `sass` by default.
`sass-embedded` offers a faster alternative.

## Expected Behavior
Switch the default `sassImplementation` option to `sass-embedded`.
",Human,"analysisWe need to classify whether the primary purpose is performance. The PR title: ""feat(webpack): set default sassImplementation to sass-embedded"". Body: explains that default is sass, sass-embedded offers faster alternative. So it's a performance improvement. So label 'performance'.assistantfinalperformance"
2260441374,684,StringUtils.equalsCaseIgnore optimisation fixes #683,tgd,109181,closed,2025-01-04T15:39:11Z,2025-01-05T01:34:27Z,2025-01-05T01:34:27Z,https://api.github.com/repos/OpenHFT/Chronicle-Core,https://github.com/OpenHFT/Chronicle-Core/pull/684,"StringUtils.equalsCaseIgnore can be optimised for the common case where both strings exactly match. See #683 

`equalsCaseIgnore` is used in wire during deserialisation - `net.openhft.chronicle.wire.TextWire#read(java.lang.CharSequence, int, java.lang.Object)`

Screenshot shows before and after:

<img width=""1141"" alt=""image"" src=""https://github.com/user-attachments/assets/3725654c-1c1a-47d4-8e13-f89c8799f281"" />
 ",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""StringUtils.equalsCaseIgnore optimisation fixes #683"". Body: ""StringUtils.equalsCaseIgnore can be optimised for the common case where both strings exactly match. See #683. equalsCaseIgnore is used in wire during deserialisation - net.openhft.chronicle.wire.TextWire#read(java.lang.CharSequence, int, java.lang.Object). Screenshot shows before and after."" This is about optimization, performance improvement. So classification: performance.assistantfinalperformance"
2257939947,1008,"fix(agents-api,memory-store): Fix docs tests and related migration",creatorrr,931887,closed,2025-01-02T14:10:07Z,2025-01-02T14:15:40Z,2025-01-02T14:15:40Z,https://api.github.com/repos/julep-ai/julep,https://github.com/julep-ai/julep/pull/1008,"### **User description**
Signed-off-by: Diwank Singh Tomer <diwank.singh@gmail.com>


___

### **PR Type**
Bug fix, Tests, Enhancement


___

### **Description**
- Fixed SQL queries to improve performance and prevent duplicates.

- Enhanced test fixtures and added new test cases for document handling.

- Replaced `ast` with `json` for safer embedding parsing.

- Updated migrations to align with new query logic and embedding handling.


___



### **Changes walkthrough** üìù
<table><thead><tr><th></th><th align=""left"">Relevant files</th></tr></thead><tbody><tr><td><strong>Enhancement</strong></td><td><table>
<tr>
  <td>
    <details>
      <summary><strong>get_doc.py</strong><dd><code>Refactored SQL query for distinct document retrieval</code>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </dd></summary>
<hr>

agents-api/agents_api/queries/docs/get_doc.py

<li>Updated SQL query to use <code>ORDER BY</code> and <code>LIMIT</code> for distinct results.<br> <li> Removed unnecessary <code>WITH</code> clause for simplification.<br> <li> Added <code>AND e.embedding IS NOT NULL</code> condition to filter embeddings.


</details>


  </td>
  <td><a href=""https://github.com/julep-ai/julep/pull/1008/files#diff-9c0264615896292cc993d540be24e25c070ccaa2929a87579675e815c36e4319"">+31/-31</a>&nbsp; </td>

</tr>

<tr>
  <td>
    <details>
      <summary><strong>list_docs.py</strong><dd><code>Simplified and optimized document listing query</code>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </dd></summary>
<hr>

agents-api/agents_api/queries/docs/list_docs.py

<li>Simplified SQL query by removing <code>WITH</code> clause.<br> <li> Added <code>GROUP BY</code> and filtering conditions for better query performance.<br> <li> Adjusted embedding handling logic in the query.


</details>


  </td>
  <td><a href=""https://github.com/julep-ai/julep/pull/1008/files#diff-8abede8e9aab32f6622b45ba6e76f699b4b3baa69c70ba8ad89ab689db9ea468"">+32/-42</a>&nbsp; </td>

</tr>

<tr>
  <td>
    <details>
      <summary><strong>utils.py</strong><dd><code>Improved embedding parsing with JSON</code>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </dd></summary>
<hr>

agents-api/agents_api/queries/docs/utils.py

<li>Replaced <code>ast.literal_eval</code> with <code>json.loads</code> for embedding parsing.<br> <li> Added error handling for JSON parsing of embeddings.


</details>


  </td>
  <td><a href=""https://github.com/julep-ai/julep/pull/1008/files#diff-77175b6d6409951ad2c2b2097c00caca5ce0132fc215c7ffa6b808fc7a54e5d2"">+10/-8</a>&nbsp; &nbsp; </td>

</tr>

<tr>
  <td>
    <details>
      <summary><strong>utils.py</strong><dd><code>Added vectorizer worker support in test utilities</code>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </dd></summary>
<hr>

agents-api/tests/utils.py

<li>Added support for starting vectorizer worker in <code>get_pg_dsn</code>.<br> <li> Integrated Docker container for vectorizer worker.<br> <li> Improved logging and environment variable handling.


</details>


  </td>
  <td><a href=""https://github.com/julep-ai/julep/pull/1008/files#diff-f57e08363e297b9a26b89a750f00ff1dbcbfdba9189591c528f35691d58a7432"">+31/-1</a>&nbsp; &nbsp; </td>

</tr>

<tr>
  <td>
    <details>
      <summary><strong>000018_doc_search.up.sql</strong><dd><code>Updated migration for embedding and search improvements</code>&nbsp; &nbsp; </dd></summary>
<hr>

memory-store/migrations/000018_doc_search.up.sql

<li>Updated SQL to join <code>docs</code> and <code>docs_embeddings</code> for embedding retrieval.<br> <li> Adjusted query logic to align with new embedding handling.


</details>


  </td>
  <td><a href=""https://github.com/julep-ai/julep/pull/1008/files#diff-1b8f7f1bc54e6b0c0090b952201ef0bd12d6484b1b88b97c84d4c425b2fe867c"">+6/-2</a>&nbsp; &nbsp; &nbsp; </td>

</tr>
</table></td></tr><tr><td><strong>Tests</strong></td><td><table>
<tr>
  <td>
    <details>
      <summary><strong>fixtures.py</strong><dd><code>Enhanced test fixtures with new user doc support</code>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </dd></summary>
<hr>

agents-api/tests/fixtures.py

<li>Added <code>test_user_doc</code> fixture for user-specific document testing.<br> <li> Introduced explicit index refreshing with <code>REINDEX DATABASE</code>.<br> <li> Removed deprecated or commented-out code.


</details>


  </td>
  <td><a href=""https://github.com/julep-ai/julep/pull/1008/files#diff-e6fdaaf046732cdbe189837561b0795f04f1235a6752d746f750330db11ca509"">+47/-41</a>&nbsp; </td>

</tr>

<tr>
  <td>
    <details>
      <summary><strong>test_docs_queries.py</strong><dd><code>Improved document query tests with dynamic embeddings</code>&nbsp; &nbsp; &nbsp; &nbsp; </dd></summary>
<hr>

agents-api/tests/test_docs_queries.py

<li>Updated tests to use dynamic query embeddings.<br> <li> Removed unnecessary delays and skips for certain tests.<br> <li> Improved test coverage for embedding and hybrid searches.


</details>


  </td>
  <td><a href=""https://github.com/julep-ai/julep/pull/1008/files#diff-36539fadcf9fdd23428ea921b45082c021d6816dddf931a806b9828fa43d1fb1"">+16/-10</a>&nbsp; </td>

</tr>

<tr>
  <td>
    <details>
      <summary><strong>test_docs_routes.py</strong><dd><code>Enhanced route tests for document searches</code>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </dd></summary>
<hr>

agents-api/tests/test_docs_routes.py

<li>Adjusted sleep durations for search tests.<br> <li> Removed skips for certain route tests.<br> <li> Enhanced route tests for agent and user document searches.


</details>


  </td>
  <td><a href=""https://github.com/julep-ai/julep/pull/1008/files#diff-d1de938cc8c2e3cd183efc6e8be6341d7e7160c463d9cc5f9907cfa2772bfef5"">+5/-8</a>&nbsp; &nbsp; &nbsp; </td>

</tr>
</table></td></tr></tr></tbody></table>

___

> üí° **PR-Agent usage**: Comment `/help ""your question""` on any pull request to receive relevant information",Human,"analysisWe need to classify as 'performance' or 'non-performance'. The PR description: ""Fixed SQL queries to improve performance and prevent duplicates."" Also mentions ""Refactored SQL query for distinct document retrieval"", ""Simplified and optimized document listing query"", ""Updated migration for embedding and search improvements"". So performance improvements. Also mentions tests. But primary purpose is performance. So label 'performance'.assistantfinalperformance"
2398817362,12889,PBS Bid Adapter : add BEFORE_PBS_HTTP event,Sir-Will,5961995,closed,2025-03-17T19:06:09Z,2025-03-27T16:37:33Z,2025-03-27T16:37:33Z,https://api.github.com/repos/prebid/Prebid.js,https://github.com/prebid/Prebid.js/pull/12889,"<!--
Thank you for your pull request! 

Please title your pull request like this: 'Module: Change', eg 'Fraggles Bid Adapter: support fragglerock'

Please make sure this PR is scoped to one change or you may be asked to resubmit. 
 
Please make sure any added or changed code includes tests with greater than 80% code coverage. 

See https://github.com/prebid/Prebid.js/blob/master/CONTRIBUTING.md#testing-prebidjs for documentation on testing Prebid.js.

For any user facing change, submit a link to a PR on the docs repo at https://github.com/prebid/prebid.github.io/
-->

## Type of change
<!-- Remove items that don't apply and/or select an item by changing [ ] to [x] -->
- [ ] Bugfix
- [x] Feature
- [ ] New bidder adapter  <!--  IMPORTANT: also submit your bidder parameter documentation as noted in https://docs.prebid.org/dev-docs/bidder-adaptor.html#submitting-your-adapter -->
- [ ] Updated bidder adapter  <!--  IMPORTANT: (1) consider whether you need to upgrade your bidder parameter documentation in https://github.com/prebid/prebid.github.io/tree/master/dev-docs/bidders and (2) if you have a Prebid Server adapter, please consider whether that should be updated as well. --> 
- [ ] Code style update (formatting, local variables)
- [ ] Refactoring (no functional changes, no api changes)
- [ ] Build related changes
- [ ] CI related changes

- [ ] Does this change affect user-facing APIs or examples documented on http://prebid.org?
- [ ] Other

## Description of change
<!-- Describe the change proposed in this pull request -->

Adds the `BEFORE_PBS_HTTP ` event to the PBS adapter. This allows modifications like compressing the payload/body.

Example use case of using it for lz4 compression:
```js
pbjs.onEvent('beforePBSHttp', function(event) {
    const requestCompressed = compress(event.requestJson);
    if (requestCompressed) {
        event.customHeaders['Content-Encoding'] = 'lz4';
        event.requestJson = requestCompressed;
    }
});
```

<!-- For new bidder adapters, please provide the following
- contact email of the adapter‚Äôs maintainer
- test parameters for validating bids:
```
{
  bidder: '<bidder name>',
  params: {
    // ...
  }
}
```

Be sure to test the integration with your adserver using the [Hello World](https://github.com/prebid/Prebid.js/blob/master/integrationExamples/gpt/hello_world.html) sample page. -->


## Other information
<!-- References to related PR or issue #s, @mentions of the person or team responsible for reviewing changes, etc. -->
",Human,"analysisWe need to classify as performance or non-performance. The PR adds an event to allow modifications like compressing the payload/body. That is about performance optimization (compressing request). The description: ""Adds the BEFORE_PBS_HTTP event to the PBS adapter. This allows modifications like compressing the payload/body."" This is a performance improvement. So classification: performance.assistantfinalperformance"
2334576796,12763,Core: suspend auctions during prerendering,dgirardi,10151381,closed,2025-02-13T19:27:09Z,2025-02-20T18:41:16Z,2025-02-20T18:41:16Z,https://api.github.com/repos/prebid/Prebid.js,https://github.com/prebid/Prebid.js/pull/12763,"## Type of change
<!-- Remove items that don't apply and/or select an item by changing [ ] to [x] -->
- [x] Feature

## Description of change

If the document is being [prerendered](https://developer.chrome.com/docs/web-platform/prerender-pages), delay auctions until the page is activated.

## Other information

Closes https://github.com/prebid/Prebid.js/issues/12724

",Human,"analysisWe need to classify whether the primary purpose is performance. The PR title: ""Core: suspend auctions during prerendering"". Description: ""If the document is being prerendered, delay auctions until the page is activated."" This is about performance? It delays auctions during prerendering to avoid unnecessary work. It's about performance optimization? It might be about resource usage, but it's a feature: ""suspend auctions during prerendering"". It's a performance improvement: avoid running auctions when page not visible. That is a performance optimization. So classify as 'performance'.assistantfinalperformance"
2625218797,397,[gemma3n] Correctly scale text embeddings for quantized gemma3n conversions,neilmehta24,173081182,closed,2025-06-27T19:08:37Z,2025-06-28T01:23:50Z,2025-06-28T01:23:50Z,https://api.github.com/repos/Blaizzy/mlx-vlm,https://github.com/Blaizzy/mlx-vlm/pull/397,"When quantizing the `Gemma3nTextScaledWordEmbedding` module, it follows this code path:
- https://github.com/ml-explore/mlx/blob/2c11d10/python/mlx/nn/layers/embedding.py#L42
- https://github.com/ml-explore/mlx/blob/2c11d10/python/mlx/nn/layers/quantized.py#L138

Which ends up overwriting the `Gemma3nTextScaledWordEmbedding` instance with `QuantizedEmbedding`. This means that `Gemma3nTextScaledWordEmbedding.__call__` never gets called when the model is quantized.

Fix this issue by moving the scaling outside of `Gemma3nTextScaledWordEmbedding` and deleting `Gemma3nTextScaledWordEmbedding`.

@awni : I would be curious to hear your thoughts on how to enable the use case of inheriting from `nn.Embeddings` while correctly quantizing the underlying weights in the base class.",Human,"analysisWe need to classify whether the primary purpose is performance. The PR title: ""[gemma3n] Correctly scale text embeddings for quantized gemma3n conversions"". Body: describes quantization, scaling, moving scaling outside, deleting class. It's about quantization, which is performance optimization. The description mentions quantized gemma3n conversions, scaling, quantization. It's about performance. So label 'performance'.assistantfinalperformance"
2530224094,2210,test.sh: the master test script,adeebshihadeh,8762862,closed,2025-05-20T03:39:06Z,2025-05-20T04:19:41Z,2025-05-20T04:19:41Z,https://api.github.com/repos/commaai/panda,https://github.com/commaai/panda/pull/2210,need to speedup cppcheck before we can move the rest of the tests in here,Human,"analysisWe need to classify the pull request as performance or non-performance. The title and body: ""test.sh: the master test script need to speedup cppcheck before we can move the rest of the tests in here"". The content indicates they want to speed up cppcheck. That is a performance improvement. So classification: performance.assistantfinalperformance"
2293275372,2298,"site: resolve render failure, capture paint performance",DanielSinclair,4412473,closed,2025-01-22T23:23:45Z,2025-01-22T23:47:54Z,2025-01-22T23:47:54Z,https://api.github.com/repos/rainbow-me/rainbowkit,https://github.com/rainbow-me/rainbowkit/pull/2298,"## Changes
- upgraded  `three`, `@react-spring/three`, `@react-three/fiber` to resolve client-side render failures when webgl is disabled
- added vercel SDK to monitor paint performance",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""site: resolve render failure, capture paint performance"". Body: upgrades dependencies to resolve client-side render failures when webgl is disabled, added vercel SDK to monitor paint performance. The focus is on capturing paint performance, monitoring. Also upgrading dependencies to fix render failures. The mention of ""capture paint performance"" indicates performance monitoring. So likely performance. The keywords: performance, paint performance. So classification: performance.assistantfinalperformance"
2337814357,344,Added string pooling feature,sandrohanea,40202887,closed,2025-02-15T16:24:27Z,2025-02-15T16:39:02Z,2025-02-15T16:39:02Z,https://api.github.com/repos/sandrohanea/whisper.net,https://github.com/sandrohanea/whisper.net/pull/344,"# Improve String Pooling in Whisper.net

## Summary

This PR enhances the string pooling mechanism in `Whisper.net` to optimize memory usage and reduce allocations when handling strings, particularly in high-performance scenarios. The changes focus on refining how strings are pooled, retrieved, and returned, ensuring efficient reuse.

## Changes

- Implemented `StringPool` to manage and reuse strings, reducing memory pressure.
- Optimized UTF8 string conversion using pooled strings to avoid unnecessary allocations.
- Introduced `ReturnString` to correctly reinsert strings into the pool for future reuse.
- Integrated `StringPool` usage across `WhisperProcessor` where applicable.
- Ensured thread safety and performance by structuring the pool using `SortedList<int, Stack<string>>`.
- Added example of how to use default string pool.

## Benefits

- **Lower GC Pressure**: Reduces the number of string allocations, minimizing garbage collection overhead.
- **Improved Performance**: Reuses pre-allocated strings where possible, speeding up frequent string operations.

## Notes

- This PR only modifies the string pooling logic; no other functional changes were made.
- The default behavior remains unchanged unless explicitly configured to use the string pool.

Inspired by https://github.com/sandrohanea/whisper.net/issues/339",Human,"analysisWe need to classify as performance or non-performance. The PR is about string pooling to reduce allocations, lower GC pressure, improve performance. It explicitly mentions performance. So classification: performance.assistantfinalperformance"
2590309954,401,Tests were isolated so that we don't run out of memory anymore,sandrohanea,40202887,closed,2025-06-13T13:36:49Z,2025-06-13T14:20:27Z,2025-06-13T14:20:27Z,https://api.github.com/repos/sandrohanea/whisper.net,https://github.com/sandrohanea/whisper.net/pull/401,,Human,"analysisWe need to classify the pull request as performance or non-performance. The title and body: ""Tests were isolated so that we don't run out of memory anymore"". This is about memory usage, tests isolated to avoid memory exhaustion. That is performance-related: memory usage, performance. So classify as 'performance'.assistantfinalperformance"
2508896643,373,Use asyncpg-pool in fastapi-usage-ex,janbjorge,16663421,closed,2025-05-09T07:45:58Z,2025-05-09T08:39:34Z,2025-05-09T08:39:34Z,https://api.github.com/repos/janbjorge/pgqueuer,https://github.com/janbjorge/pgqueuer/pull/373,,Human,"analysisWe need to classify the pull request. The title and body: ""Use asyncpg-pool in fastapi-usage-ex"". That seems like a change to use asyncpg-pool, likely to improve performance? The description: ""Use asyncpg-pool in fastapi-usage-ex"". That suggests using a connection pool for asyncpg, which can improve performance by reusing connections. But is the primary purpose performance? It might be to improve performance by using a pool. The keywords: ""asyncpg-pool"" is a performance improvement. The PR likely is about using a connection pool to reduce overhead. So classification: performance. There's no mention of other aspects. So answer: performance.assistantfinalperformance"
2556468457,390,Postgres tuning,janbjorge,16663421,closed,2025-05-31T08:41:19Z,2025-06-02T14:40:58Z,,https://api.github.com/repos/janbjorge/pgqueuer,https://github.com/janbjorge/pgqueuer/pull/390,,Human,"analysisWe need to classify the pull request as performance or non-performance. The title and body: ""Postgres tuning"". That suggests performance tuning. So classify as 'performance'.assistantfinalperformance"
2421585261,3507,fix(providers): add logging and fix custom python provider caching,Mikkolehtimaki,13274545,closed,2025-03-27T08:01:52Z,2025-03-28T23:30:52Z,2025-03-28T23:30:52Z,https://api.github.com/repos/promptfoo/promptfoo,https://github.com/promptfoo/promptfoo/pull/3507,"Custom Python providers were not always caching responses. The below example shows a provider where caching fails:

```
from typing import Any

from pydantic import BaseModel
from openai import OpenAI


OPENAI_CLIENT = OpenAI()


class Poem(BaseModel):
    """"""A funny poem.""""""

    poem: str


# The following classes are needed for promptfoo
class ProviderOptions:
    id: str | None
    config: dict[str, Any] | None


class CallApiContextParams:
    vars: dict[str, str]


class TokenUsage(BaseModel):
    total: int
    prompt: int
    completion: int


class ProviderResponse(BaseModel):
    output: list[str] | str| None
    error: str | None = None
    tokenUsage: TokenUsage | None = None
    cost: float | None = None
    cached: bool | None = None
    logProbs: list[float] | None = None


# Default function called by Promptfoo
def call_api(prompt: str, options: dict[str, Any], context: dict[str, Any]) -> dict[str, Any]:
    config = options.get(""config"", None)
    llm_model = config.get(""llm_model"", ""gpt-4o"")

    response = OPENAI_CLIENT.beta.chat.completions.parse(
        model=llm_model,
        messages=[
            {
                ""role"": ""user"",
                ""content"": prompt,
            }
        ],
        response_format=Poem,
    )
    poem = response.choices[0].message.parsed.poem
    return ProviderResponse(output=poem).model_dump()
```

Run with this config:

```
description: Write poems

prompts:
  - ""Write a funny poem about {topic}""

providers:
  - file://providers/poem.py

tests:
  - vars:
      topic: ""a cat""
  - vars:
      topic: ""a dog""
  - vars:
      topic: ""cakes""
  - vars:
      topic: ""a computer""
```
",Human,"analysisWe need to classify whether the primary purpose of the PR is to improve performance. The PR title: ""fix(providers): add logging and fix custom python provider caching"". The body describes that custom Python providers were not always caching responses. They added logging and fixed caching. This is about caching responses, which is a performance improvement. The description: ""Custom Python providers were not always caching responses."" They fix caching. That is a performance improvement. So classification: 'performance'.assistantfinalperformance"
2568697411,4322,feat: add maximum evaluation time limit with PROMPTFOO_MAX_EVAL_TIME_MS,mldangelo,7235481,closed,2025-06-04T23:31:35Z,2025-06-12T05:40:34Z,2025-06-12T05:40:34Z,https://api.github.com/repos/promptfoo/promptfoo,https://github.com/promptfoo/promptfoo/pull/4322,"Adds a new maximum evaluation time limit feature with PROMPTFOO_MAX_EVAL_TIME_MS environment variable and maxEvalTimeMs API option. Includes comprehensive documentation and test coverage. Useful for CI/CD time limits, cost control, and preventing runaway evaluations.",Human,"analysisWe need to classify as performance or non-performance. The PR adds a maximum evaluation time limit. That is about limiting time, controlling cost, preventing runaway evaluations. It's about performance? It's about limiting runtime, but it's more about controlling resource usage, not necessarily improving performance. But it's about limiting evaluation time, which is a performance constraint. The description: ""Adds a new maximum evaluation time limit feature with PROMPTFOO_MAX_EVAL_TIME_MS environment variable and maxEvalTimeMs API option. Includes comprehensive documentation and test coverage. Useful for CI/CD time limits, cost control, and preventing runaway evaluations."" This is about limiting time, not improving performance. But it's about controlling runtime, which is a performance-related feature. The classification: performance. The guidelines: performance-related PRs may contain keywords like latency, performance, optimization, etc. This PR is about limiting evaluation time, which is a performance constraint. I'd classify as performance.assistantfinalperformance"
2537690761,1231,Speed up duplicative printing in taylor-alts,pavpanchekha,30707,closed,2025-05-22T16:21:04Z,2025-05-22T16:51:29Z,2025-05-22T16:51:29Z,https://api.github.com/repos/herbie-fp/herbie,https://github.com/herbie-fp/herbie/pull/1231,"This is super minor and dumb and a mis-feature already but this PR makes colonnade way faster without any significant code change, just by caching a string.",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Speed up duplicative printing in taylor-alts"". Body: ""This is super minor and dumb and a mis-feature already but this PR makes colonnade way faster without any significant code change, just by caching a string."" This is clearly a performance improvement: speed up printing, caching string. So classification: performance.assistantfinalperformance"
2497750262,1218,Add a hash-consing pass to Taylor series,pavpanchekha,30707,closed,2025-05-05T03:52:43Z,2025-05-05T04:26:44Z,,https://api.github.com/repos/herbie-fp/herbie,https://github.com/herbie-fp/herbie/pull/1218,"Taylor series use a weird special-purpose simplifier called `reduce.rkt`. Honestly, the whole thing is legacy code, but the specific issue now is that it spins out on this one benchmark, Trowbridge-Rietz cosTheta. This PR is based on a bit of tinkering with this benchmark; it turns out that it spends _most_ of its runtime testing the `equal?` parts of standard Racket `equal?`-hash tables. This PR thus adds a single hash-consing pass and converts all of the hash tables to `eq?`.",Human,analysisWe need to classify as performance or non-performance. The PR adds a hash-consing pass to improve performance by converting hash tables to eq? to reduce runtime. The description says it spends most runtime testing equal? parts of hash tables. So it's a performance improvement. So label 'performance'.assistantfinalperformance
2443864788,1182,Delete unneeded `expand-rules` function,pavpanchekha,30707,closed,2025-04-07T16:59:52Z,2025-04-08T19:32:24Z,,https://api.github.com/repos/herbie-fp/herbie,https://github.com/herbie-fp/herbie/pull/1182,"It is, I think, unnecessary, and it costs a bit of time (though not a lot). I also removed the cache‚Äîlet's see if it pays for itself.",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Delete unneeded `expand-rules` function"". Body: ""It is, I think, unnecessary, and it costs a bit of time (though not a lot). I also removed the cache‚Äîlet's see if it pays for itself.""

This is about removing an unneeded function that costs time, and removing cache. It's about performance improvement. The keywords: ""costs a bit of time"", ""removed the cache‚Äîlet's see if it pays for itself."" So it's performance. So classification: performance.assistantfinalperformance"
2288772617,1133,Hint optimization support,AYadrov,45910827,closed,2025-01-21T01:10:46Z,2025-01-24T01:24:35Z,2025-01-24T01:24:35Z,https://api.github.com/repos/herbie-fp/herbie,https://github.com/herbie-fp/herbie/pull/1133,"This PR introduces a support for Rival's new feature called _hint_ for reducing computational path by a given insight.
The insight gets obtained from Rival at `analyze` stage and can be applied to any point within the analyzed hyperrect.
The feature mainly affects benchmarks with `min`/`max`/`if`/`assert` operations, and more such instructions a benchmark has - more speedup is observed.

To support the feature, the search function and `sampler` got updated.
Search function now continues the search not only when the expression has not produced an error, but also when the insight has not converged yet. 
For better understanding, thinner the hyperrect - more computational paths can be pruned, once nothing left to prune - the insight gets converged.
This logic brings ~3 mins of speedup when comparing [main](https://nightly.cs.washington.edu/reports/herbie/1737372343:nightly:main:bb83a5a804) and [branch](https://nightly.cs.washington.edu/reports/herbie/1737414248:nightly:min-max-optimizations-support:39a4b18684/), 4 mins of speedup on sampling and ~1.5mins of slowdown at analyze.
 ",Human,"analysisWe need to classify as performance or non-performance. The PR description: ""Hint optimization support"" and mentions speedup, computational path reduction, speedup of 3 mins, 4 mins, etc. It's clearly performance improvement. So label 'performance'.assistantfinalperformance"
2425740828,1172,We now always constant-fold when doing egraphs,pavpanchekha,30707,closed,2025-03-28T16:36:02Z,2025-03-29T17:21:34Z,2025-03-29T17:21:34Z,https://api.github.com/repos/herbie-fp/herbie,https://github.com/herbie-fp/herbie/pull/1172,This is also enabled by #1167 and #1168.,Human,"analysisWe need to classify whether the primary purpose is performance improvement. The PR title: ""We now always constant-fold when doing egraphs"". Body: ""This is also enabled by #1167 and #1168."" So it's about constant folding in egraphs. That is a performance optimization: constant folding reduces runtime cost. So it's performance. So label 'performance'.assistantfinalperformance"
2288671373,1132,Fix derivation speed broken by #1130,pavpanchekha,30707,closed,2025-01-20T22:49:26Z,2025-01-21T00:01:58Z,2025-01-21T00:01:58Z,https://api.github.com/repos/herbie-fp/herbie,https://github.com/herbie-fp/herbie/pull/1132,"PR #1130 was good but made derivations slower; I just set up the cache wrong, using `eq?` instead of `equal?`, which meant we were computing the same proof many times. This PR changes the cache, making derivations fast again.",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Fix derivation speed broken by #1130"". Body: ""PR #1130 was good but made derivations slower; I just set up the cache wrong, using `eq?` instead of `equal?`, which meant we were computing the same proof many times. This PR changes the cache, making derivations fast again.""

Clearly performance improvement. So label 'performance'.assistantfinalperformance"
2278232233,1124,Change the encoding of lifting and lowering rules,pavpanchekha,30707,closed,2025-01-15T03:52:56Z,2025-01-27T23:20:05Z,,https://api.github.com/repos/herbie-fp/herbie,https://github.com/herbie-fp/herbie/pull/1124,"In the current egg egraph we have `(exp 1) = (exp.f64 1)` during rewriting, which we enforce with rewrite rules like `(expm1.f64 x) = (- (exp x) 1)`. Eclasses thus contain both real and float terms. Then, we manually go through the egraph and split everything. This is slow and requires a _lot_ of code.

But in egglog we adopt a different encoding, where `(exp 1)` is a different eclass from `(exp.f64 1)`. The relationship between them is established by a function, `impl`, so that `(exp.f64 x) = (impl f64 (exp y))`. With this encoding, real terms like `(exp y)` and float terms like `(impl f64 (exp y))` end up in different e-classes, so there's no need to split anything. We can extract what we need without having to split the egraph, though we do need to make sure not to extract the `impl` pseudo-operator.

Then there are various rules on `lower` like:

    (exp.f64 (impl binary64 x)) = (impl binary64 (exp x))
    (expm1.f64 (impl binary64 x)) = (impl binary64 (- (exp x) 1))
    (cast.f32_f64 (impl binary32 x)) = (impl binary64 x)

Note that the `impl` terms on the left indicate argument types while `impl` terms on the right indicate the return type. The `x` variable is always bound to a real expression. Available operators, their signatures, and their specs are all expressed using rules, which should hopefully give us the speed of egg without messing with nasty FFI stuff. In an ideal case I think the egg portion can be >2x faster, leading to a >10% speedup to Herbie.









",Human,"analysisWe need to classify whether the primary purpose of the pull request is to improve performance. The description: ""Change the encoding of lifting and lowering rules... This is slow and requires a lot of code.... With this encoding, real terms like (exp y) and float terms like (impl binary64 x) end up in different e-classes, so there's no need to split anything. We can extract what we need without having to split the egraph, though we do need to make sure not to extract the impl pseudo-operator.... In an ideal case I think the egg portion can be >2x faster, leading to a >10% speedup to Herbie.""

This is clearly about performance improvement: they want to speed up the egraph rewriting by changing encoding, leading to >2x faster, >10% speedup. So classification: 'performance'.

Thus output: performance.assistantfinalperformance"
2408616836,1092,‚ö°Ô∏è Speed up function `select_top_confidence_detection` by 188%,misrasaurabh1,1271289,closed,2025-03-21T06:00:21Z,2025-03-26T13:40:42Z,2025-03-26T13:40:42Z,https://api.github.com/repos/roboflow/inference,https://github.com/roboflow/inference/pull/1092,"### üìÑ 188% (1.88x) speedup for ***`select_top_confidence_detection` in `inference/core/workflows/core_steps/common/query_language/operations/detections/base.py`***

‚è±Ô∏è Runtime :   **`1.02 millisecond`**  **‚Üí** **`355 microseconds`** (best of `491` runs)
<details>
<summary> üìù Explanation and details</summary>

Here is an optimized version of the program.

</details>

‚úÖ **Correctness verification report:**


| Test                        | Status            |
| --------------------------- | ----------------- |
| ‚öôÔ∏è Existing Unit Tests | üîò **None Found** |
| üåÄ Generated Regression Tests | ‚úÖ **20 Passed** |
| ‚è™ Replay Tests | üîò **None Found** |
| üîé Concolic Coverage Tests | üîò **None Found** |
|üìä Tests Coverage       | 100.0% |
<details>
<summary>üåÄ Generated Regression Tests Details</summary>

```python
from copy import deepcopy

import numpy as np
# imports
import pytest  # used for our unit tests
from inference.core.workflows.core_steps.common.query_language.operations.detections.base import \
    select_top_confidence_detection


# Mocking the sv.Detections class for testing purposes
class Detections:
    def __init__(self, confidence):
        self.confidence = np.array(confidence)
    
    def __len__(self):
        return len(self.confidence)
    
    def __getitem__(self, index):
        return Detections([self.confidence[index]])
from inference.core.workflows.core_steps.common.query_language.operations.detections.base import \
    select_top_confidence_detection

# unit tests

def test_single_detection():
    # Single detection
    detections = Detections([0.5])
    codeflash_output = select_top_confidence_detection(detections)

def test_multiple_unique_confidence_scores():
    # Multiple detections with unique confidence scores
    detections = Detections([0.1, 0.5, 0.9])
    codeflash_output = select_top_confidence_detection(detections)

def test_empty_detections():
    # Empty detections
    detections = Detections([])
    codeflash_output = select_top_confidence_detection(detections)

def test_all_zero_confidence_scores():
    # All zero confidence scores
    detections = Detections([0.0, 0.0, 0.0])
    codeflash_output = select_top_confidence_detection(detections)

def test_multiple_maximum_confidence_scores():
    # Multiple maximum confidence scores
    detections = Detections([0.9, 0.9, 0.8])
    codeflash_output = select_top_confidence_detection(detections)

def test_large_number_of_detections():
    # Large number of detections
    large_confidences = np.random.rand(1000)
    detections = Detections(large_confidences)
    codeflash_output = select_top_confidence_detection(detections)

def test_high_precision_confidence_scores():
    # High precision confidence scores
    detections = Detections([0.123456789, 0.987654321, 0.555555555])
    codeflash_output = select_top_confidence_detection(detections)

def test_negative_confidence_scores():
    # Negative confidence scores
    detections = Detections([-0.1, -0.5, -0.9])
    codeflash_output = select_top_confidence_detection(detections)


def test_minimum_and_maximum_float_values():
    # Minimum and maximum float values
    detections = Detections([np.finfo(float).min, np.finfo(float).max])
    codeflash_output = select_top_confidence_detection(detections)

def test_immutable_input():
    # Ensure the original detections object is not modified
    detections = Detections([0.5, 0.7, 0.9])
    original_confidences = deepcopy(detections.confidence)
    codeflash_output = select_top_confidence_detection(detections)

def test_non_numeric_confidence_scores():
    # Non-numeric confidence scores should raise an error
    detections = Detections([0.5, 'high', None])
    with pytest.raises(TypeError):
        select_top_confidence_detection(detections)

# Run the tests
if __name__ == ""__main__"":
    pytest.main()
# codeflash_output is used to check that the output of the original code is the same as that of the optimized code.

from copy import deepcopy

import numpy as np
# imports
import pytest  # used for our unit tests
from inference.core.workflows.core_steps.common.query_language.operations.detections.base import \
    select_top_confidence_detection


# Mocking the sv.Detections class for testing purposes
class MockDetections:
    def __init__(self, confidence):
        self.confidence = np.array(confidence)
    
    def __len__(self):
        return len(self.confidence)
    
    def __getitem__(self, index):
        return MockDetections([self.confidence[index]])
from inference.core.workflows.core_steps.common.query_language.operations.detections.base import \
    select_top_confidence_detection


# unit tests
def test_empty_detections():
    # Test with empty detections
    detections = MockDetections([])
    codeflash_output = select_top_confidence_detection(detections)

def test_single_detection():
    # Test with a single detection
    detections = MockDetections([0.5])
    codeflash_output = select_top_confidence_detection(detections)

def test_multiple_unique_confidences():
    # Test with multiple detections with unique confidence scores
    detections = MockDetections([0.1, 0.5, 0.9])
    codeflash_output = select_top_confidence_detection(detections)

def test_multiple_duplicate_highest_confidences():
    # Test with multiple detections with duplicate highest confidence scores
    detections = MockDetections([0.5, 0.9, 0.9])
    codeflash_output = select_top_confidence_detection(detections)

def test_all_equal_confidences():
    # Test with multiple detections with all equal confidence scores
    detections = MockDetections([0.5, 0.5, 0.5])
    codeflash_output = select_top_confidence_detection(detections)

def test_non_numeric_confidences():
    # Test with non-numeric confidence scores (if applicable)
    detections = MockDetections(['high', 'medium', 'low'])
    with pytest.raises(TypeError):
        select_top_confidence_detection(detections)


def test_large_number_of_detections():
    # Test with a large number of detections
    detections = MockDetections(np.random.rand(10000))
    codeflash_output = select_top_confidence_detection(detections)

def test_performance_large_scale():
    # Performance test with large scale dataset
    detections = MockDetections(np.random.rand(1000000))
    codeflash_output = select_top_confidence_detection(detections)

# Run the tests
if __name__ == ""__main__"":
    pytest.main()
# codeflash_output is used to check that the output of the original code is the same as that of the optimized code.
```

</details>



[![Codeflash](https://img.shields.io/badge/Optimized%20with-Codeflash-yellow?style=flat&color=%23ffc428&logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNDgwIiBoZWlnaHQ9ImF1dG8iIHZpZXdCb3g9IjAgMCA0ODAgMjgwIiBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgo8cGF0aCBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTI4Ni43IDAuMzc4NDE4SDIwMS43NTFMNTAuOTAxIDE0OC45MTFIMTM1Ljg1MUwwLjk2MDkzOCAyODEuOTk5SDk1LjQzNTJMMjgyLjMyNCA4OS45NjE2SDE5Ni4zNDVMMjg2LjcgMC4zNzg0MThaIiBmaWxsPSIjRkZDMDQzIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMzExLjYwNyAwLjM3ODkwNkwyNTguNTc4IDU0Ljk1MjZIMzc5LjU2N0w0MzIuMzM5IDAuMzc4OTA2SDMxMS42MDdaIiBmaWxsPSIjMEIwQTBBIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMzA5LjU0NyA4OS45NjAxTDI1Ni41MTggMTQ0LjI3NkgzNzcuNTA2TDQzMC4wMjEgODkuNzAyNkgzMDkuNTQ3Vjg5Ljk2MDFaIiBmaWxsPSIjMEIwQTBBIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMjQyLjg3MyAxNjQuNjZMMTg5Ljg0NCAyMTkuMjM0SDMxMC44MzNMMzYzLjM0NyAxNjQuNjZIMjQyLjg3M1oiIGZpbGw9IiMwQjBBMEEiLz4KPC9zdmc+Cg==)](https://codeflash.ai)
",Human,"analysisWe need to classify as 'performance' or 'non-performance'. The PR title: ""‚ö°Ô∏è Speed up function `select_top_confidence_detection` by 188%"". That clearly indicates performance improvement. So classification: performance.assistantfinalperformance"
2616290996,1385,‚ö°Ô∏è Speed up method `WithFixedSizeCache.add_model` by 50% in PR #1373 (`feat/pass-countinference-to-serverless-getweights`),codeflash-ai[bot],148906541,closed,2025-06-24T21:57:23Z,2025-06-26T08:59:58Z,,https://api.github.com/repos/roboflow/inference,https://github.com/roboflow/inference/pull/1385,"## ‚ö°Ô∏è This pull request contains optimizations for PR #1373
If you approve this dependent PR, these changes will be merged into the original PR branch `feat/pass-countinference-to-serverless-getweights`.
>This PR will be automatically closed if the original PR is merged.
----
### üìÑ 50% (0.50x) speedup for ***`WithFixedSizeCache.add_model` in `inference/core/managers/decorators/fixed_size_cache.py`***

‚è±Ô∏è Runtime :   **`1.08 seconds`**  **‚Üí** **`722 milliseconds`** (best of `12` runs)
### üìù Explanation and details

Here's an optimized rewrite of your program, addressing profiling hot spots and general efficiency improvements.

**Optimization Summary:**

1. **Avoid Redundant Method Calls:** 
   - Minimize repeated lookups and calculations.
   - Cache computations/results when possible within function scope.
2. **Lazy Imports:** 
   - Move GC and optional torch imports where needed (they are only used upon eviction).
3. **Deque Optimizations:** 
   - In `WithFixedSizeCache.add_model`, avoid repeated `self._key_queue.remove(queue_id)` by checking position or maintaining a set for fast checks (no need, since only called if known present, and block is rare). Still, code can be reduced for clarity.
4. **Reduce logging** in the hot add logic (unless DEBUG mode; logging is a major time sink during profiling).
5. **Batch Removals:** 
   - Accumulate models to remove and do a single `gc.collect()` call after, instead of per-iteration. 
6. **Data structure** choices are left unchanged (deque is still best for explicit ordering here).
7. **General Logic**: Use local variables for lookups on attributes used multiple times (minor, but helps).

---




**Key Runtime Optimizations:**
- Only call `gc.collect()` after all removals in a batch, not after every single model eviction.
- Reduced logging in hot code paths (this was responsible for noticeable time in profiling).
- Use local variables when repeatedly accessing class attributes.
- Use direct inlining for `_resolve_queue_id` for this use case.
- Defensive handling if queue/model state falls out of sync‚Äînever throws unnecessarily.

**Performance Note:**
If you profile again after these changes, most of the time will now be in actual model loading and removal. That is, this code will not be a noticeable bottleneck anymore in the workflow. If LRU cache size is much larger, consider further data structure optimizations such as a dict for constant-time eviction and presence checking, but for N ~ 8 this is not needed.


‚úÖ **Correctness verification report:**

| Test                        | Status            |
| --------------------------- | ----------------- |
| ‚è™ Replay Tests | üîò **None Found** |
| ‚öôÔ∏è Existing Unit Tests | üîò **None Found** |
| üîé Concolic Coverage Tests | üîò **None Found** |
| üåÄ Generated Regression Tests | ‚úÖ **476 Passed** |
|üìä Tests Coverage       | 85.2% |
<details>
<summary>üåÄ Generated Regression Tests and Runtime</summary>

```python
import sys
from collections import deque

# imports
import pytest
from inference.core.managers.decorators.fixed_size_cache import \
    WithFixedSizeCache

# function to test and minimal stubs/mocks

class DummyModel:
    """"""Minimal dummy model for testing.""""""
    def __init__(self, model_id, api_key):
        self.model_id = model_id
        self.api_key = api_key
        self.has_model_metadata = False

    def clear_cache(self, delete_from_disk=True):
        pass

class DummyModelRegistry:
    """"""Minimal dummy registry that returns DummyModel.""""""
    def get_model(self, resolved_identifier, api_key, countinference=None, service_secret=None):
        return DummyModel
class InferenceModelNotFound(Exception): pass
class InvalidModelIDError(Exception): pass

# Enum stub
class ModelEndpointType:
    ORT = type(""ORT"", (), {""value"": ""ort""})()
    value = ""ort""

# ModelManager and WithFixedSizeCache as in prompt, but minimal
class ModelManager:
    def __init__(self, model_registry, models=None):
        self.model_registry = model_registry
        self._models = models if models is not None else {}

    def add_model(self, model_id, api_key, model_id_alias=None, endpoint_type=ModelEndpointType.ORT, countinference=None, service_secret=None):
        resolved_identifier = model_id if model_id_alias is None else model_id_alias
        if resolved_identifier in self._models:
            return
        model_class = self.model_registry.get_model(resolved_identifier, api_key, countinference=countinference, service_secret=service_secret)
        model = model_class(model_id=model_id, api_key=api_key)
        self._models[resolved_identifier] = model

    def remove(self, model_id, delete_from_disk=True):
        if model_id not in self._models:
            raise InferenceModelNotFound()
        self._models[model_id].clear_cache(delete_from_disk=delete_from_disk)
        del self._models[model_id]

    def __contains__(self, model_id):
        return model_id in self._models

    def __getitem__(self, key):
        if key not in self._models:
            raise InferenceModelNotFound()
        return self._models[key]

    def __len__(self):
        return len(self._models)

    def keys(self):
        return self._models.keys()

# ========== UNIT TESTS BELOW ==========

@pytest.fixture
def cache_manager():
    """"""Returns a WithFixedSizeCache with max_size=3 for testing.""""""
    registry = DummyModelRegistry()
    base_manager = ModelManager(registry)
    return WithFixedSizeCache(base_manager, max_size=3)

@pytest.fixture
def unique_model_id():
    """"""Returns a function to generate unique model_ids for tests.""""""
    counter = [0]
    def _gen():
        counter[0] += 1
        return f""dataset{counter[0]}/1""
    return _gen

# 1. BASIC TEST CASES

def test_add_single_model(cache_manager, unique_model_id):
    """"""Test adding a single model works and is present.""""""
    model_id = unique_model_id()
    cache_manager.add_model(model_id, api_key=""key"")

def test_add_duplicate_model_noop(cache_manager, unique_model_id):
    """"""Adding the same model twice does not increase count.""""""
    model_id = unique_model_id()
    cache_manager.add_model(model_id, api_key=""key"")
    cache_manager.add_model(model_id, api_key=""key"")

def test_add_model_with_alias(cache_manager, unique_model_id):
    """"""Adding with an alias stores under the alias, not model_id.""""""
    model_id = unique_model_id()
    alias = ""alias1""
    cache_manager.add_model(model_id, api_key=""key"", model_id_alias=alias)

def test_add_multiple_models_up_to_capacity(cache_manager, unique_model_id):
    """"""Add up to max_size models, all should be present.""""""
    ids = [unique_model_id() for _ in range(3)]
    for mid in ids:
        cache_manager.add_model(mid, api_key=""key"")
    for mid in ids:
        pass

# 2. EDGE TEST CASES

def test_eviction_on_capacity(cache_manager, unique_model_id):
    """"""Adding more than max_size evicts least recently used.""""""
    ids = [unique_model_id() for _ in range(4)]
    for mid in ids[:3]:
        cache_manager.add_model(mid, api_key=""key"")
    # Now add a 4th, should evict ids[0]
    cache_manager.add_model(ids[3], api_key=""key"")

def test_eviction_marks_mru(cache_manager, unique_model_id):
    """"""Adding a model again marks it as most recently used (no eviction).""""""
    ids = [unique_model_id() for _ in range(3)]
    for mid in ids:
        cache_manager.add_model(mid, api_key=""key"")
    # Access ids[0] to mark it as MRU
    cache_manager.add_model(ids[0], api_key=""key"")
    # Add new model, should evict ids[1] now (was LRU)
    new_id = unique_model_id()
    cache_manager.add_model(new_id, api_key=""key"")

def test_add_model_with_alias_then_same_id(cache_manager, unique_model_id):
    """"""Adding with alias, then with same model_id, both can exist.""""""
    model_id = unique_model_id()
    alias = ""alias2""
    cache_manager.add_model(model_id, api_key=""key"", model_id_alias=alias)
    cache_manager.add_model(model_id, api_key=""key"")

def test_add_model_eviction_multiple_rounds(cache_manager, unique_model_id):
    """"""Eviction removes 3 at a time if possible when over threshold.""""""
    # Fill up to 3
    ids = [unique_model_id() for _ in range(3)]
    for mid in ids:
        cache_manager.add_model(mid, api_key=""key"")
    # Add 4th, should evict 1st
    cache_manager.add_model(""dataset999/1"", api_key=""key"")
    # Add 5th, should evict 3 more (but only 3 in cache, so only possible to evict all)
    cache_manager.add_model(""dataset1000/1"", api_key=""key"")

def test_remove_model(cache_manager, unique_model_id):
    """"""Test removing a model actually removes it.""""""
    model_id = unique_model_id()
    cache_manager.add_model(model_id, api_key=""key"")
    cache_manager.remove(model_id)

def test_remove_nonexistent_model_raises(cache_manager):
    """"""Removing a model not present raises InferenceModelNotFound.""""""
    with pytest.raises(InferenceModelNotFound):
        cache_manager.remove(""not-present/1"")


def test_add_model_with_alias_eviction(cache_manager, unique_model_id):
    """"""Eviction works when models are added by alias.""""""
    ids = [unique_model_id() for _ in range(2)]
    alias = ""alias3""
    cache_manager.add_model(ids[0], api_key=""key"", model_id_alias=alias)
    cache_manager.add_model(ids[1], api_key=""key"")
    cache_manager.add_model(""dataset888/1"", api_key=""key"")
    # Now add another to force eviction
    cache_manager.add_model(""dataset889/1"", api_key=""key"")
    # At least one of the first 3 should be evicted
    count = sum(mid in cache_manager for mid in [alias, ids[1], ""dataset888/1""])

def test_lru_eviction_order(cache_manager, unique_model_id):
    """"""Eviction order is LRU, not FIFO.""""""
    ids = [unique_model_id() for _ in range(3)]
    for mid in ids:
        cache_manager.add_model(mid, api_key=""key"")
    # Access ids[1] to make it MRU
    cache_manager.add_model(ids[1], api_key=""key"")
    # Add new model, should evict ids[0]
    new_id = unique_model_id()
    cache_manager.add_model(new_id, api_key=""key"")

def test_add_model_memory_pressure(monkeypatch, cache_manager, unique_model_id):
    """"""If memory_pressure_detected returns True, eviction is triggered.""""""
    monkeypatch.setattr(cache_manager, ""memory_pressure_detected"", lambda: True)
    # Fill up cache
    ids = [unique_model_id() for _ in range(3)]
    for mid in ids:
        cache_manager.add_model(mid, api_key=""key"")
    # Add another, should evict 3 at once
    cache_manager.add_model(""dataset2000/1"", api_key=""key"")

def test_add_model_exception_removes_from_queue(cache_manager, monkeypatch):
    """"""If add_model raises, queue is cleaned up.""""""
    # Patch model_manager.add_model to raise
    def raise_exc(*a, **kw): raise RuntimeError(""fail!"")
    monkeypatch.setattr(cache_manager.model_manager, ""add_model"", raise_exc)
    before_len = len(cache_manager._key_queue)
    with pytest.raises(RuntimeError):
        cache_manager.add_model(""dataset/1"", api_key=""key"")

# 3. LARGE SCALE TEST CASES

def test_large_number_of_models_eviction():
    """"""Add 10 models to a cache of size 5, only last 5 remain.""""""
    registry = DummyModelRegistry()
    base_manager = ModelManager(registry)
    cache_manager = WithFixedSizeCache(base_manager, max_size=5)
    ids = [f""ds{i}/1"" for i in range(10)]
    for mid in ids:
        cache_manager.add_model(mid, api_key=""key"")
    # Only last 5 should remain
    for mid in ids[:5]:
        pass
    for mid in ids[5:]:
        pass

def test_stress_add_and_access():
    """"""Add 20 models, repeatedly access some to keep them in cache.""""""
    registry = DummyModelRegistry()
    base_manager = ModelManager(registry)
    cache_manager = WithFixedSizeCache(base_manager, max_size=10)
    ids = [f""ds{i}/1"" for i in range(20)]
    for mid in ids[:10]:
        cache_manager.add_model(mid, api_key=""key"")
    # Repeatedly access first 5 to keep them MRU
    for _ in range(5):
        for mid in ids[:5]:
            cache_manager.add_model(mid, api_key=""key"")
    # Add next 10
    for mid in ids[10:]:
        cache_manager.add_model(mid, api_key=""key"")
    # The first 5 should still be in cache, next 5 should have been evicted
    for mid in ids[:5]:
        pass
    for mid in ids[5:10]:
        pass
    for mid in ids[10:]:
        pass

def test_add_models_with_aliases_large_scale():
    """"""Add 50 models with unique aliases, only last 10 remain in cache.""""""
    registry = DummyModelRegistry()
    base_manager = ModelManager(registry)
    cache_manager = WithFixedSizeCache(base_manager, max_size=10)
    for i in range(50):
        model_id = f""dataset{i}/1""
        alias = f""alias{i}""
        cache_manager.add_model(model_id, api_key=""key"", model_id_alias=alias)
    # Only last 10 aliases should be present
    for i in range(40):
        pass
    for i in range(40, 50):
        pass

def test_eviction_never_exceeds_max_size():
    """"""After many operations, cache never exceeds max_size.""""""
    registry = DummyModelRegistry()
    base_manager = ModelManager(registry)
    cache_manager = WithFixedSizeCache(base_manager, max_size=7)
    for i in range(30):
        cache_manager.add_model(f""ds{i}/1"", api_key=""key"")

def test_eviction_when_queue_empty_does_not_crash():
    """"""Eviction with empty queue does not raise.""""""
    registry = DummyModelRegistry()
    base_manager = ModelManager(registry)
    cache_manager = WithFixedSizeCache(base_manager, max_size=1)
    # Remove all models to empty queue
    cache_manager._key_queue.clear()
    try:
        cache_manager.add_model(""ds1/1"", api_key=""key"")
    except Exception:
        pytest.fail(""add_model should not raise when queue is empty"")
# codeflash_output is used to check that the output of the original code is the same as that of the optimized code.

from collections import deque

# imports
import pytest
from inference.core.managers.decorators.fixed_size_cache import \
    WithFixedSizeCache

# --- Minimal stubs and mocks for dependencies ---

# Exception classes
class RoboflowAPINotAuthorizedError(Exception):
    pass

class InferenceModelNotFound(Exception):
    pass

# ModelEndpointType enum stub
class ModelEndpointType:
    ORT = ""ort""

# Model stub
class DummyModel:
    def __init__(self, model_id, api_key):
        self.model_id = model_id
        self.api_key = api_key
        self.cleared = False

    def clear_cache(self, delete_from_disk=True):
        self.cleared = True

# ModelRegistry stub
class DummyModelRegistry:
    def get_model(self, resolved_identifier, api_key, countinference=None, service_secret=None):
        # Always returns DummyModel constructor
        return DummyModel

# --- The ModelManager, ModelManagerDecorator, and WithFixedSizeCache implementations ---

class ModelManager:
    def __init__(self, model_registry, models=None):
        self.model_registry = model_registry
        self._models = {} if models is None else models

    def add_model(
        self,
        model_id,
        api_key,
        model_id_alias=None,
        endpoint_type=ModelEndpointType.ORT,
        countinference=None,
        service_secret=None,
    ):
        resolved_identifier = model_id if model_id_alias is None else model_id_alias
        if resolved_identifier in self._models:
            return
        model_class = self.model_registry.get_model(
            resolved_identifier, api_key, countinference=countinference, service_secret=service_secret
        )
        model = model_class(model_id=model_id, api_key=api_key)
        self._models[resolved_identifier] = model

    def remove(self, model_id, delete_from_disk=True):
        if model_id not in self._models:
            raise InferenceModelNotFound(f""Model {model_id} not found"")
        self._models[model_id].clear_cache(delete_from_disk=delete_from_disk)
        del self._models[model_id]

    def __contains__(self, model_id):
        return model_id in self._models

    def __getitem__(self, key):
        if key not in self._models:
            raise InferenceModelNotFound(f""Model {key} not found"")
        return self._models[key]

    def __len__(self):
        return len(self._models)

    def keys(self):
        return self._models.keys()

# Global flag for API key check
MODELS_CACHE_AUTH_ENABLED = False

# --- UNIT TESTS ---

@pytest.fixture
def model_manager():
    # Returns a fresh ModelManager with DummyModelRegistry
    return ModelManager(DummyModelRegistry())

@pytest.fixture
def cache_manager(model_manager):
    # Returns a WithFixedSizeCache wrapping the above
    return WithFixedSizeCache(model_manager, max_size=4)

# 1. BASIC TEST CASES

def test_add_single_model_basic(cache_manager):
    """"""Test adding a single model to an empty cache.""""""
    cache_manager.add_model(""modelA/1"", ""KEY"")

def test_add_duplicate_model_noop(cache_manager):
    """"""Test that adding the same model twice does not increase cache size.""""""
    cache_manager.add_model(""modelA/1"", ""KEY"")
    cache_manager.add_model(""modelA/1"", ""KEY"")

def test_add_model_with_alias(cache_manager):
    """"""Test adding a model with an alias as queue id.""""""
    cache_manager.add_model(""modelA/1"", ""KEY"", model_id_alias=""aliasA"")

def test_add_model_with_different_aliases(cache_manager):
    """"""Test that different aliases are treated as different cache entries.""""""
    cache_manager.add_model(""modelA/1"", ""KEY"", model_id_alias=""aliasA"")
    cache_manager.add_model(""modelA/1"", ""KEY"", model_id_alias=""aliasB"")

def test_add_multiple_models_basic(cache_manager):
    """"""Test adding multiple distinct models.""""""
    cache_manager.add_model(""modelA/1"", ""KEY"")
    cache_manager.add_model(""modelB/1"", ""KEY"")
    cache_manager.add_model(""modelC/1"", ""KEY"")

# 2. EDGE TEST CASES

def test_add_model_eviction_lru(cache_manager):
    """"""Test that adding models over max_size evicts least recently used.""""""
    # Fill up cache
    cache_manager.add_model(""A/1"", ""KEY"")
    cache_manager.add_model(""B/1"", ""KEY"")
    cache_manager.add_model(""C/1"", ""KEY"")
    cache_manager.add_model(""D/1"", ""KEY"")
    # Add one more, triggers eviction (removes A/1, B/1, C/1 in order)
    cache_manager.add_model(""E/1"", ""KEY"")
    # Add another, triggers more evictions
    cache_manager.add_model(""F/1"", ""KEY"")

def test_add_model_lru_refresh(cache_manager):
    """"""Test that re-adding an existing model refreshes its LRU position.""""""
    cache_manager.add_model(""A/1"", ""KEY"")
    cache_manager.add_model(""B/1"", ""KEY"")
    cache_manager.add_model(""C/1"", ""KEY"")
    cache_manager.add_model(""D/1"", ""KEY"")
    # Refresh A/1
    cache_manager.add_model(""A/1"", ""KEY"")
    # Add E/1, should evict B/1, C/1, D/1 (A/1 was refreshed)
    cache_manager.add_model(""E/1"", ""KEY"")


def test_add_model_with_invalid_model_id(cache_manager):
    """"""Test that a model_id_alias with same name as another model_id is treated as distinct.""""""
    cache_manager.add_model(""modelA/1"", ""KEY"")
    cache_manager.add_model(""modelB/1"", ""KEY"", model_id_alias=""modelA/1"")

def test_add_model_evicts_all_when_cache_full(cache_manager):
    """"""Test that if more than max_size+3 models are added, all old models are evicted.""""""
    # Fill cache
    cache_manager.add_model(""A/1"", ""KEY"")
    cache_manager.add_model(""B/1"", ""KEY"")
    cache_manager.add_model(""C/1"", ""KEY"")
    cache_manager.add_model(""D/1"", ""KEY"")
    # Add 4 more, causing two eviction rounds
    cache_manager.add_model(""E/1"", ""KEY"")
    cache_manager.add_model(""F/1"", ""KEY"")
    cache_manager.add_model(""G/1"", ""KEY"")
    cache_manager.add_model(""H/1"", ""KEY"")
    # Only last 4 models should remain
    for mid in [""E/1"", ""F/1"", ""G/1"", ""H/1""]:
        pass
    for mid in [""A/1"", ""B/1"", ""C/1"", ""D/1""]:
        pass

def test_add_model_handles_exception_and_removes_from_queue(cache_manager):
    """"""Test that if ModelManager.add_model raises, the queue is cleaned up.""""""
    # Patch model_manager.add_model to raise
    orig_add_model = cache_manager.model_manager.add_model
    def raise_exc(*a, **kw):
        raise ValueError(""fail!"")
    cache_manager.model_manager.add_model = raise_exc
    with pytest.raises(ValueError):
        cache_manager.add_model(""Z/1"", ""KEY"")
    # Restore
    cache_manager.model_manager.add_model = orig_add_model

def test_add_model_with_alias_and_duplicate(cache_manager):
    """"""Test that adding same model with and without alias treats them as separate.""""""
    cache_manager.add_model(""A/1"", ""KEY"")
    cache_manager.add_model(""A/1"", ""KEY"", model_id_alias=""aliasA"")

# 3. LARGE SCALE TEST CASES

def test_add_many_models_and_evictions():
    """"""Test adding up to 20 models with cache size 10, check LRU eviction.""""""
    mm = ModelManager(DummyModelRegistry())
    cache = WithFixedSizeCache(mm, max_size=10)
    # Add 20 models
    for i in range(20):
        cache.add_model(f""model{i}/1"", ""KEY"")
    # Only last 10 should remain
    for i in range(10, 20):
        pass
    for i in range(10):
        pass

def test_add_models_with_aliases_large_scale():
    """"""Test adding models with unique aliases does not cause collisions.""""""
    mm = ModelManager(DummyModelRegistry())
    cache = WithFixedSizeCache(mm, max_size=50)
    # Add 50 models with unique aliases
    for i in range(50):
        cache.add_model(f""modelX/1"", ""KEY"", model_id_alias=f""alias_{i}"")
    # All aliases should be present
    for i in range(50):
        pass

def test_lru_eviction_pattern_stress():
    """"""Test LRU eviction pattern with repeated access and additions.""""""
    mm = ModelManager(DummyModelRegistry())
    cache = WithFixedSizeCache(mm, max_size=5)
    # Add 5 models
    for i in range(5):
        cache.add_model(f""M{i}/1"", ""KEY"")
    # Access models to change LRU order
    cache.add_model(""M2/1"", ""KEY"")
    cache.add_model(""M4/1"", ""KEY"")
    # Add new model, should evict oldest (M0/1, M1/1, M3/1 in order)
    cache.add_model(""M5/1"", ""KEY"")
    # Only most recently used and new should remain
    for mid in [""M2/1"", ""M4/1"", ""M5/1""]:
        pass

def test_add_models_performance_under_load():
    """"""Test that adding 100 models with cache size 50 only keeps last 50.""""""
    mm = ModelManager(DummyModelRegistry())
    cache = WithFixedSizeCache(mm, max_size=50)
    for i in range(100):
        cache.add_model(f""large_{i}/1"", ""KEY"")
    for i in range(50, 100):
        pass
    for i in range(50):
        pass

def test_add_models_with_same_alias_large_scale():
    """"""Test that adding many models with same alias overwrites previous.""""""
    mm = ModelManager(DummyModelRegistry())
    cache = WithFixedSizeCache(mm, max_size=10)
    for i in range(20):
        cache.add_model(f""modelQ_{i}/1"", ""KEY"", model_id_alias=""shared_alias"")
# codeflash_output is used to check that the output of the original code is the same as that of the optimized code.
```

</details>


To edit these changes `git checkout codeflash/optimize-pr1373-2025-06-24T21.57.17` and push.

[![Codeflash](https://img.shields.io/badge/Optimized%20with-Codeflash-yellow?style=flat&color=%23ffc428&logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNDgwIiBoZWlnaHQ9ImF1dG8iIHZpZXdCb3g9IjAgMCA0ODAgMjgwIiBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgo8cGF0aCBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTI4Ni43IDAuMzc4NDE4SDIwMS43NTFMNTAuOTAxIDE0OC45MTFIMTM1Ljg1MUwwLjk2MDkzOCAyODEuOTk5SDk1LjQzNTJMMjgyLjMyNCA4OS45NjE2SDE5Ni4zNDVMMjg2LjcgMC4zNzg0MThaIiBmaWxsPSIjRkZDMDQzIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMzExLjYwNyAwLjM3ODkwNkwyNTguNTc4IDU0Ljk1MjZIMzc5LjU2N0w0MzIuMzM5IDAuMzc4OTA2SDMxMS42MDdaIiBmaWxsPSIjMEIwQTBBIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMzA5LjU0NyA4OS45NjAxTDI1Ni41MTggMTQ0LjI3NkgzNzcuNTA2TDQzMC4wMjEgODkuNzAyNkgzMDkuNTQ3Vjg5Ljk2MDFaIiBmaWxsPSIjMEIwQTBBIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMjQyLjg3MyAxNjQuNjZMMTg5Ljg0NCAyMTkuMjM0SDMxMC44MzNMMzYzLjM0NyAxNjQuNjZIMjQyLjg3M1oiIGZpbGw9IiMwQjBBMEEiLz4KPC9zdmc+Cg==)](https://codeflash.ai)",Human,"analysisWe need to classify the pull request as 'performance' or 'non-performance'. The PR title: ""‚ö°Ô∏è Speed up method `WithFixedSizeCache.add_model` by 50% in PR #1373"". The body describes performance optimizations, speedup, runtime improvements, etc. So it's definitely performance. So output 'performance'.assistantfinalperformance"
2519831355,1280,‚ö°Ô∏è Speed up function `execute_gpt_4v_request` by 100% in PR #1214 (`openai-apikey-passthrough`),codeflash-ai[bot],148906541,closed,2025-05-14T16:33:00Z,2025-05-19T19:46:04Z,2025-05-19T19:46:04Z,https://api.github.com/repos/roboflow/inference,https://github.com/roboflow/inference/pull/1280,"## ‚ö°Ô∏è This pull request contains optimizations for PR #1214
If you approve this dependent PR, these changes will be merged into the original PR branch `openai-apikey-passthrough`.
>This PR will be automatically closed if the original PR is merged.
----
### üìÑ 100% (1.00x) speedup for ***`execute_gpt_4v_request` in `inference/core/workflows/core_steps/models/foundation/openai/v3.py`***

‚è±Ô∏è Runtime :   **`107 milliseconds`**  **‚Üí** **`53.6 milliseconds`** (best of `5` runs)
### üìù Explanation and details

Here is an optimized version of your program for runtime and memory. The majority of runtime is IO/network-bound (API requests) and not CPU-bound code, so the best possible single-process CPU optimization is to **avoid repeated work** (e.g., repeated endpoint string formatting or client allocation) and **simplify fast paths**. If you can batch or async requests, that would reduce end-to-end latency, but that changes function signatures and semantics so is out of scope. Here we focus on making your function as lean as possible within its expected use. 

**Key improvements:**
- **Reuse OpenAI client (`OpenAI`) where possible**: Creating the client is surprisingly expensive per your profiling.
- **Optimize prompt and payload building:** Avoid unnecessary field-level assignments.
- **Use exception chaining efficiently.**
- **Minimize calls to `.startswith()` by using a tuple form.**
- **Precompute endpoint format string if possible.**
- **Move non-error computations out of try/except.**



**Summary:**  
- OpenAI client creation is now cached, saving repeated cost.
- Efficient prefix checking for OpenAI key.
- Payloads & try/except blocks are trimmed for speed and clarity.
- Function signatures and return values are preserved.
- Comments are updated only where logic is improved or needs clarification.

If you control parallelism at a higher level, running requests in parallel (with `asyncio` or threading) would yield much higher throughput as both requests and OpenAI are IO bound.


‚úÖ **Correctness verification report:**

| Test                        | Status            |
| --------------------------- | ----------------- |
| ‚öôÔ∏è Existing Unit Tests | üîò **None Found** |
| üåÄ Generated Regression Tests | ‚úÖ **9 Passed** |
| ‚è™ Replay Tests | üîò **None Found** |
| üîé Concolic Coverage Tests | üîò **None Found** |
|üìä Tests Coverage       |  |
<details>
<summary>üåÄ Generated Regression Tests Details</summary>

```python
import types
from typing import List, Optional

# imports
import pytest  # used for our unit tests
# function to test
import requests
from inference.core.env import API_BASE_URL
from inference.core.workflows.core_steps.models.foundation.openai.v3 import \
    execute_gpt_4v_request
from openai import OpenAI
from openai._types import NOT_GIVEN

# unit tests


# --------- Test helpers and monkeypatching ---------
class DummyResponse:
    """"""A dummy response object to simulate requests.Response.""""""
    def __init__(self, json_data=None, status_code=200, raise_exc=None, text=None):
        self._json_data = json_data or {}
        self.status_code = status_code
        self._raise_exc = raise_exc
        self.text = text or str(json_data)
    def json(self):
        return self._json_data
    def raise_for_status(self):
        if self._raise_exc:
            raise self._raise_exc

# --------- Basic Test Cases ---------











def test_proxied_request_missing_content(monkeypatch):
    """"""Test proxied request with missing 'content' in response (should raise).""""""
    def bad_post(url, json):
        return DummyResponse({""choices"": [{""message"": {}}]}, status_code=200)
    monkeypatch.setattr(requests, ""post"", bad_post)
    with pytest.raises(RuntimeError) as excinfo:
        execute_gpt_4v_request(
            roboflow_api_key=""rfkey123"",
            openai_api_key=""rf_key:account:abc"",
            prompt=[{""role"": ""user"", ""content"": ""Say hi""}],
            gpt_model_version=""gpt-4v"",
            max_tokens=10,
            temperature=0.5,
        )

def test_proxied_request_http_error(monkeypatch):
    """"""Test proxied request with HTTP error (should raise).""""""
    def bad_post(url, json):
        return DummyResponse({}, status_code=500, raise_exc=requests.HTTPError(""500""))
    monkeypatch.setattr(requests, ""post"", bad_post)
    with pytest.raises(RuntimeError) as excinfo:
        execute_gpt_4v_request(
            roboflow_api_key=""rfkey123"",
            openai_api_key=""rf_key:account:abc"",
            prompt=[{""role"": ""user"", ""content"": ""Say hi""}],
            gpt_model_version=""gpt-4v"",
            max_tokens=10,
            temperature=0.5,
        )

def test_direct_request_exception(monkeypatch):
    """"""Test direct request with OpenAI client raising exception (should raise).""""""
    class FailingOpenAIClient:
        def __init__(self, api_key):
            pass
        @property
        def chat(self):
            class C:
                @property
                def completions(self):
                    class D:
                        def create(self, *a, **k):
                            raise Exception(""OpenAI failure"")
                    return D()
            return C()
    monkeypatch.setattr(""openai.OpenAI"", lambda api_key: FailingOpenAIClient(api_key))
    with pytest.raises(RuntimeError) as excinfo:
        execute_gpt_4v_request(
            roboflow_api_key=""rfkey123"",
            openai_api_key=""sk-openai-002"",
            prompt=[{""role"": ""user"", ""content"": ""Say hi""}],
            gpt_model_version=""gpt-4v"",
            max_tokens=10,
            temperature=0.5,
        )

def test_proxied_request_index_error(monkeypatch):
    """"""Test proxied request with empty choices list (should raise).""""""
    def bad_post(url, json):
        return DummyResponse({""choices"": []}, status_code=200)
    monkeypatch.setattr(requests, ""post"", bad_post)
    with pytest.raises(RuntimeError) as excinfo:
        execute_gpt_4v_request(
            roboflow_api_key=""rfkey123"",
            openai_api_key=""rf_key:account:abc"",
            prompt=[{""role"": ""user"", ""content"": ""Say hi""}],
            gpt_model_version=""gpt-4v"",
            max_tokens=10,
            temperature=0.5,
        )

# --------- Large Scale Test Cases ---------








import types
from typing import List, Optional

# imports
import pytest  # used for our unit tests
# function to test
import requests
from inference.core.env import API_BASE_URL
from inference.core.workflows.core_steps.models.foundation.openai.v3 import \
    execute_gpt_4v_request
from openai import OpenAI
from openai._types import NOT_GIVEN

# unit tests


# --- Helpers for monkeypatching ---

class DummyResponse:
    def __init__(self, json_data, status_code=200):
        self._json = json_data
        self.status_code = status_code
        self.text = str(json_data)
    def json(self):
        return self._json
    def raise_for_status(self):
        if self.status_code >= 400:
            raise requests.exceptions.HTTPError(f""Status {self.status_code}"")

class DummyOpenAIChoices:
    def __init__(self, content):
        self.message = types.SimpleNamespace(content=content)

class DummyOpenAIResponse:
    def __init__(self, content):
        self.choices = [DummyOpenAIChoices(content)]

class DummyOpenAIChatCompletions:
    def __init__(self, content):
        self._content = content
    def create(self, model, messages, max_tokens, temperature):
        return DummyOpenAIResponse(self._content)

# --- Test cases ---

# BASIC TEST CASES


def test_proxied_openai_basic(monkeypatch):
    """"""Test proxied OpenAI call with normal parameters.""""""
    # Patch requests.post to return a dummy response
    def dummy_post(url, json):
        return DummyResponse({
            ""choices"": [
                {""message"": {""content"": ""proxied hello""}}
            ]
        })
    monkeypatch.setattr(requests, ""post"", dummy_post)
    # Patch API_BASE_URL to a dummy value for test
    monkeypatch.setattr(""inference.core.env.API_BASE_URL"", ""http://dummy"")
    # Call function with a proxied key
    codeflash_output = execute_gpt_4v_request(
        roboflow_api_key=""rf_dummy"",
        openai_api_key=""rf_key:account:abc123"",
        prompt=[{""role"": ""user"", ""content"": ""Say hello""}],
        gpt_model_version=""gpt-4-vision-preview"",
        max_tokens=10,
        temperature=0.5,
    ); result = codeflash_output


def test_invalid_openai_key(monkeypatch):
    """"""Test with an invalid OpenAI key (simulate exception from OpenAI).""""""
    def dummy_openai_init(self, api_key):
        raise Exception(""Invalid API key"")
    monkeypatch.setattr(OpenAI, ""__init__"", dummy_openai_init)
    with pytest.raises(RuntimeError) as e:
        execute_gpt_4v_request(
            roboflow_api_key=""rf_dummy"",
            openai_api_key=""sk-bad"",
            prompt=[{""role"": ""user"", ""content"": ""Test""}],
            gpt_model_version=""gpt-4-vision-preview"",
            max_tokens=10,
            temperature=0.5,
        )

def test_proxied_http_error(monkeypatch):
    """"""Test proxied call with HTTP error from requests.""""""
    def dummy_post(url, json):
        return DummyResponse({}, status_code=500)
    monkeypatch.setattr(requests, ""post"", dummy_post)
    monkeypatch.setattr(""inference.core.env.API_BASE_URL"", ""http://dummy"")
    with pytest.raises(RuntimeError) as e:
        execute_gpt_4v_request(
            roboflow_api_key=""rf_dummy"",
            openai_api_key=""rf_key:account:bad"",
            prompt=[{""role"": ""user"", ""content"": ""Test""}],
            gpt_model_version=""gpt-4-vision-preview"",
            max_tokens=10,
            temperature=0.5,
        )

def test_proxied_invalid_response_structure(monkeypatch):
    """"""Test proxied call with invalid response structure (missing keys).""""""
    def dummy_post(url, json):
        return DummyResponse({""bad"": ""data""})
    monkeypatch.setattr(requests, ""post"", dummy_post)
    monkeypatch.setattr(""inference.core.env.API_BASE_URL"", ""http://dummy"")
    with pytest.raises(RuntimeError) as e:
        execute_gpt_4v_request(
            roboflow_api_key=""rf_dummy"",
            openai_api_key=""rf_key:user:bad"",
            prompt=[{""role"": ""user"", ""content"": ""Test""}],
            gpt_model_version=""gpt-4-vision-preview"",
            max_tokens=10,
            temperature=0.5,
        )







def test_large_scale_proxied(monkeypatch):
    """"""Test proxied call with large prompt and max_tokens.""""""
    large_prompt = [{""role"": ""user"", ""content"": f""Message {i}""} for i in range(900)]
    def dummy_post(url, json):
        return DummyResponse({
            ""choices"": [
                {""message"": {""content"": ""large proxied""}}
            ]
        })
    monkeypatch.setattr(requests, ""post"", dummy_post)
    monkeypatch.setattr(""inference.core.env.API_BASE_URL"", ""http://dummy"")
    codeflash_output = execute_gpt_4v_request(
        roboflow_api_key=""rf_dummy"",
        openai_api_key=""rf_key:account:abc123"",
        prompt=large_prompt,
        gpt_model_version=""gpt-4-vision-preview"",
        max_tokens=999,
        temperature=0.5,
    ); result = codeflash_output
```

</details>


To edit these changes `git checkout codeflash/optimize-pr1214-2025-05-14T16.32.54` and push.

[![Codeflash](https://img.shields.io/badge/Optimized%20with-Codeflash-yellow?style=flat&color=%23ffc428&logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNDgwIiBoZWlnaHQ9ImF1dG8iIHZpZXdCb3g9IjAgMCA0ODAgMjgwIiBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgo8cGF0aCBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTI4Ni43IDAuMzc4NDE4SDIwMS43NTFMNTAuOTAxIDE0OC45MTFIMTM1Ljg1MUwwLjk2MDkzOCAyODEuOTk5SDk1LjQzNTJMMjgyLjMyNCA4OS45NjE2SDE5Ni4zNDVMMjg2LjcgMC4zNzg0MThaIiBmaWxsPSIjRkZDMDQzIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMzExLjYwNyAwLjM3ODkwNkwyNTguNTc4IDU0Ljk1MjZIMzc5LjU2N0w0MzIuMzM5IDAuMzc4OTA2SDMxMS42MDdaIiBmaWxsPSIjMEIwQTBBIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMzA5LjU0NyA4OS45NjAxTDI1Ni41MTggMTQ0LjI3NkgzNzcuNTA2TDQzMC4wMjEgODkuNzAyNkgzMDkuNTQ3Vjg5Ljk2MDFaIiBmaWxsPSIjMEIwQTBBIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMjQyLjg3MyAxNjQuNjZMMTg5Ljg0NCAyMTkuMjM0SDMxMC44MzNMMzYzLjM0NyAxNjQuNjZIMjQyLjg3M1oiIGZpbGw9IiMwQjBBMEEiLz4KPC9zdmc+Cg==)](https://codeflash.ai)",Human,"analysisWe need to classify as 'performance' or 'non-performance'. The PR title: ""‚ö°Ô∏è Speed up function `execute_gpt_4v_request` by 100% in PR #1214 (`openai-apikey-passthrough`)"". The body describes optimizations, speedup, runtime reduction. Keywords: speed, performance, optimization, runtime, speedup. So it's performance. So output 'performance'.assistantfinalperformance"
2309126247,666,fixes: prevented block on start_session and cold_session,Abraham599,76934781,closed,2025-01-31T12:51:31Z,2025-03-10T23:58:30Z,,https://api.github.com/repos/AgentOps-AI/agentops,https://github.com/AgentOps-AI/agentops/pull/666,"## üì• Pull Request

**üìò Description**
these are the steps that I took:-

i)(fix for start_session) JWT fetch will happen in a background thread and queue events until the JWT is available 
ii) (fix for end_session) handle the flush in a background thread

",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""fixes: prevented block on start_session and cold_session"". Body: describes fixes for start_session and end_session, moving JWT fetch to background thread and queue events until JWT available, handle flush in background thread. This is about preventing blocking, likely improving performance or responsiveness. The keywords: ""prevented block"" indicates performance improvement. So classify as 'performance'.assistantfinalperformance"
2593194103,42,Full Fix and Update,Zevas1993,188794821,open,2025-06-15T03:15:14Z,,,https://api.github.com/repos/leonardsellem/n8n-mcp-server,https://github.com/leonardsellem/n8n-mcp-server/pull/42,"This is for connecting ai agents/claude desktop/Cursor/etc to a locally hosted n8n instance on docker desktop. This update also further implements other features, such as connecting nodes and adding proper nodes, creating and deleting workflows, updating credentials, updating folders, and so much more. I use the api/v1/docs to parse the entire api setup for the locally hosted n8n.

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->
## Summary by CodeRabbit

- **New Features**
  - Added a comprehensive AI-powered Outlook Email Assistant workflow automating email ingestion, AI classification, response generation, storage, analytics, and notifications.
  - Introduced a complete PostgreSQL database schema supporting advanced email processing, analytics, conversation tracking, error logging, and user preferences.
  - Enhanced the n8n node catalog with detailed metadata, examples, search, and recommendation capabilities for AI agents and workflow automation.
  - Added new, fully described node definitions for popular integrations including Airtable, Asana, Bitbucket, Box, Copper, ClickUp, Claude AI, Code, Discord, Dropbox, Facebook, and GitHub.
  - Implemented a robust node discovery system, production-ready node registry, and real-time search engine for all nodes.
  - Provided a detailed AI agent guidance module, usage patterns, and best practices for workflow creation.
  - Introduced scripts for automated node verification, fixing, optimization, and comprehensive testing.
  - Added performance optimizations: smart caching, connection pooling, real-time monitoring, and enhanced error handling.
  - Supplied quick setup guides, workflow documentation, and project status reports for production deployment.

- **Bug Fixes**
  - Automated scripts to identify and fix TypeScript, formatting, and structural issues across all node files.
  - Resolved duplicate imports/exports, missing properties, and syntax errors in node definitions and registries.

- **Documentation**
  - Added extensive user, agent, and setup guides, AI optimization summaries, and troubleshooting documentation.
  - Updated workflow and API documentation examples for clarity and accuracy.

- **Chores**
  - Introduced scripts for registry generation, node discovery, batch fixes, validation, and production readiness checks.
  - Improved .gitignore to protect sensitive files and development artifacts.

- **Style**
  - Standardized node definition formats, naming conventions, and metadata for consistency and searchability.

- **Tests**
  - Added automated test suites for node validation, server functionality, and MCP protocol compliance.
  - Generated detailed test and validation reports confirming 100% success and production readiness.
<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,"analysisWe need to classify as performance or non-performance. The PR description includes ""Added performance optimizations: smart caching, connection pooling, real-time monitoring, and enhanced error handling."" That is performance. Also mentions ""performance optimizations"". So classification: performance.assistantfinalperformance"
2576019481,41,feat: Prepare project for Smithery deployment,SynapticSolutionsAI,169534822,closed,2025-06-07T22:26:41Z,2025-06-09T09:54:44Z,2025-06-09T09:54:44Z,https://api.github.com/repos/leonardsellem/n8n-mcp-server,https://github.com/leonardsellem/n8n-mcp-server/pull/41,"

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->

## Summary by CodeRabbit

- **Refactor**
  - Improved Docker container build process with a multi-stage approach for smaller, more efficient images.
  - Removed unnecessary Python dependencies and healthcheck configuration from the container setup.

- **Chores**
  - Added a new configuration file to define the application start command and service port.

<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""feat: Prepare project for Smithery deployment"". Body: mentions refactor: improved Docker container build process with multi-stage approach for smaller, more efficient images. Removed unnecessary Python dependencies and healthcheck configuration. Added new config file. This is about Docker build process, making images smaller, more efficient. That is performance improvement? It's about build performance, image size, maybe runtime performance. The keywords: ""smaller, more efficient images"" could be performance. But the classification is about primary purpose of PR: improve performance. This PR is about preparing project for deployment, refactoring Docker build. It improves build performance and image size. That is performance. So classify as 'performance'.assistantfinalperformance"
2615665027,90,Add pagination to SQLAlchemy relationship resolvers,simba-git,64661186,closed,2025-06-24T17:14:40Z,2025-06-24T17:41:05Z,2025-06-24T17:41:05Z,https://api.github.com/repos/featureform/enrichmcp,https://github.com/featureform/enrichmcp/pull/90,"## Summary
- autogenerate paginated relationship resolvers for SQLAlchemy list relationships
- document paginated relationships
- update changelog
- optimize list relationship pagination queries and add bounds checks
- add integration test using mcp_use against the SQLAlchemy example
- improve relationship pagination tests and standalone mcp_use test

## Testing
- `pytest tests/test_sqlalchemy_autogen.py::test_auto_resources_and_resolvers -q`
- `pytest tests/test_sqlalchemy_mcp_use.py::test_mcp_client_autogen_pagination -q`
- `pytest -q`


------
https://chatgpt.com/codex/tasks/task_e_685609a106dc832a84a1e6f55ee3d4c9",Human,"analysisWe need to classify whether the primary purpose is performance. The PR adds pagination to SQLAlchemy relationship resolvers, includes ""optimize list relationship pagination queries and add bounds checks"". That suggests performance improvement. Keywords: optimize, pagination queries, bounds checks. The title: ""Add pagination to SQLAlchemy relationship resolvers"". That is more about adding pagination, but also includes optimization. The body includes ""optimize list relationship pagination queries"". So it's performance. So classify as 'performance'.assistantfinalperformance"
2539734895,3225,feat: optimize `__tact_load_address_opt` function,pyAndr3w,43111428,closed,2025-05-23T12:32:05Z,2025-05-23T12:51:13Z,2025-05-23T12:51:13Z,https://api.github.com/repos/tact-lang/tact,https://github.com/tact-lang/tact/pull/3225,Closes #3224 ,Human,"analysisWe need to classify as performance or non-performance. The title: ""feat: optimize `__tact_load_address_opt` function"". The body: ""Closes #3224"". The title says ""optimize"". That likely refers to performance improvement. The function name suggests some optimization. So classify as 'performance'.assistantfinalperformance"
2427008542,27,Only fetch user installed apps,jkcoxson,28877351,closed,2025-03-29T16:28:39Z,2025-03-29T16:30:37Z,2025-03-29T16:30:37Z,https://api.github.com/repos/StephenDev0/StikDebug,https://github.com/StephenDev0/StikDebug/pull/27,This cuts down A LOT on load time and logging. iOS allows you to filter what apps installation proxy returns.,Human,"analysisWe need to classify the pull request as performance or non-performance. The title: ""Only fetch user installed apps"". Body: ""This cuts down A LOT on load time and logging. iOS allows you to filter what apps installation proxy returns.""

Keywords: ""load time"" indicates performance. ""cuts down A LOT on load time"" is performance improvement. So classification: performance.assistantfinalperformance"
2521500649,774,feat(ci): run cypress tests in parallel,kolaente,13721712,closed,2025-05-15T09:27:53Z,2025-05-15T09:51:26Z,2025-05-15T09:51:26Z,https://api.github.com/repos/go-vikunja/vikunja,https://github.com/go-vikunja/vikunja/pull/774,,Human,"analysisWe need to classify whether the primary purpose is to improve performance. The PR title: ""feat(ci): run cypress tests in parallel"". Running tests in parallel is a performance improvement: reduces test run time. So classification: performance.assistantfinalperformance"
2533685733,808,feat: docker layer cache,dpschen,6173598,closed,2025-05-21T08:43:16Z,2025-05-21T19:34:15Z,2025-05-21T19:34:15Z,https://api.github.com/repos/go-vikunja/vikunja,https://github.com/go-vikunja/vikunja/pull/808,Follow up from https://github.com/go-vikunja/vikunja/pull/803,Human,"analysisWe need to classify whether the primary purpose is to improve performance. The PR title: ""feat: docker layer cache"". That suggests adding Docker layer caching to speed up builds. That is a performance improvement. So classification: performance.assistantfinalperformance"
2356811134,3329,‚ôªÔ∏è Refactor: Migrate randString to rand v2,ksw2000,13825170,closed,2025-02-25T14:05:22Z,2025-02-25T16:11:46Z,2025-02-25T16:11:46Z,https://api.github.com/repos/gofiber/fiber,https://github.com/gofiber/fiber/pull/3329,"# Description

This PR migrates `randString` from `math/rand` to `math/rand/v2` to improve randomness quality and concurrency safety. In addition, it reduces execution time by approximately **97%**, as shown in the benchmark test below.

```go
func Benchmark_RandomString(b *testing.B) {
	for i := 0; i < b.N; i++ {
		_ = randString(100)
	}
}
```

```
goos: linux
goarch: amd64
pkg: github.com/gofiber/fiber/v3/client
cpu: AMD EPYC 7763 64-Core Processor                
                ‚îÇ   old.txt    ‚îÇ               new.txt               ‚îÇ
                ‚îÇ    sec/op    ‚îÇ   sec/op     vs base                ‚îÇ
_RandomString-4   9862.0n ¬± 0%   252.1n ¬± 3%  -97.44% (p=0.000 n=10)
```

## Changes introduced

List the new features or adjustments introduced in this pull request. Provide details on benchmarks, documentation updates, changelog entries, and if applicable, the migration guide.

- [x] Benchmarks: Describe any performance benchmarks and improvements related to the changes.
- [ ] Documentation Update: Detail the updates made to the documentation and links to the changed files.
- [ ] Changelog/What's New: Include a summary of the additions for the upcoming release notes.
- [ ] Migration Guide: If necessary, provide a guide or steps for users to migrate their existing code to accommodate these changes.
- [ ] API Alignment with Express: Explain how the changes align with the Express API.
- [ ] API Longevity: Discuss the steps taken to ensure that the new or updated APIs are consistent and not prone to breaking changes.
- [ ] Examples: Provide examples demonstrating the new features or changes in action.

## Type of change

- [x] Performance improvement (non-breaking change which improves efficiency)
- [x] Code consistency (non-breaking change which improves code reliability and robustness)

## Checklist

Before you submit your pull request, please make sure you meet these requirements:

- [x] Followed the inspiration of the Express.js framework for new functionalities, making them similar in usage.
- [x] Conducted a self-review of the code and provided comments for complex or critical parts.
- [ ] Updated the documentation in the `/docs/` directory for [Fiber's documentation](https://docs.gofiber.io/).
- [ ] Added or updated unit tests to validate the effectiveness of the changes or new features.
- [x] Ensured that new and existing unit tests pass locally with the changes.
- [ ] Verified that any new dependencies are essential and have been agreed upon by the maintainers/community.
- [x] Aimed for optimal performance with minimal allocations in the new code.
- [x] Provided benchmarks for the new code to analyze and improve upon.
",Human,"analysisWe need to classify as 'performance' or 'non-performance'. The PR title: ""‚ôªÔ∏è Refactor: Migrate randString to rand v2"". Body: mentions improving randomness quality and concurrency safety, reduces execution time by 97%. Contains performance keywords: ""improve randomness quality"", ""reduces execution time"", ""benchmark"", ""97%"". Also ""Performance improvement"" in checklist. So it's performance. So label 'performance'.assistantfinalperformance"
2351395795,3320,‚ôªÔ∏è Refactor: the value of map is unused in uniqueRouteStack,ksw2000,13825170,closed,2025-02-22T16:57:59Z,2025-02-24T07:12:06Z,2025-02-24T07:12:06Z,https://api.github.com/repos/gofiber/fiber,https://github.com/gofiber/fiber/pull/3320,"# Description

The value of map `m` in `uniqueRouteStack` is unused. We can set the value type of `m` to `struct{}` instead of int to save some memory and improve code clarity and readability.

## Type of change

Please delete options that are not relevant.

- [ ] New feature (non-breaking change which adds functionality)
- [ ] Enhancement (improvement to existing features and functionality)
- [ ] Documentation update (changes to documentation)
- [ ] Performance improvement (non-breaking change which improves efficiency)
- [x] Code consistency (non-breaking change which improves code reliability and robustness)

## Checklist

Before you submit your pull request, please make sure you meet these requirements:

- [x] Followed the inspiration of the Express.js framework for new functionalities, making them similar in usage.
- [x] Conducted a self-review of the code and provided comments for complex or critical parts.
- [ ] Updated the documentation in the `/docs/` directory for [Fiber's documentation](https://docs.gofiber.io/).
- [ ] Added or updated unit tests to validate the effectiveness of the changes or new features.
- [x] Ensured that new and existing unit tests pass locally with the changes.
- [ ] Verified that any new dependencies are essential and have been agreed upon by the maintainers/community.
- [ ] Aimed for optimal performance with minimal allocations in the new code.
- [ ] Provided benchmarks for the new code to analyze and improve upon.
",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""‚ôªÔ∏è Refactor: the value of map is unused in uniqueRouteStack"". Body: says value of map m in uniqueRouteStack is unused. We can set value type to struct{} instead of int to save memory and improve code clarity. The type of change: ""Performance improvement"" is checked? Actually the checkboxes: New feature, Enhancement, Documentation update, Performance improvement, Code consistency. The PR has [x] Performance improvement? Wait the list shows [x] Performance improvement? Let's read: The list:

- [ ] New feature
- [ ] Enhancement
- [ ] Documentation update
- [ ] Performance improvement
- [x] Code consistency

So the PR is marked as Code consistency, not Performance improvement. But the description says ""save some memory and improve code clarity"". That is a minor memory improvement, but not necessarily performance. The question: ""determine whether the primary purpose of the pull request is to improve performance."" The PR is about refactoring to use struct{} instead of int to save memory. That is a memory optimization, but is it performance? It might reduce memory usage, but not necessarily speed. The guidelines: performance-related PRs may contain keywords like performance, optimization, etc. This PR contains ""performance"" in the type of change? Actually the type of change is ""Performance improvement"" but it's not checked. The body doesn't mention performance. The title: ""Refactor: the value of map is unused in uniqueRouteStack"". The body: ""We can set the value type of m to struct{} instead of int to save some memory and improve code clarity and readability."" That is a memory optimization, but not necessarily performance. The guidelines: performance-related PRs may contain keywords such as respons, react, wait, responsive, throughput, slow, performance, optimization, latency, compile-time-hog, fast, memory, stuck, leak, hang, perf, minor, speed, perform, lag, latenc, tim, instant, or any other terms referring to how faster and efficiently software runs. The body contains ""memory"" but not ""performance"". The guidelines say ""memory"" is a term referring to how faster and efficiently software runs? Memory is a resource, but not necessarily performance. But memory optimization can be considered performance improvement. However, the guidelines mention ""memory"" as a keyword. So we might classify as performance. But the type of change is Code consistency. The question: ""determine whether the primary purpose of the pull request is to improve performance."" The primary purpose is to refactor to use struct{} to save memory. That is a performance improvement? It's a memory optimization, but not necessarily speed. But memory usage can affect performance. The guidelines: ""performance-related pull requests may contain keywords such as... memory"". So yes, memory is a keyword. So we should classify as performance. But the type of change is Code consistency. But the question is about classification. So we need to output 'performance' or 'non-performance'. I think it's performance. The PR is about memory optimization. So classification: performance.assistantfinalperformance"
2404112284,3365,"Revert ""üî• feat: Add Context Support to RequestID Middleware""",sixcolors,6501251,closed,2025-03-19T15:07:07Z,2025-04-08T06:59:24Z,2025-04-08T06:59:24Z,https://api.github.com/repos/gofiber/fiber,https://github.com/gofiber/fiber/pull/3365,"Reverts gofiber/fiber#3200

# **Reversion of Middleware Changes in Fiber v3**  

Following discussions in [issue #3358](https://github.com/gofiber/fiber/issues/3358), we have reverted changes that attempted to copy values into `context.Context` within various middlewares. These changes were removed to maintain Fiber‚Äôs lightweight design, improve performance, and avoid unnecessary complexity for users who do not require `context.Context`.  

## **Why Were These Changes Reverted?**  
- **Fiber prioritizes performance**, and modifying `context.Context` for every request introduces unnecessary overhead.  
- **Values are already accessible via `c.Locals()`**, making context modifications redundant.  
- **Users who need `context.Context` values can implement a trivial custom middleware** without affecting default behavior.  
",Human,"analysisWe need to classify as performance or non-performance. The PR is a revert of a feature that added context support to RequestID middleware. The description says ""revert changes that attempted to copy values into context.Context within various middlewares. These changes were removed to maintain Fiber‚Äôs lightweight design, improve performance, and avoid unnecessary complexity."" So the primary purpose is to improve performance. So classification: performance.assistantfinalperformance"
2544691147,3479,"Improve performance for ""equalFieldType"" function",ReneWerner87,7063188,closed,2025-05-26T19:04:13Z,2025-05-27T08:48:37Z,2025-05-27T08:48:37Z,https://api.github.com/repos/gofiber/fiber,https://github.com/gofiber/fiber/pull/3479,"OLD:
```
Benchmark_equalFieldType-12    	 3320424	       361.0 ns/op	      80 B/op	       9 allocs/op
```

NEW:
```
Benchmark_equalFieldType-12    	12170480	        99.85 ns/op	      16 B/op	       3 allocs/op
```
\+ solve the problem with passing on the tag name",Human,"analysisWe need to classify as 'performance' or 'non-performance'. The PR title: ""Improve performance for 'equalFieldType' function"". Body shows benchmark improvements. So it's performance. So output 'performance'.assistantfinalperformance"
2607579182,3532,Improve performance #3476,ksw2000,13825170,closed,2025-06-20T13:50:28Z,2025-06-23T06:04:56Z,2025-06-23T06:04:56Z,https://api.github.com/repos/gofiber/fiber,https://github.com/gofiber/fiber/pull/3532,"# Description

Improve the performance of #3476 by using `nextCustom` and `next` separately

## Type of change

Please delete options that are not relevant.

- [x] Enhancement (improvement to existing features and functionality)

## Checklist

Before you submit your pull request, please make sure you meet these requirements:

- [ ] Followed the inspiration of the Express.js framework for new functionalities, making them similar in usage.
- [x] Conducted a self-review of the code and provided comments for complex or critical parts.
- [ ] Updated the documentation in the `/docs/` directory for [Fiber's documentation](https://docs.gofiber.io/).
- [ ] Added or updated unit tests to validate the effectiveness of the changes or new features.
- [ x Ensured that new and existing unit tests pass locally with the changes.
- [ ] Verified that any new dependencies are essential and have been agreed upon by the maintainers/community.
- [x] Aimed for optimal performance with minimal allocations in the new code.
- [ ] Provided benchmarks for the new code to analyze and improve upon.
",Human,"analysisWe need to classify as performance or non-performance. The title: ""Improve performance #3476"". Body mentions ""Improve the performance of #3476 by using nextCustom and next separately"". Also mentions ""Aimed for optimal performance with minimal allocations"". So it's definitely performance. So label 'performance'.assistantfinalperformance"
2522491150,3457,:bug: bug: fix redirection flash messages violate cookie structure,efectn,45270788,closed,2025-05-15T15:35:41Z,2025-05-18T13:08:03Z,2025-05-18T13:08:03Z,https://api.github.com/repos/gofiber/fiber,https://github.com/gofiber/fiber/pull/3457,"# Description

This issue fixes redirection with flash messages breaks HTTP header format by encoding msgpack serialized binary data with hex. It has little overhead, but still we get benefit of using Messagepack. Here are the benchmark results:

```go
New:
Benchmark_Redirect_Route-16                         	 6792206	       173.7 ns/op	      16 B/op	       1 allocs/op
Benchmark_Redirect_Route_WithQueries-16             	 3043641	       391.3 ns/op	      40 B/op	       2 allocs/op
Benchmark_Redirect_Route_WithFlashMessages-16       	 2439327	       481.1 ns/op	     293 B/op	       3 allocs/op
Benchmark_Redirect_parseAndClearFlashMessages-16    	 2350215	       510.5 ns/op	     192 B/op	       7 allocs/op
Benchmark_Redirect_processFlashMessages-16          	 3483320	       344.6 ns/op	     288 B/op	       2 allocs/op
Benchmark_Redirect_Messages-16                      	13537572	        87.54 ns/op	     128 B/op	       2 allocs/op
Benchmark_Redirect_OldInputs-16                     	14712171	        77.44 ns/op	      96 B/op	       2 allocs/op
Benchmark_Redirect_Message-16                       	83187558	        14.43 ns/op	       0 B/op	       0 allocs/op
Benchmark_Redirect_OldInput-16                      	153820603	         7.814 ns/op	       0 B/op	       0 allocs/op

Old:
Benchmark_Redirect_Route-16                         	 6861298	       175.9 ns/op	      16 B/op	       1 allocs/op
Benchmark_Redirect_Route_WithQueries-16             	 2901906	       408.6 ns/op	      40 B/op	       2 allocs/op
Benchmark_Redirect_Route_WithFlashMessages-16       	 3456422	       328.7 ns/op	     117 B/op	       2 allocs/op
Benchmark_Redirect_parseAndClearFlashMessages-16    	 4298755	       278.1 ns/op	      32 B/op	       6 allocs/op
Benchmark_Redirect_processFlashMessages-16          	 6022623	       198.2 ns/op	     112 B/op	       1 allocs/op
Benchmark_Redirect_Messages-16                      	13548512	        87.84 ns/op	     128 B/op	       2 allocs/op
Benchmark_Redirect_OldInputs-16                     	14798696	        78.96 ns/op	      96 B/op	       2 allocs/op
Benchmark_Redirect_Message-16                       	82884747	        14.44 ns/op	       0 B/op	       0 allocs/op
Benchmark_Redirect_OldInput-16                      	148364563	         8.104 ns/op	       0 B/op	       0 allocs/op
```

Fixes https://github.com/gofiber/fiber/issues/3437

## Type of change

- [x] Enhancement (improvement to existing features and functionality)

## Checklist

Before you submit your pull request, please make sure you meet these requirements:

- [ ] Followed the inspiration of the Express.js framework for new functionalities, making them similar in usage.
- [ ] Conducted a self-review of the code and provided comments for complex or critical parts.
- [ ] Updated the documentation in the `/docs/` directory for [Fiber's documentation](https://docs.gofiber.io/).
- [ ] Added or updated unit tests to validate the effectiveness of the changes or new features.
- [ ] Ensured that new and existing unit tests pass locally with the changes.
- [ ] Verified that any new dependencies are essential and have been agreed upon by the maintainers/community.
- [ ] Aimed for optimal performance with minimal allocations in the new code.
- [ ] Provided benchmarks for the new code to analyze and improve upon.

## Commit formatting

Please use emojis in commit messages for an easy way to identify the purpose or intention of a commit. Check out the emoji cheatsheet here: [CONTRIBUTING.md](https://github.com/gofiber/fiber/blob/master/.github/CONTRIBUTING.md#pull-requests-or-commits)
",Human,"analysisWe need to classify as performance or non-performance. The PR title: "":bug: bug: fix redirection flash messages violate cookie structure"". Body includes benchmark results, mentions performance improvements. It says ""Fixes..."". It includes benchmarks comparing new vs old. It says ""Aimed for optimal performance with minimal allocations"". So it's performance improvement. So label 'performance'.assistantfinalperformance"
2519312120,2040,Change `states_to_numbers` array from Jax to NumPy in `_reorder_kronecker_product`,waleed-sh,85965501,closed,2025-05-14T13:20:06Z,2025-05-14T13:50:07Z,2025-05-14T13:50:07Z,https://api.github.com/repos/netket/netket,https://github.com/netket/netket/pull/2040,"This PR in-place changes the [`n_unsorted`](https://github.com/netket/netket/blob/c6424cc9c540ffb9e3f1380b23d2be82abf3534a/netket/operator/_local_operator/helpers.py#L198) in the [`_reorder_kronecker_product`](https://github.com/netket/netket/blob/c6424cc9c540ffb9e3f1380b23d2be82abf3534a/netket/operator/_local_operator/helpers.py#L149) from a Jax to a NumPy array. 

It is only used internally in that function, and leaving it as a Jax array causes the [`np.argsort()`](https://github.com/netket/netket/blob/c6424cc9c540ffb9e3f1380b23d2be82abf3534a/netket/operator/_local_operator/helpers.py#L203) to become comparatively slow. 

Quick benchmark of constructing an operator with a rather large number of products of LocalOperators shows runtime went from approx. 10 minutes to approx. 3 minutes after the change.",Human,"analysisWe need to classify: is this PR about performance? The description: ""Change states_to_numbers array from Jax to NumPy in _reorder_kronecker_product. It is only used internally in that function, and leaving it as a Jax array causes the np.argsort() to become comparatively slow. Quick benchmark of constructing an operator with a rather large number of products of LocalOperators shows runtime went from approx. 10 minutes to approx. 3 minutes after the change."" This is clearly a performance improvement. So label 'performance'.assistantfinalperformance"
2275434469,1978,try to fix CI memory issues,PhilipVinc,2407108,closed,2025-01-13T23:12:19Z,2025-02-03T13:32:08Z,,https://api.github.com/repos/netket/netket,https://github.com/netket/netket/pull/1978,,Human,"analysisWe need to classify the pull request as performance or non-performance. The title/body: ""try to fix CI memory issues"". This is about memory issues in CI. Memory issues are performance-related? Memory usage is a performance concern. The instruction says performance-related PRs may contain keywords like memory. So yes, it's performance. So output 'performance'.assistantfinalperformance"
2296367778,252,Handle audio samples as f32 internally,Brendonovich,14191578,closed,2025-01-24T09:41:31Z,2025-01-24T10:02:56Z,2025-01-24T10:02:56Z,https://api.github.com/repos/CapSoftware/Cap,https://github.com/CapSoftware/Cap/pull/252,"This PR fixes some audio processing issues we've been having by simplifying how we handle audio data.
Like many applications that process audio, we convert all audio to f32 samples (packed bc i've heard it's more memory efficient). Currently this data is represented as `Vec<u8>` - a Vec of f32 samples as their native endian representation.
This required extra bookkeeping to track both bytes and samples elapsed.
This PR changes `AudioData` to store a `Vec<f32>` directly, so we only need to consider elapsed samples.
In places that require a `Vec<u8>` (eg audio playback), we do a cast from `Vec<f32>` to `Vec<u8>` to prevent copying all the samples. This is safe since everywhere that expects `Vec<u8>` uses native endianness.",Human,"analysisWe need to classify as performance or non-performance. The PR description: ""Handle audio samples as f32 internally"" and ""simplifying how we handle audio data"" ""convert all audio to f32 samples"" ""currently data represented as Vec<u8> - a Vec of f32 samples as their native endian representation"" ""This required extra bookkeeping to track both bytes and samples elapsed."" ""This PR changes AudioData to store a Vec<f32> directly, so we only need to consider elapsed samples."" ""In places that require a Vec<u8> (eg audio playback), we do a cast from Vec<f32> to Vec<u8> to prevent copying all the samples. This is safe since everywhere that expects Vec<u8> uses native endianness.""

This is about simplifying data representation, reducing bookkeeping, and avoiding copying. That likely improves performance. The keywords: ""performance"" not explicitly but ""prevent copying"" ""simplifying"" ""only need to consider elapsed samples"" ""memory efficient"" ""f32"" ""Vec<f32>"" ""Vec<u8>"" ""cast"" ""prevent copying"" ""memory efficient"" ""performance"" maybe. The classification: performance. So output ""performance"".assistantfinalperformance"
2401612063,371,transfer acceleration,Brendonovich,14191578,closed,2025-03-18T17:28:49Z,2025-03-18T17:29:10Z,2025-03-18T17:29:10Z,https://api.github.com/repos/CapSoftware/Cap,https://github.com/CapSoftware/Cap/pull/371,,Human,"analysisWe have a pull request title and body combined: ""transfer acceleration"". We need to decide if primary purpose is to improve performance. The title ""transfer acceleration"" suggests speeding up transfer, likely performance. There's no body provided. The title alone indicates performance improvement. So classify as 'performance'.assistantfinalperformance"
2583077255,631,initialise camera in background to account for hanging,Brendonovich,14191578,closed,2025-06-11T05:19:32Z,2025-06-11T05:24:33Z,2025-06-11T05:24:33Z,https://api.github.com/repos/CapSoftware/Cap,https://github.com/CapSoftware/Cap/pull/631,,Human,"analysisWe need to classify the pull request as performance or non-performance. The title and body: ""initialise camera in background to account for hanging"". The phrase ""hanging"" indicates a hang, which is a performance issue. Also ""initialise camera in background"" to avoid hanging. So it's performance. So output 'performance'.assistantfinalperformance"
2576212936,10689,cache a venv to avoid pip usage,geohot,72895,closed,2025-06-08T02:19:16Z,2025-06-08T03:13:41Z,2025-06-08T03:13:41Z,https://api.github.com/repos/tinygrad/tinygrad,https://github.com/tinygrad/tinygrad/pull/10689,,Human,"analysisWe need to classify the pull request. Title: ""cache a venv to avoid pip usage"". Body likely about caching virtual environment to avoid pip usage. This is about performance? It might be about speed, reducing pip usage, caching environment. That is a performance improvement: avoid pip usage speeds up installation. So classify as 'performance'.assistantfinalperformance"
2324987642,8994,realized tensors become RESHAPE(BUFFER),Qazalin,77887910,closed,2025-02-10T03:13:24Z,2025-02-10T09:17:54Z,2025-02-10T09:17:54Z,https://api.github.com/repos/tinygrad/tinygrad,https://github.com/tinygrad/tinygrad/pull/8994,,Human,"analysisWe need to classify the pull request as performance or non-performance. The title and body: ""realized tensors become RESHAPE(BUFFER)"". This seems like a change in implementation: realized tensors become reshape buffer. That might be about memory or performance? The phrase ""RESHAPE(BUFFER)"" suggests maybe a change to avoid copying data, maybe performance improvement. But the description is minimal. The instruction: performance-related PR may contain keywords like performance, optimization, etc. The title doesn't contain those. But the content might be about memory or performance. However, the instruction says ""Given a text combining the title and body of a pull request, determine whether the primary purpose of the pull request is to improve performance."" The text is just ""realized tensors become RESHAPE(BUFFER)"". That seems like a change to the implementation of tensors, maybe to avoid copying. That could be a performance improvement. But is it explicitly about performance? The title doesn't mention performance. But the content might be about memory usage. The instruction says performance-related PR may contain keywords such as... or any other terms referring to how faster and efficiently software runs. The phrase ""RESHAPE(BUFFER)"" might be a performance improvement. But it's ambiguous. The safe classification might be 'non-performance' because no explicit performance keyword. But the content might be about memory. The instruction says ""or any other terms referring to how faster and efficiently software runs."" ""RESHAPE(BUFFER)"" is not obviously a performance term. But it might be a memory optimization. But the instruction says ""performance-related pull requests may contain keywords such as... or any other terms referring to how faster and efficiently software runs."" So if the PR is about memory usage, that might be performance. But the title is minimal. I'd lean 'performance' because it's about changing the representation of tensors to avoid copying, which is a performance improvement. But the instruction says ""primary purpose of the pull request is to improve performance."" The title ""realized tensors become RESHAPE(BUFFER)"" suggests a change to the implementation of realized tensors. That likely is a performance improvement. So classify as 'performance'.assistantfinalperformance"
2435585913,1258,Add tensorwide FP8 cuda quant kernel,EricLBuehler,65165915,open,2025-04-03T01:25:11Z,,,https://api.github.com/repos/EricLBuehler/mistral.rs,https://github.com/EricLBuehler/mistral.rs/pull/1258,,Human,"analysisWe need to classify whether the primary purpose is performance improvement. The PR title: ""Add tensorwide FP8 cuda quant kernel"". This seems to add a new kernel for FP8 quantization. Likely performance improvement? It might be adding a new kernel to improve performance of quantization. But is it performance? The description: ""Add tensorwide FP8 cuda quant kernel"". That suggests adding a new kernel for FP8 quantization, likely to improve performance of quantization. The keywords: ""performance"" not present. But the content: ""Add tensorwide FP8 cuda quant kernel"" - it's adding a new kernel, likely for performance. But the classification: performance or non-performance. The guidelines: performance-related PRs may contain keywords such as respons, react, wait, responsive, throughput, slow, performance, optimization, latency, compile-time-hog, fast, memory, stuck, leak, hang, perf, minor, speed, perform, lag, latenc, tim, instant, or any other terms referring to how faster and efficiently software runs. The title doesn't contain those. But the content might. But we only have title. The title indicates adding a new kernel for FP8 quantization. That is likely a performance improvement. But the guidelines: we need to decide. The presence of ""kernel"" might be performance. But the guidelines mention ""performance"" keywords. The title doesn't contain them. But the content might. But we don't have content. The title alone: ""Add tensorwide FP8 cuda quant kernel"". That is likely a performance improvement. But is it a new feature? It might be a new kernel for quantization, which could be a new feature. But the guidelines: performance-related PRs may contain keywords. But the guidelines also say ""or any other terms referring to how faster and efficiently software runs."" The title refers to adding a kernel, but not explicitly performance. But it's likely performance. But we need to decide. The guidelines: ""Performance-related pull requests may contain keywords such as... or any other terms referring to how faster and efficiently software runs."" The title doesn't mention those. But the content might. But we only have title. The title is ambiguous. But likely it's a performance improvement. But we need to decide. The safe classification: performance. Because adding a new kernel for FP8 quantization is likely to improve performance. So classification: performance.assistantfinalperformance"
2545078467,1379,Faster UQFF serialization,EricLBuehler,65165915,closed,2025-05-27T02:02:14Z,2025-05-27T02:02:56Z,2025-05-27T02:02:56Z,https://api.github.com/repos/EricLBuehler/mistral.rs,https://github.com/EricLBuehler/mistral.rs/pull/1379,"

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->

## Summary by CodeRabbit

- **New Features**
  - Improved serialization of raw byte buffers, enabling direct and efficient export as tensors without unnecessary data cloning.
  - Additional residual tensors (`q_norm` and `k_norm`) are now included in exported model data for enhanced completeness.

- **Performance Improvements**
  - Optimized serialization process for quantized layers, reducing memory usage and improving efficiency.

<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,"analysisWe need to classify as 'performance' or 'non-performance'. The PR title: ""Faster UQFF serialization"". Body mentions performance improvements: ""Optimized serialization process for quantized layers, reducing memory usage and improving efficiency."" So it's performance. So output 'performance'.assistantfinalperformance"
2431203212,1256,Implement blockwise scales for cuBLASlt matmul,EricLBuehler,65165915,open,2025-04-01T10:33:11Z,,,https://api.github.com/repos/EricLBuehler/mistral.rs,https://github.com/EricLBuehler/mistral.rs/pull/1256,,Human,"analysisWe need to classify whether the primary purpose of the pull request is to improve performance. The title: ""Implement blockwise scales for cuBLASlt matmul"". Body not provided. The title suggests adding blockwise scales for cuBLASlt matmul. That likely is a performance improvement? Blockwise scaling might be for performance optimization. But we need to decide. The instruction: performance-related PR may contain keywords like performance, optimization, etc. The title doesn't contain those words. But the content might. We don't have body. We need to guess. ""Implement blockwise scales for cuBLASlt matmul"" likely is a performance improvement to cuBLASlt matmul. It might be a new feature, but likely performance. The guidelines: if the primary purpose is to improve performance, classify as performance. The title suggests a new feature for blockwise scaling, which might be for performance. I'd lean performance. So output ""performance"".assistantfinalperformance"
2563903991,1416,Gumbel sampling for fast sampler,EricLBuehler,65165915,closed,2025-06-03T13:07:20Z,2025-06-03T13:17:50Z,2025-06-03T13:17:50Z,https://api.github.com/repos/EricLBuehler/mistral.rs,https://github.com/EricLBuehler/mistral.rs/pull/1416,"

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->

## Summary by CodeRabbit

- **New Features**
  - Introduced a new stochastic sampling method for token selection, resulting in more varied and natural outputs.
- **Performance Improvements**
  - Added caching for random noise generation to enhance sampling efficiency.

<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Gumbel sampling for fast sampler"". Body: mentions new stochastic sampling method, more varied outputs, and performance improvements: added caching for random noise generation to enhance sampling efficiency. So it's performance-related. So label 'performance'.assistantfinalperformance"
2565581414,1421,Propely handle consecutive searches,EricLBuehler,65165915,closed,2025-06-04T00:16:13Z,2025-06-04T00:19:07Z,2025-06-04T00:19:07Z,https://api.github.com/repos/EricLBuehler/mistral.rs,https://github.com/EricLBuehler/mistral.rs/pull/1421,"

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->

## Summary by CodeRabbit

- **Refactor**
  - Improved the handling of web search and extraction tool calls, resulting in smoother and more efficient multi-step search interactions.
  - Enhanced consistency between streaming and non-streaming responses, ensuring user-visible results are delivered only after all necessary tool actions are completed.

<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Propely handle consecutive searches"". Body: mentions refactor, improved handling of web search and extraction tool calls, smoother and more efficient multi-step search interactions, enhanced consistency between streaming and non-streaming responses, ensuring user-visible results delivered only after all necessary tool actions are completed.

Keywords: ""smoother"", ""more efficient"", ""improved handling"", ""efficient multi-step search interactions"". This is about performance? It says smoother and more efficient multi-step search interactions. That suggests performance improvement. The guidelines: performance-related PR may contain keywords such as respons, react, wait, responsive, throughput, slow, performance, optimization, latency, compile-time-hog, fast, memory, stuck, leak, hang, perf, minor, speed, perform, lag, latenc, tim, instant, or any other terms referring to how faster and efficiently software runs.

Here we have ""more efficient"" and ""smoother"". That indicates performance improvement. So classify as 'performance'.assistantfinalperformance"
2555512787,1391,Fewer .contiguous calls for qwen3 moe,EricLBuehler,65165915,closed,2025-05-30T19:26:10Z,2025-05-30T19:32:45Z,2025-05-30T19:32:45Z,https://api.github.com/repos/EricLBuehler/mistral.rs,https://github.com/EricLBuehler/mistral.rs/pull/1391,"

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->

## Summary by CodeRabbit

- **Refactor**
  - Improved performance and efficiency of certain operations, resulting in smoother and faster processing for end-users. No visible changes to the user interface.

<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,"analysisWe need to classify as performance or non-performance. The title: ""Fewer.contiguous calls for qwen3 moe"". Body: summary says ""Refactor - Improved performance and efficiency of certain operations, resulting in smoother and faster processing for end-users. No visible changes to the user interface."" That indicates performance improvement. So classification: performance.assistantfinalperformance"
2456276880,1277,Inplace ISQ support and default to mmap,EricLBuehler,65165915,closed,2025-04-13T22:01:22Z,2025-05-13T01:37:16Z,2025-05-13T01:37:16Z,https://api.github.com/repos/EricLBuehler/mistral.rs,https://github.com/EricLBuehler/mistral.rs/pull/1277,"ISQ update to support models that don't fit in system RAM, instead quantizing them as they are loaded.

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->
## Summary by CodeRabbit

- **New Features**
  - Introduced a global, thread-safe mechanism for setting and retrieving immediate in-situ quantization (ISQ) configuration.
  - Added support for applying ISQ to model layers immediately upon creation when configured.
  - Added predicate-based filtering for sharded tensor access to control accessible tensors more granularly.
  - Added measurement and reporting of ""time to first token"" (TTFT) in interactive text and vision modes.
  - Extended model loaders with new immediate ISQ predicate methods for enhanced quantization targeting.

- **Bug Fixes**
  - Improved handling of buffer offsets in quantization and dequantization operations for correct data alignment.
  - Corrected regex patterns for self-attention output projections in multiple model loaders.

- **Refactor**
  - Simplified and clarified ISQ flag logic and quantization setup during model loading.
  - Enhanced tensor loading logic to explicitly handle cases with and without LoRA paths.
  - Updated layer constructors to conditionally apply immediate ISQ after creation, improving quantization consistency.
  - Parallelized model layer loading across numerous architectures using Rayon, significantly improving initialization performance.
  - Integrated progress bars with parallel iteration for better feedback during concurrent operations.
  - Improved error handling in model layer construction by replacing panics with proper error propagation in several models.
  - Moved logging utilities for one-time info/warn messages to a shared quantization utility module.
<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,"analysisWe need to classify as performance or non-performance. The PR is about ""Inplace ISQ support and default to mmap"". It includes new features, bug fixes, refactor. It mentions ""Parallelized model layer loading across numerous architectures using Rayon, significantly improving initialization performance."" Also ""Added measurement and reporting of 'time to first token' (TTFT)"". Also ""Improved handling of buffer offsets"". The primary purpose? It seems to improve performance by parallelizing loading, measuring TTFT, etc. The title: ""Inplace ISQ support and default to mmap"". That is about quantization and memory mapping. The body includes performance improvements. So classification: performance.assistantfinalperformance"
2602952332,1482,Fix phi3.5 moe (#1447),EricLBuehler,65165915,closed,2025-06-18T21:32:49Z,2025-06-19T08:43:51Z,2025-06-19T08:43:51Z,https://api.github.com/repos/EricLBuehler/mistral.rs,https://github.com/EricLBuehler/mistral.rs/pull/1482,"Fixes #1447, @sempervictus (WIP).

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->
## Summary by CodeRabbit

- **Bug Fixes**
  - Resolved an issue where certain operations were incorrectly performed on the CPU instead of the original device, improving performance and compatibility for users running models on non-CPU devices.
<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Fix phi3.5 moe (#1447)"". Body: mentions bug fix: ""Resolved an issue where certain operations were incorrectly performed on the CPU instead of the original device, improving performance and compatibility for users running models on non-CPU devices."" The body explicitly says improving performance. So it's performance. The classification: 'performance'.assistantfinalperformance"
2272646019,1059,Paged attention support for multi gpu,EricLBuehler,65165915,closed,2025-01-12T18:44:01Z,2025-01-16T11:40:31Z,2025-01-16T11:40:31Z,https://api.github.com/repos/EricLBuehler/mistral.rs,https://github.com/EricLBuehler/mistral.rs/pull/1059,"- Maximum PA. GPU allocation is now equivalent of the model max. seq length (should this not be included for the case of multiple requests?)
- Handle different GPUs with different memory availability and therefore a lower max kv cache size for the model
- Better measure memory on Metal devices",Human,"analysisWe need to classify whether the primary purpose is performance improvement. The PR title: ""Paged attention support for multi gpu"". Body: mentions GPU allocation, memory, kv cache size, better measure memory on Metal devices. This seems about memory usage and multi-GPU support, likely performance improvement. Keywords: memory, performance? The description is about memory allocation, handling GPUs with different memory availability, better measure memory. That is performance optimization. So classify as 'performance'.assistantfinalperformance"
2395675294,245,Parallel tests,scosman,848343,closed,2025-03-15T19:59:18Z,2025-03-15T20:01:39Z,,https://api.github.com/repos/Kiln-AI/Kiln,https://github.com/Kiln-AI/Kiln/pull/245,"Bring back parallel tests, for speed",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Parallel tests"" and body: ""Bring back parallel tests, for speed"". This is clearly about improving performance by enabling parallel tests. So classification: performance.assistantfinalperformance"
2615702170,264,patch/propagate extract parallelism,karakanb,16530606,closed,2025-06-24T17:30:28Z,2025-06-24T17:43:15Z,2025-06-24T17:43:15Z,https://api.github.com/repos/bruin-data/ingestr,https://github.com/bruin-data/ingestr/pull/264,"- **propagate extract parallelism**
- **remove redundant stripe sources**
",Human,"analysisWe need to classify whether the primary purpose is to improve performance. The PR title: ""patch/propagate extract parallelism"" and body: ""- propagate extract parallelism - remove redundant stripe sources"". This seems to be about parallelism and removing redundant stripe sources. That likely improves performance by enabling parallelism and removing redundant work. So it's performance. There's no explicit performance keyword but the content indicates parallelism. The instruction says performance-related PRs may contain keywords such as... or any other terms referring to how faster and efficiently software runs. ""parallelism"" is a performance improvement. So classify as 'performance'.assistantfinalperformance"
2578057970,383,Grida Canvas - skia-safe rust backend - standalone performance testing,softmarshmallow,16307013,closed,2025-06-09T10:44:49Z,2025-06-21T06:27:24Z,2025-06-21T06:27:23Z,https://api.github.com/repos/gridaco/grida,https://github.com/gridaco/grida/pull/383,"- https://github.com/gridaco/grida/pull/377
- https://github.com/gridaco/grida/pull/382
- https://github.com/gridaco/grida/pull/384
- https://github.com/gridaco/grida/pull/388

---

Related:
- https://github.com/gridaco/grida/issues/354
- https://github.com/gridaco/grida/issues/378

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->

## Summary by CodeRabbit

- **New Features**
  - Introduced a high-performance, real-time 2D rendering engine with comprehensive scene graph support, geometry caching, and advanced rendering optimizations.
  - Added extensive support for vector graphics, shapes, text, gradients, images, and effects such as blur and shadows.
  - Implemented asynchronous image and font loading with caching, and support for web fonts.
  - Enabled Figma and JSON file import for scene creation.
  - Provided interactive demo applications and benchmarking examples for rendering performance and resource management.
  - Added a modular math utilities library for geometry, layout, color, and rasterization.

- **Documentation**
  - Added detailed README files, architectural overviews, and optimization strategy documents for both engine and math libraries.

- **Tests**
  - Included comprehensive unit and integration tests for geometry, rendering, resource loading, hit testing, and performance benchmarks.

- **Chores**
  - Added configuration files for Rust workspaces, Docker, Makefile automation, and package management.

<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,"analysisWe need to classify as performance or non-performance. The title: ""Grida Canvas - skia-safe rust backend - standalone performance testing"". That indicates performance testing. Body mentions high-performance, real-time 2D rendering engine, optimization, benchmarking examples for rendering performance. So it's performance. So label 'performance'.assistantfinalperformance"
2540272739,351,[Grida Canvas] Rendering Performance,softmarshmallow,16307013,closed,2025-05-23T15:56:07Z,2025-05-26T10:08:53Z,2025-05-26T10:08:53Z,https://api.github.com/repos/gridaco/grida,https://github.com/gridaco/grida/pull/351,"React Renderer Performance Improvements.

- introduces a new Editor instance, with `use-sync-external-store` react bridge



**Pt.1**

https://github.com/user-attachments/assets/ec047e3a-2717-4c79-8708-e3187f13ac5a



<!-- This is an auto-generated comment: release notes by coderabbit.ai -->
## Summary by CodeRabbit

- **New Features**
  - Introduced a new editor core with improved state management, gesture handling, and document hierarchy querying.
  - Added React hooks for editor integration, enabling efficient state updates and modular component usage.
  - Enhanced canvas and surface interaction, including support for advanced gestures and tool modes.

- **Refactor**
  - Migrated and reorganized editor modules, consolidating and updating import paths for improved maintainability.
  - Replaced previous reducer-based state management with a hook-based approach for editor state.
  - Updated and streamlined document and node property management APIs.

- **Bug Fixes**
  - Improved re-render performance in canvas components by refining effect dependencies.
  - Fixed issues related to transformation logic and state synchronization across editor components.

- **Chores**
  - Updated dependencies to include state synchronization utilities.
  - Removed deprecated and redundant code, including old reducers, state definitions, and transformation utilities.

- **Tests**
  - Added comprehensive test suites for transformation and delta logic to ensure mathematical correctness.

- **Documentation**
  - Updated type annotations and exported interfaces for clarity and stronger type safety.

<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""[Grida Canvas] Rendering Performance"". Body mentions ""React Renderer Performance Improvements."" Also mentions improved re-render performance, etc. So it's performance. So label 'performance'.assistantfinalperformance"
2497441108,319,[Grida Library] Speed Optimization & Infinite Scrolling,softmarshmallow,16307013,closed,2025-05-04T18:34:30Z,2025-05-04T18:39:19Z,2025-05-04T18:39:19Z,https://api.github.com/repos/gridaco/grida,https://github.com/gridaco/grida/pull/319,"

https://github.com/user-attachments/assets/0520b9a6-b8d5-4296-9a3f-1ee0183091d7


<!-- This is an auto-generated comment: release notes by coderabbit.ai -->

## Summary by CodeRabbit

- **New Features**
  - Introduced infinite scrolling in galleries, allowing more items to load automatically as you scroll.
  - Added a loading spinner and improved loading state visuals during data fetches.
  - Introduced a new ""Similar"" section on object pages, displaying related items with dynamic loading.

- **Improvements**
  - Enhanced gallery UI with better item deduplication and updated styling for image cards and overlays.
  - Refined metadata generation for object pages to use more descriptive fallbacks.
  - Improved category and color data accuracy in the library.

- **Bug Fixes**
  - Fixed tooltip interactions in galleries for better usability.

- **Other**
  - Image outputs are now saved as JPEGs with optimized quality instead of PNGs.

<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,"analysisWe need to classify as performance or non-performance. The title: ""Speed Optimization & Infinite Scrolling"". The body mentions infinite scrolling, loading spinner, improved loading state visuals, new ""Similar"" section, improved gallery UI, deduplication, metadata generation, bug fixes, image outputs saved as JPEGs with optimized quality. The title explicitly says ""Speed Optimization"". So it's performance. So classification: performance.assistantfinalperformance"
2319826569,736,feat: improve llm calls list performances,hughcrt,24587702,closed,2025-02-06T12:41:28Z,2025-02-06T13:28:46Z,2025-02-06T13:28:46Z,https://api.github.com/repos/lunary-ai/lunary,https://github.com/lunary-ai/lunary/pull/736,,Human,"analysisWe need to classify as performance or non-performance. The title: ""feat: improve llm calls list performances"". The phrase ""improve llm calls list performances"" suggests improving performance of LLM calls list. Likely performance. So classify 'performance'.assistantfinalperformance"
2352318434,1823,Reducing testing time of car/tests to be about 0.3 seconds (link to #1184),tanyamalik2000,29354143,closed,2025-02-23T20:39:28Z,2025-03-02T04:07:52Z,,https://api.github.com/repos/commaai/opendbc,https://github.com/commaai/opendbc/pull/1823,"I was able to reduce the testing time of car/tests to about 0.3 seconds just by removing the parameterized import in all the files and replacing it with for loops. This links to #1184. 

I can check to see if I can reduce it further to less than 0.1 seconds according to the bounty, but this pull request can either be used as a checkpoint (or completion if the reduction is not possible).",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Reducing testing time of car/tests to be about 0.3 seconds"". Body: ""I was able to reduce the testing time of car/tests to about 0.3 seconds just by removing the parameterized import in all the files and replacing it with for loops. This links to #1184. I can check to see if I can reduce it further to less than 0.1 seconds according to the bounty, but this pull request can either be used as a checkpoint (or completion if the reduction is not possible).""

Clearly performance improvement. So label 'performance'.assistantfinalperformance"
2575937922,18,Implement SDL caching in CI,joamag,25441,closed,2025-06-07T21:09:37Z,2025-06-07T21:57:23Z,2025-06-07T21:57:23Z,https://api.github.com/repos/joamag/boytacean,https://github.com/joamag/boytacean/pull/18,"## Summary
- cache SDL build outputs using actions/cache in `main.yml`
- revert caching changes from `deploy.yml` and `extra.yml`

## Testing
- `cargo test`

------
https://chatgpt.com/codex/tasks/task_e_68449fd1c2d883289dcb500367ad3cc4

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->
## Summary by CodeRabbit

- **Chores**
  - Improved build process by adding caching for SDL build dependencies, resulting in faster and more efficient builds.
<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,"analysisWe need to classify as performance or non-performance. The PR implements SDL caching in CI, adding caching to speed up builds. That is performance improvement. So label 'performance'.assistantfinalperformance"
2542615571,10,Refactor PPU for performance improvements,joamag,25441,open,2025-05-25T19:55:21Z,,,https://api.github.com/repos/joamag/boytacean,https://github.com/joamag/boytacean/pull/10,"This commit introduces two main performance enhancements to the PPU:

1.  **Eager DMG Frame Buffer Calculation:** The `Ppu::frame_buffer()` method previously used lazy evaluation for DMG mode, calculating the entire frame buffer from the shade_buffer on the first request. This commit changes `render_map_dmg` to populate `self.frame_buffer` directly during scanline rendering, similar to CGB mode. This distributes the computation cost and makes `Ppu::frame_buffer()` a consistently fast operation for both modes.

2.  **Optimize `fill_frame_buffer` with Pattern Copy:** The `Ppu::fill_frame_buffer` method, used for clearing the screen or filling it with a specific color, has been optimized. For the main loop that populates `self.frame_buffer` with a uniform color, the implementation now uses `std::ptr::copy_nonoverlapping` with a pre-filled repeating RGB pattern when the `simd` feature is enabled. This leverages potential compiler optimizations for bulk memory copies, which can be significantly faster than scalar iteration for this task. The original scalar loop is retained if the `simd` feature is not active.

These changes aim to reduce potential stutter in DMG mode and accelerate screen fill operations, contributing to overall emulator performance.

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->
## Summary by CodeRabbit

- **Performance Improvements**
  - Enhanced frame buffer filling with SIMD optimization for faster rendering when supported.
- **Bug Fixes**
  - Improved accuracy of color rendering in DMG mode by updating the frame buffer immediately with the correct palette colors.
<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,"analysisWe need to classify as 'performance' or 'non-performance'. The PR title: ""Refactor PPU for performance improvements"". Body describes performance enhancements. So classification: performance.assistantfinalperformance"
2316356365,988,WIP JLBH perf test with integrated async profiler call,benbonavia,174972019,open,2025-02-04T23:55:26Z,,,https://api.github.com/repos/OpenHFT/Chronicle-Wire,https://github.com/OpenHFT/Chronicle-Wire/pull/988,WIP - this is a trail at adding a profiler to run with the JLBH perf tests,Human,"analysisWe need to classify as performance or non-performance. The title: ""WIP JLBH perf test with integrated async profiler call"". Body: ""WIP - this is a trail at adding a profiler to run with the JLBH perf tests"". This is about adding a profiler to run with JLBH perf tests. It's about performance testing. So classification: performance.assistantfinalperformance"
2277950711,985,Optimisation around append sep [WIP],tgd,109181,open,2025-01-14T23:16:52Z,,,https://api.github.com/repos/OpenHFT/Chronicle-Wire,https://github.com/OpenHFT/Chronicle-Wire/pull/985,,Human,"analysisWe need to classify as performance or non-performance. The title: ""Optimisation around append sep [WIP]"". The body is not provided. The title indicates optimization. The keyword ""Optimisation"" is a performance-related term. So classify as 'performance'.assistantfinalperformance"
2495510561,2278,shu/remove foreign key constraints on large table actions,wintonzheng,5225702,closed,2025-05-02T11:45:59Z,2025-05-02T11:51:24Z,2025-05-02T11:51:24Z,https://api.github.com/repos/Skyvern-AI/skyvern,https://github.com/Skyvern-AI/skyvern/pull/2278,"- drop foreign key constrains in actions - organization_id, workflow_run_id
- drop most foreign keys

<!-- ELLIPSIS_HIDDEN -->


----

> [!IMPORTANT]
> Remove foreign key constraints from multiple tables to simplify schema and improve performance.
> 
>   - **Database Migrations**:
>     - Drop foreign key constraints in `actions`, `artifacts`, `workflow_runs`, and other tables in `2025_05_02_1148-e8285b6ddcf0_drop_most_foreign_keys.py`.
>   - **Model Changes**:
>     - Remove `ForeignKey` from `task_id` and `step_id` in `ArtifactModel`.
>     - Remove `ForeignKey` from `workflow_id`, `parent_workflow_run_id`, and `organization_id` in `WorkflowRunModel`.
>     - Remove `ForeignKey` from `workflow_id` in `WorkflowParameterModel`, `OutputParameterModel`, `AWSSecretParameterModel`, `BitwardenLoginCredentialParameterModel`, `BitwardenSensitiveInformationParameterModel`, `BitwardenCreditCardDataParameterModel`, and `CredentialParameterModel`.
>     - Remove `ForeignKey` from `workflow_run_id` and `workflow_parameter_id` in `WorkflowRunParameterModel`.
>     - Remove `ForeignKey` from `workflow_run_id` and `output_parameter_id` in `WorkflowRunOutputParameterModel`.
>     - Remove `ForeignKey` from `organization_id` in `TaskGenerationModel`, `PersistentBrowserSessionModel`, `TaskV2Model`, `ThoughtModel`, and `ActionModel`.
>     - Remove `ForeignKey` from `workflow_run_id`, `block_workflow_run_id`, `parent_workflow_run_block_id`, `organization_id`, and `task_id` in `WorkflowRunBlockModel`.
> 
> <sup>This description was created by </sup>[<img alt=""Ellipsis"" src=""https://img.shields.io/badge/Ellipsis-blue?color=175173"">](https://www.ellipsis.dev?ref=Skyvern-AI%2Fskyvern&utm_source=github&utm_medium=referral)<sup> for 0f688292acc360d802f70ea0e34493f2d056e485. You can [customize](https://app.ellipsis.dev/Skyvern-AI/settings/summaries) this summary. It will automatically update as commits are pushed.</sup>


<!-- ELLIPSIS_HIDDEN -->",Human,"analysisWe need to classify as performance or non-performance. The PR removes foreign key constraints on large tables to simplify schema and improve performance. The description explicitly says ""Remove foreign key constraints from multiple tables to simplify schema and improve performance."" So it's performance. So label 'performance'.assistantfinalperformance"
2297770994,26842,added optional mask to cv::threshold,chacha21,7251617,closed,2025-01-24T23:12:34Z,2025-03-12T14:55:07Z,2025-03-12T14:55:07Z,https://api.github.com/repos/opencv/opencv,https://github.com/opencv/opencv/pull/26842,"Proposal for #26777

To avoid code duplication, and keep performance when no mask is used, inner implementation always propagate the const cv::Mat& mask, but they use a template<bool useMask> parameter that let the compiler optimize out unecessary tests when the mask is not to be used.

See details at https://github.com/opencv/opencv/wiki/How_to_contribute#making-a-good-pull-request

- [x] I agree to contribute to the project under Apache 2 License.
- [x] To the best of my knowledge, the proposed patch is not based on a code under GPL or another license that is incompatible with OpenCV
- [X] The PR is proposed to the proper branch
- [X] There is a reference to the original bug report and related work
- [X] There is accuracy test, performance test and test data in opencv_extra repository, if applicable
      Patch to opencv_extra has the same branch name.
- [ ] The feature is well documented and sample code can be built with the project CMake
",Human,"analysisWe need to classify as performance or non-performance. The PR adds optional mask to cv::threshold, with template<bool useMask> to let compiler optimize out unnecessary tests when mask not used. It mentions performance test. The primary purpose is to improve performance by avoiding code duplication and optimizing out tests. So it's performance.assistantfinalperformance"
2290575196,6320,Add option to disable variable copying in Jint engine,sfmskywalker,938393,closed,2025-01-21T18:11:50Z,2025-01-22T09:24:58Z,2025-01-22T09:24:58Z,https://api.github.com/repos/elsa-workflows/elsa-core,https://github.com/elsa-workflows/elsa-core/pull/6320,"Introduce a `DisableVariableCopying` option to improve performance by preventing workflow variables from being copied into the Jint engine or back into the workflow context. Updated related logic to honor this setting and ensure compatibility with existing behavior.

<!-- Reviewable:start -->
- - -
This change is‚ÄÇ[<img src=""https://reviewable.io/review_button.svg"" height=""34"" align=""absmiddle"" alt=""Reviewable""/>](https://reviewable.io/reviews/elsa-workflows/elsa-core/6320)
<!-- Reviewable:end -->
",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Add option to disable variable copying in Jint engine"". Body: ""Introduce a `DisableVariableCopying` option to improve performance by preventing workflow variables from being copied into the Jint engine or back into the workflow context. Updated related logic to honor this setting and ensure compatibility with existing behavior.""

Keywords: ""improve performance"", ""prevent... copying"", ""performance"". So it's performance. So label 'performance'.assistantfinalperformance"
2427144857,3,Enhanced AppleScript robustness and response handling for improved reliability,htlin222,61491860,closed,2025-03-29T18:22:11Z,2025-05-20T09:00:33Z,2025-05-20T09:00:32Z,https://api.github.com/repos/syedazharmbnr1/claude-chatgpt-mcp,https://github.com/syedazharmbnr1/claude-chatgpt-mcp/pull/3,"Dear @syedazharmbnr1 

I hope this message finds you well! üôå I've been using your excellent ChatGPT MCP tool and wanted to contribute some enhancements that might benefit all users.

## üöÄ Improvements

### üìù Enhanced AppleScript Robustness
- Improved text encoding with better handling of special characters
- Added clipboard-based input to handle complex prompts reliably
- Enhanced UI element targeting to handle ChatGPT interface changes

### ‚è±Ô∏è Response Handling Optimizations
- Replaced fixed waiting times with dynamic response detection
- Implemented text stability detection (waits until text stops changing)
- Added intelligent completion detection for ChatGPT responses
- Improved error handling with detailed error messages

### üìö Documentation
- Added a detailed ""Optimizations"" section to the README explaining the enhancements

These changes make the integration more reliable across different scenarios, more resilient to UI changes in the ChatGPT application, and better at handling longer response times without message cutoff issues.

Thank you for creating this valuable tool! I hope these enhancements might be useful for the project. üôè

Best regards,
Hsieh-Ting Lin",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Enhanced AppleScript robustness and response handling for improved reliability"". Body: mentions ""Response Handling Optimizations"", ""Replaced fixed waiting times with dynamic response detection"", ""Implemented text stability detection"", ""Added intelligent completion detection"", ""Improved error handling"". These are performance improvements? They mention dynamic response detection, waiting times, etc. It's about responsiveness, reliability, but also performance? The keywords: ""performance"" not present. But ""Response Handling Optimizations"" and ""dynamic response detection"" could be considered performance. The instruction: performance-related PR may contain keywords such as respons, react, wait, responsive, throughput, slow, performance, optimization, latency, compile-time-hog, fast, memory, stuck, leak, hang, perf, minor, speed, perform, lag, latenc, tim, instant, or any other terms referring to how faster and efficiently software runs. The body contains ""Response Handling Optimizations"", ""dynamic response detection"", ""waits until text stops changing"", ""intelligent completion detection"". The word ""wait"" appears. ""Optimizations"" appears. ""Response Handling Optimizations"" indicates performance. So classify as 'performance'.assistantfinalperformance"
2575890016,870,feat: add --download flag for faster Flutter SDK installation,leoafarias,435833,open,2025-06-07T20:23:47Z,,,https://api.github.com/repos/leoafarias/fvm,https://github.com/leoafarias/fvm/pull/870,"## üöÄ Feature: Download Flag for Faster Flutter SDK Installation

Adds a new `--download` flag to the `fvm install` command that downloads pre-built Flutter SDK archives instead of Git cloning, providing faster installation for official releases.

## ‚ú® What's New

### Command Usage
```bash
# Download pre-built Flutter 3.24.0 archive
fvm install 3.24.0 --download

# Short form
fvm install 3.24.0 -d

# Falls back to Git for unsupported versions
fvm install custom_version --download  # Uses Git cloning
fvm install myfork/stable --download    # Uses Git cloning
```

### Key Features
- ‚úÖ **Faster Installation**: Pre-built archives vs Git clone + build
- ‚úÖ **Reduced Bandwidth**: Smaller archive files vs full Git repositories
- ‚úÖ **Smart Fallback**: Automatically uses Git when download isn't available
- ‚úÖ **Official Releases Only**: Supports stable, beta, dev channels
- ‚úÖ **Backward Compatible**: Existing workflows unchanged
- ‚úÖ **Cross-Platform**: Works on Linux, macOS, Windows

## üîß Implementation Details

### Files Added
- `lib/src/services/download_service.dart` - Handles archive downloads and extraction
- `test/services/download_service_test.dart` - Comprehensive test suite

### Files Modified
- `lib/src/commands/install_command.dart` - Added --download flag
- `lib/src/workflows/ensure_cache.workflow.dart` - Integrated download workflow
- `lib/src/utils/context.dart` - Registered DownloadService
- `pubspec.yaml` - Added archive package dependency
- `test/commands/install_command_test.dart` - Added flag parsing tests

### Architecture
1. **DownloadService**: Handles tar.xz archive downloads and extraction
2. **EnsureCacheWorkflow**: Enhanced to support download mode with Git fallback
3. **InstallCommand**: Added --download flag integration
4. **Smart Detection**: Only official releases can be downloaded

## üß™ Testing

### Test Coverage
- ‚úÖ Unit tests for DownloadService functionality
- ‚úÖ Integration tests with real Flutter releases API
- ‚úÖ Command-line flag parsing tests
- ‚úÖ Error scenarios and fallback behavior
- ‚úÖ Archive structure validation

### Quality Checks
- ‚úÖ `dart analyze` - No issues
- ‚úÖ `dart fix --apply` - Applied fixes
- ‚úÖ All tests passing (11/11)
- ‚úÖ Successful compilation
- ‚úÖ **Updated with latest main branch**

## üéØ Benefits

1. **Performance**: Significantly faster installation for official releases
2. **Reliability**: Less dependent on Git infrastructure
3. **Bandwidth**: Reduced download size
4. **User Experience**: Faster developer onboarding
5. **Robustness**: Automatic fallback ensures compatibility

## üîÑ Backward Compatibility

- Default behavior unchanged - existing scripts continue to work
- Git cloning remains the default installation method
- Download flag is opt-in only
- All existing command options preserved

## üìã Code Quality

- Follows DRY, KISS, YAGNI principles
- Reuses existing services (FlutterReleaseClient, CacheService)
- Clean error handling with meaningful messages
- Comprehensive test coverage
- Zero static analysis issues

## üîÑ Recent Updates

**Latest Merge (a2d89c6)**: 
- ‚úÖ Merged with latest main branch
- ‚úÖ Resolved conflicts in test files
- ‚úÖ Maintained both download flag tests and new project config tests
- ‚úÖ All tests passing after merge
- ‚úÖ Zero code quality issues

## üîç Review Notes

- Implementation focuses on official Flutter releases only
- Smart fallback ensures no breaking changes
- Uses existing FVM infrastructure and patterns
- Minimal code footprint with maximum benefit
- **Branch is up-to-date with main**

Closes #[issue-number] (if applicable)",Human,"analysisWe need to classify as 'performance' or 'non-performance'. The PR adds a --download flag for faster Flutter SDK installation. It explicitly mentions faster installation, performance improvement. So classification: performance.assistantfinalperformance"
2544065380,153,Add Docker support with multi-stage build and nginx configuration,pcbimon,8252967,closed,2025-05-26T13:15:03Z,2025-05-27T11:58:15Z,2025-05-27T11:58:15Z,https://api.github.com/repos/lyqht/mini-qr,https://github.com/lyqht/mini-qr/pull/153,"This pull request enhances Docker support for the Mini QR project, following best practices commonly used in open source GitHub repositories.

Changes

- Refactored the [Dockerfile] to use a multi-stage build (Node.js for building, Nginx for serving static files), as recommended for production-ready images.

- Added a [.dockerignore] file to reduce build context and speed up Docker builds.

- Created a production [nginx.conf] for optimized static file serving.

- Updated [docker-compose.yml] to support both prebuilt images and local builds, with commented options for custom configuration.

- Improved the Docker usage section in [README.md] with clear, step-by-step instructions for running, building, and customizing the app using Docker and Docker Compose.",Human,"analysisWe need to classify whether the primary purpose is performance. The PR adds Docker support with multi-stage build,.dockerignore, nginx config, docker-compose, README. The focus is on Docker best practices, multi-stage build, reducing build context, optimized static file serving. This is about performance? Multi-stage build reduces image size,.dockerignore reduces build context, nginx config for optimized static file serving. These are performance improvements. The description says ""following best practices commonly used in open source GitHub repositories."" The changes: multi-stage build,.dockerignore to reduce build context and speed up Docker builds, production nginx config for optimized static file serving. So yes, it's performance. The classification: 'performance'.assistantfinalperformance"
2402397520,1599,Add data_type info to ast::IndexItem::PrimaryKey,panarch,2025065,open,2025-03-19T01:27:24Z,,,https://api.github.com/repos/gluesql/gluesql,https://github.com/gluesql/gluesql/pull/1599,"- [ ] Currently, the implementation calls fetch_schema multiple times, leading to inefficiencies. This should be improved through refactoring.

ref. https://github.com/gluesql/gluesql/pull/1582",Human,"analysisWe need to classify whether the primary purpose is to improve performance. The PR title: ""Add data_type info to ast::IndexItem::PrimaryKey"". Body: ""Currently, the implementation calls fetch_schema multiple times, leading to inefficiencies. This should be improved through refactoring."" So it's about reducing inefficiencies, likely performance improvement. The body explicitly mentions inefficiencies. So classification: performance.assistantfinalperformance"
2404325040,153,Improved Vendor Table,claudfuen,26839654,closed,2025-03-19T16:13:04Z,2025-03-19T16:17:16Z,2025-03-19T16:17:16Z,https://api.github.com/repos/trycompai/comp,https://github.com/trycompai/comp/pull/153,"

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->

## Summary by CodeRabbit

- **New Features**
  - Expanded vendor management with enhanced risk assessment, categorization, and status tracking.
  - Improved task handling with additional details for assignments, comments, and file attachments.

- **Chores**
  - Updated and optimized backend database structure for improved data consistency and performance.

<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Improved Vendor Table"". Body: mentions new features, chores, updated and optimized backend database structure for improved data consistency and performance. The keyword ""performance"" appears. So it's performance. The primary purpose is to improve performance. So label 'performance'.assistantfinalperformance"
2504407177,4511,[stdlib] Small improvements to `BitSet`,soraros,6129707,closed,2025-05-07T13:29:01Z,2025-05-13T06:11:33Z,,https://api.github.com/repos/modular/modular,https://github.com/modular/modular/pull/4511,"CC @abduld.

* Replaced `InlineArray.__getitem__` with the unchecked `unsafe_get` to eliminate bounds checks. This avoids significant IR bloat‚Äîup to 20√ó for cases like `BitSet(mask)._words[0]`.
* Removed an unnecessary `String` allocation to skip a `malloc` and reduce memory overhead.",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""[stdlib] Small improvements to `BitSet`"". Body: Replaced InlineArray.__getitem__ with unchecked unsafe_get to eliminate bounds checks, avoids IR bloat up to 20√ó. Removed unnecessary String allocation to skip malloc and reduce memory overhead.

This is clearly performance improvements: eliminating bounds checks, reducing IR bloat, memory overhead. So classification: performance.assistantfinalperformance"
2538980518,450,fix: filter page in large data,islxyqwe,15280968,closed,2025-05-23T06:52:32Z,2025-05-23T13:18:37Z,2025-05-23T13:18:37Z,https://api.github.com/repos/Kanaries/graphic-walker,https://github.com/Kanaries/graphic-walker/pull/450,"fix a problem when with large data(10m+ row), the filter page will crash when trying to create a 10m+ array.",Human,"analysisWe need to classify whether the primary purpose is performance improvement. The PR title: ""fix: filter page in large data"". Body: ""fix a problem when with large data(10m+ row), the filter page will crash when trying to create a 10m+ array."" This is a bug fix for large data causing crash. It's about memory usage, but is it performance? The description: ""filter page will crash when trying to create a 10m+ array."" That is a memory issue, not necessarily performance. But the keywords: ""large data"", ""10m+ array"" indicates memory usage. The classification: performance vs non-performance. The guidelines: performance-related PRs may contain keywords like memory, leak, etc. The guidelines mention memory as a keyword? They mention memory. Yes, memory is listed. So memory is a performance-related keyword. But is this a performance improvement? It's a bug fix to avoid crash due to memory usage. It's not necessarily improving performance, but addressing memory usage. The guidelines say performance-related PRs may contain keywords such as memory. So this could be considered performance. But the primary purpose is to fix a crash, not to improve performance. However, memory usage is a performance aspect. The guidelines: ""Performance-related pull requests may contain keywords such as... memory..."". So memory is a keyword. So we should classify as performance. But is it a performance improvement? The PR is about preventing crash due to large array. That is more of a bug fix. But the guidelines say memory is a performance keyword. So we should classify as performance. The question: ""determine whether the primary purpose of the pull request is to improve performance."" The primary purpose is to fix a crash. But the guidelines say memory is a performance keyword. But the question: ""Performance-related pull requests may contain keywords such as... memory..."". So memory is a performance keyword. But the PR is about memory usage. So likely performance. So classification: performance.assistantfinalperformance"
2542698947,452,fix: filter page in large data,islxyqwe,15280968,closed,2025-05-25T23:17:10Z,2025-05-26T02:25:02Z,,https://api.github.com/repos/Kanaries/graphic-walker,https://github.com/Kanaries/graphic-walker/pull/452,"fix a problem when with large data(10m+ row), the filter page will crash when trying to create a 10m+ array.
also added size controll of full component, and a reporter when the editor is edited„ÄÇ
BREAK CHANGE: the component will not have a fixed height anymore, and you should set the component's height by youself, or it will grow as the field num increases.",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""fix: filter page in large data"". Body: ""fix a problem when with large data(10m+ row), the filter page will crash when trying to create a 10m+ array. also added size controll of full component, and a reporter when the editor is edited. BREAK CHANGE: the component will not have a fixed height anymore, and you should set the component's height by youself, or it will grow as the field num increases.""

This is about handling large data, preventing crash when creating large array. It's about performance? It's about memory usage and crash due to large array. It's a performance fix: large data causing crash, memory usage. The keywords: large data, crash, array. It's about performance. So classify as 'performance'.assistantfinalperformance"
2282210055,3046,feature(rf optimizations): enabling oneDPL and sort primitive refactoring,Alexandr-Solovev,43129356,closed,2025-01-16T15:58:06Z,2025-03-21T22:07:14Z,2025-03-21T22:07:14Z,https://api.github.com/repos/uxlfoundation/oneDAL,https://github.com/uxlfoundation/oneDAL/pull/3046,"### Description:
**RF optimizations: enabling oneDPL and sort primitive refactoring and several functions optimization**

#### Summary:
This PR introduces oneDPL enabling and radix sort replacement. Also the engine_type support has been added for RF GPU. A lot of CPU functions have been replaced with GPU analogues.

**PR completeness and readability**

- [x] I have reviewed my changes thoroughly before submitting this pull request.
- [x] I have commented my code, particularly in hard-to-understand areas.
- [x] I have updated the documentation to reflect the changes or created a separate PR with update and provided its number in the description, if necessary.
- [x] Git commit message contains an appropriate signed-off-by string _(see [CONTRIBUTING.md](https://github.com/uxlfoundation/scikit-learn-intelex/blob/main/CONTRIBUTING.md#pull-requests) for details)_.
- [x] I have added a respective label(s) to PR if I have a permission for that.
- [x] I have resolved any merge conflicts that might occur with the base branch.

**Testing**

- [x] I have run it locally and tested the changes extensively.
- [x] All CI jobs are green or I have provided justification why they aren't.
- [x] I have extended testing suite if new functionality was introduced in this PR.

**Performance**

- [x] I have measured performance for affected algorithms using [scikit-learn_bench](https://github.com/IntelPython/scikit-learn_bench) and provided at least summary table with measured data, if performance change is expected.
- [x] I have provided justification why performance has changed or why changes are not expected.
- [x] I have provided justification why quality metrics have changed or why changes are not expected.
- [x] I have extended benchmarking suite and provided corresponding scikit-learn_bench PR if new measurable functionality was introduced in this PR.
",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""feature(rf optimizations): enabling oneDPL and sort primitive refactoring"". The description mentions enabling oneDPL and radix sort replacement, engine_type support for RF GPU, CPU functions replaced with GPU analogues. Also mentions performance section: measured performance, provided summary table. So it's definitely performance-related. So classification: performance.assistantfinalperformance"
2524180167,112,‚ö°Ô∏è Speed up method `ObjectComparer.__init__` by 51%,codeflash-ai[bot],148906541,closed,2025-05-16T10:20:41Z,2025-05-16T10:27:00Z,2025-05-16T10:27:00Z,https://api.github.com/repos/mangiucugna/json_repair,https://github.com/mangiucugna/json_repair/pull/112,"### üìÑ 51% (0.51x) speedup for ***`ObjectComparer.__init__` in `src/json_repair/object_comparer.py`***

‚è±Ô∏è Runtime :   **`125 nanoseconds`**  **‚Üí** **`83.0 nanoseconds`** (best of `187` runs)
### üìù Explanation and details

Here is a more optimized version of your program.



**Optimizations made:**
- Added `__slots__ = ()` to prevent the overhead of the instance `__dict__` since the class does not define any instance attributes.
- Changed `return` to `pass` since `return` is unnecessary in `__init__` without a value.


‚úÖ **Correctness verification report:**

| Test                        | Status            |
| --------------------------- | ----------------- |
| ‚öôÔ∏è Existing Unit Tests | üîò **None Found** |
| üåÄ Generated Regression Tests | ‚úÖ **3 Passed** |
| ‚è™ Replay Tests | üîò **None Found** |
| üîé Concolic Coverage Tests | ‚úÖ **3 Passed** |
|üìä Tests Coverage       | 100.0% |
<details>
<summary>üåÄ Generated Regression Tests Details</summary>

```python
import pytest  # used for our unit tests
from src.json_repair.object_comparer import ObjectComparer

# unit tests

# 1. Basic Test Cases



















import pytest
from src.json_repair.object_comparer import ObjectComparer

# unit tests

# -------------------------------
# Basic Test Cases
# -------------------------------

def test_init_no_arguments():
    """"""Test __init__ with no arguments.""""""
    obj = ObjectComparer()











def test_init_with_invalid_items_type():
    """"""Test __init__ with invalid items type (should raise TypeError).""""""
    with pytest.raises(TypeError):
        ObjectComparer(items=""not a list"")

def test_init_with_invalid_strict_type():
    """"""Test __init__ with invalid strict type (should raise TypeError).""""""
    with pytest.raises(TypeError):
        ObjectComparer(strict=""yes"")









from src.json_repair.object_comparer import ObjectComparer

def test_ObjectComparer___init__():
    ObjectComparer.__init__(ObjectComparer())
```

</details>


To edit these changes `git checkout codeflash/optimize-ObjectComparer.__init__-maqndxva` and push.

[![Codeflash](https://img.shields.io/badge/Optimized%20with-Codeflash-yellow?style=flat&color=%23ffc428&logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNDgwIiBoZWlnaHQ9ImF1dG8iIHZpZXdCb3g9IjAgMCA0ODAgMjgwIiBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgo8cGF0aCBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTI4Ni43IDAuMzc4NDE4SDIwMS43NTFMNTAuOTAxIDE0OC45MTFIMTM1Ljg1MUwwLjk2MDkzOCAyODEuOTk5SDk1LjQzNTJMMjgyLjMyNCA4OS45NjE2SDE5Ni4zNDVMMjg2LjcgMC4zNzg0MThaIiBmaWxsPSIjRkZDMDQzIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMzExLjYwNyAwLjM3ODkwNkwyNTguNTc4IDU0Ljk1MjZIMzc5LjU2N0w0MzIuMzM5IDAuMzc4OTA2SDMxMS42MDdaIiBmaWxsPSIjMEIwQTBBIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMzA5LjU0NyA4OS45NjAxTDI1Ni41MTggMTQ0LjI3NkgzNzcuNTA2TDQzMC4wMjEgODkuNzAyNkgzMDkuNTQ3Vjg5Ljk2MDFaIiBmaWxsPSIjMEIwQTBBIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMjQyLjg3MyAxNjQuNjZMMTg5Ljg0NCAyMTkuMjM0SDMxMC44MzNMMzYzLjM0NyAxNjQuNjZIMjQyLjg3M1oiIGZpbGw9IiMwQjBBMEEiLz4KPC9zdmc+Cg==)](https://codeflash.ai)",Human,"analysisWe need to classify the pull request as 'performance' or 'non-performance'. The title: ""‚ö°Ô∏è Speed up method `ObjectComparer.__init__` by 51%"". The body describes performance optimization. So it's performance. So output 'performance'.assistantfinalperformance"
2524300649,114,‚ö°Ô∏è Speed up method `JSONParser.parse_number` by 17%,codeflash-ai[bot],148906541,closed,2025-05-16T11:24:40Z,2025-05-16T11:35:05Z,,https://api.github.com/repos/mangiucugna/json_repair,https://github.com/mangiucugna/json_repair/pull/114,"### üìÑ 17% (0.17x) speedup for ***`JSONParser.parse_number` in `src/json_repair/json_parser.py`***

‚è±Ô∏è Runtime :   **`7.25 microseconds`**  **‚Üí** **`6.21 microseconds`** (best of `27` runs)
### üìù Explanation and details

Here is an optimized version of your program, with a focus on reducing runtime and memory usage in the `parse_number` and `get_char_at` functions, as suggested by the profile data.  
Key improvements.
- **Avoid unnecessary set creation** inside hot loops.
- **Minimize attribute access** in tight loops (local variable caching for `self.get_char_at` rather than repeated method call).
- **Reduce string concatenation** inside loops by collecting characters into a list and joining once at the end.

All comments are preserved except those adjacent to changed lines, which are updated if relevant.



**Key optimization notes:**
- `parse_number` now uses a `number_chars` list to gather characters, avoiding ""string + char"" concatenation which is O(n¬≤) in Python.  
- `get_char_at` is bound to a local variable to avoid repeated attribute/method lookup inside the loop.
- Direct string `""0123456789-.eE/,""` is used for membership check instead of recreating a set each call, as the set is tiny and `in`-string checks are fast for small sets.

All changes preserve existing functionality and logging behavior. The rest of the code remains unmodified (other than whitespace for style).  

You can further optimize if profiling shows parse_string as another hot spot,  
but the major bottleneck per your profile was only in parse_number and get_char_at.


‚úÖ **Correctness verification report:**

| Test                        | Status            |
| --------------------------- | ----------------- |
| ‚öôÔ∏è Existing Unit Tests | üîò **None Found** |
| üåÄ Generated Regression Tests | ‚úÖ **97 Passed** |
| ‚è™ Replay Tests | üîò **None Found** |
| üîé Concolic Coverage Tests | ‚úÖ **4 Passed** |
|üìä Tests Coverage       | 81.8% |
<details>
<summary>üåÄ Generated Regression Tests Details</summary>

```python
from typing import Any, ClassVar, Literal

# imports
import pytest  # used for our unit tests
from src.json_repair.json_parser import JSONParser


# Minimal stubs for ContextValues and JsonContext to allow testing
class ContextValues:
    ARRAY = ""ARRAY""
    OBJECT_KEY = ""OBJECT_KEY""
    OBJECT_VALUE = ""OBJECT_VALUE""

# unit tests

@pytest.mark.parametrize(
    ""input_str,expected,desc"",
    [
        # Basic integer
        (""123"", 123, ""Simple integer""),
        (""0"", 0, ""Zero integer""),
        (""-42"", -42, ""Negative integer""),
        # Basic float
        (""3.14"", 3.14, ""Simple float""),
        (""-0.001"", -0.001, ""Negative float""),
        (""0.0"", 0.0, ""Zero float""),
        # Basic scientific notation
        (""1e3"", 1000.0, ""Scientific notation positive exponent""),
        (""-2.5E-2"", -0.025, ""Scientific notation negative exponent""),
        (""6.02e23"", 6.02e23, ""Large scientific notation""),
        # Number with trailing non-numeric
        (""42abc"", ""42"", ""Number followed by alpha (should fallback to string)""),
        (""3.14pie"", ""3.14"", ""Float followed by alpha (should fallback to string)""),
        (""-123.45xyz"", ""-123.45"", ""Negative float followed by alpha""),
        # Number with comma (should return as string)
        (""1,234"", ""1,234"", ""Number with comma""),
        (""12,345.67"", ""12,345.67"", ""Float with comma""),
        # Number ending with invalid char
        (""789-"", 789, ""Number ending with - (should ignore)""),
        (""5.3e/"", 5.3, ""Number ending with / (should ignore)""),
        (""10E,"", 10, ""Number ending with , (should ignore)""),
        # Edge: Only sign
        (""-"", """", ""Just a minus sign""),
        # Edge: Only decimal point
        (""."", """", ""Just a dot""),
        # Edge: Only exponent
        (""e"", """", ""Just an exponent""),
        # Edge: Empty string
        ("""", """", ""Empty input""),
        # Edge: Multiple dots
        (""1.2.3"", 1.2, ""Multiple dots, should parse up to second dot""),
        # Edge: Multiple exponents
        (""1e2e3"", 100.0, ""Multiple exponents, should parse up to second e""),
        # Edge: Leading zeros
        (""000123"", 123, ""Leading zeros""),
        (""000.456"", 0.456, ""Leading zeros in float""),
        # Edge: Negative zero
        (""-0"", 0, ""Negative zero""),
        # Edge: Large integer
        (""999999999"", 999999999, ""Large integer""),
        # Edge: Large negative integer
        (""-999999999"", -999999999, ""Large negative integer""),
        # Edge: Large float
        (""3.141592653589793238"", 3.141592653589793, ""Very long float (Python float precision)""),
        # Edge: Array context, should stop at comma
        (""123,456"", 123, ""Array context, stops at comma""),
        # Edge: Array context, with negative number
        (""-789,123"", -789, ""Array context, negative number stops at comma""),
        # Edge: Array context, float
        (""1.23,4.56"", 1.23, ""Array context, float stops at comma""),
        # Edge: Number with leading plus (should fail to parse as number)
        (""+123"", """", ""Leading plus is not handled, should return empty string""),
        # Edge: Number with embedded whitespace
        (""12 34"", 12, ""Whitespace breaks number parsing""),
        # Edge: Number with tab
        (""56\t78"", 56, ""Tab breaks number parsing""),
        # Edge: Negative float with exponent
        (""-1.23e-4"", -1.23e-4, ""Negative float with exponent""),
        # Edge: Float with positive exponent
        (""2.5E+3"", 2500.0, ""Float with explicit positive exponent""),
        # Edge: Float with exponent and trailing chars
        (""7.89e2abc"", ""7.89e2"", ""Float with exponent and trailing alpha""),
        # Edge: Number with multiple commas
        (""1,234,567"", ""1,234,567"", ""Number with multiple commas""),
        # Edge: Number with trailing whitespace
        (""123 "", 123, ""Number with trailing space""),
        # Edge: Number with leading whitespace
        ("" 456"", """", ""Leading whitespace not handled, should return empty string""),
        # Edge: Negative sign only
        (""-"", """", ""Just a negative sign""),
        # Edge: Dot only
        (""."", """", ""Just a dot""),
        # Edge: Exponent only
        (""e"", """", ""Just an e""),
        # Edge: Negative exponent only
        (""-e"", """", ""Negative sign and e""),
        # Edge: Float with no leading digit
        ("".5"", 0.5, ""Float with no leading digit""),
        # Edge: Float with no trailing digit
        (""5."", 5.0, ""Float with no trailing digit""),
        # Edge: Number with slash (should ignore slash)
        (""123/456"", 123, ""Slash breaks number parsing""),
        # Edge: Number with multiple slashes
        (""12/34/56"", 12, ""Multiple slashes break parsing""),
        # Edge: Number with embedded dash
        (""12-34"", 12, ""Dash in the middle breaks parsing""),
        # Edge: Number with multiple exponents (invalid)
        (""1e2e3"", 100.0, ""Multiple exponents, only first is parsed""),
        # Edge: Number with trailing dot and comma
        (""5.,"", 5.0, ""Number ending with dot and comma""),
        # Edge: Number with trailing dot and slash
        (""5./"", 5.0, ""Number ending with dot and slash""),
        # Edge: Number with trailing e and comma
        (""5e,"", 5, ""Number ending with e and comma""),
        # Edge: Number with trailing e and slash
        (""5e/"", 5, ""Number ending with e and slash""),
        # Edge: Number with trailing dash and comma
        (""5-,"", 5, ""Number ending with dash and comma""),
        # Edge: Number with trailing dash and slash
        (""5-/"", 5, ""Number ending with dash and slash""),
        # Edge: Number with comma at start (should return empty string)
        ("",123"", """", ""Comma at start, should return empty string""),
        # Edge: Number with dot at start (should return empty string)
        ("".123"", 0.123, ""Dot at start, float with no leading digit""),
        # Edge: Number with only comma
        ("","", """", ""Only comma""),
        # Edge: Number with only dash
        (""-"", """", ""Only dash""),
        # Edge: Number with only slash
        (""/"", """", ""Only slash""),
    ]
)
def test_parse_number_basic_and_edge(input_str, expected, desc):
    """"""
    Test basic and edge cases for parse_number.
    """"""
    parser = JSONParser(input_str)
    codeflash_output = parser.parse_number(); result = codeflash_output













from typing import Any, ClassVar, Literal

# imports
import pytest
from src.json_repair.json_parser import JSONParser


# Dummy ContextValues and JsonContext for test purposes
class ContextValues:
    OBJECT_KEY = ""OBJECT_KEY""
    OBJECT_VALUE = ""OBJECT_VALUE""
    ARRAY = ""ARRAY""

# unit tests

# -------------------------
# 1. Basic Test Cases
# -------------------------

@pytest.mark.parametrize(
    ""input_str,expected"",
    [
        # Integer
        (""123"", 123),
        (""0"", 0),
        (""-42"", -42),
        # Float
        (""3.14"", 3.14),
        (""-0.001"", -0.001),
        (""2e3"", 2000.0),
        (""-2E-2"", -0.02),
        # Leading zeros (should parse as int)
        (""007"", 7),
        # Trailing whitespace (should ignore)
        (""123 "", 123),
        # Number with comma (should return as string)
        (""1,234"", ""1,234""),
        # Number with trailing comma (should ignore comma)
        (""123,"", 123),
        # Number with trailing slash (should ignore slash)
        (""123/"", 123),
        # Number with trailing e (should ignore e)
        (""123e"", 123),
        # Number with trailing - (should ignore -)
        (""123-"", 123),
        # Negative float with exponent
        (""-1.23e-10"", -1.23e-10),
        # Float with positive exponent
        (""1.23E+10"", 1.23e10),
    ]
)
def test_parse_number_basic(input_str, expected):
    parser = JSONParser(input_str)
    codeflash_output = parser.parse_number(); result = codeflash_output
    if isinstance(expected, float):
        pass
    else:
        pass

# -------------------------
# 2. Edge Test Cases
# -------------------------

@pytest.mark.parametrize(
    ""input_str,expected"",
    [
        # Empty string
        ("""", """"),
        # Only minus sign
        (""-"", """"),
        # Only dot
        (""."", """"),
        # Only exponent
        (""e"", """"),
        # Only comma
        ("","", """"),
        # Only slash
        (""/"", """"),
        # Multiple dots (invalid float)
        (""1.2.3"", ""1.2.3""),
        # Multiple exponents (invalid float)
        (""1e2e3"", ""1e2e3""),
        # Number followed by alpha (should call parse_string, so returns as string)
        (""123abc"", ""123abc""),
        # Number with comma in array context (should stop at comma)
        (""123,456"", 123),
        # Negative number with trailing comma
        (""-42,"", -42),
        # Negative float with trailing comma
        (""-42.5,"", -42.5),
        # Number with embedded slash (invalid, returns as string)
        (""12/34"", ""12/34""),
        # Number with embedded comma (invalid, returns as string)
        (""12,34"", ""12,34""),
        # Large negative exponent
        (""1e-308"", 1e-308),
        # Large positive exponent
        (""1e308"", 1e308),
        # Number with leading whitespace (should parse correctly)
        (""   42"", 42),
        # Number with trailing whitespace and comma
        (""42 ,"", 42),
        # Number with leading/trailing whitespace
        (""  42  "", 42),
    ]
)
def test_parse_number_edge(input_str, expected):
    parser = JSONParser(input_str.strip())
    codeflash_output = parser.parse_number(); result = codeflash_output
    if isinstance(expected, float):
        pass
    else:
        pass














from src.json_repair.json_parser import JSONParser

def test_JSONParser_parse_number():
    JSONParser.parse_number(JSONParser('e', None, False, json_fd_chunk_length=0, stream_stable=True))

def test_JSONParser_parse_number_2():
    JSONParser.parse_number(JSONParser('53', None, None, json_fd_chunk_length=0, stream_stable=False))
```

</details>


To edit these changes `git checkout codeflash/optimize-JSONParser.parse_number-maqpo82d` and push.

[![Codeflash](https://img.shields.io/badge/Optimized%20with-Codeflash-yellow?style=flat&color=%23ffc428&logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNDgwIiBoZWlnaHQ9ImF1dG8iIHZpZXdCb3g9IjAgMCA0ODAgMjgwIiBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgo8cGF0aCBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTI4Ni43IDAuMzc4NDE4SDIwMS43NTFMNTAuOTAxIDE0OC45MTFIMTM1Ljg1MUwwLjk2MDkzOCAyODEuOTk5SDk1LjQzNTJMMjgyLjMyNCA4OS45NjE2SDE5Ni4zNDVMMjg2LjcgMC4zNzg0MThaIiBmaWxsPSIjRkZDMDQzIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMzExLjYwNyAwLjM3ODkwNkwyNTguNTc4IDU0Ljk1MjZIMzc5LjU2N0w0MzIuMzM5IDAuMzc4OTA2SDMxMS42MDdaIiBmaWxsPSIjMEIwQTBBIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMzA5LjU0NyA4OS45NjAxTDI1Ni41MTggMTQ0LjI3NkgzNzcuNTA2TDQzMC4wMjEgODkuNzAyNkgzMDkuNTQ3Vjg5Ljk2MDFaIiBmaWxsPSIjMEIwQTBBIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMjQyLjg3MyAxNjQuNjZMMTg5Ljg0NCAyMTkuMjM0SDMxMC44MzNMMzYzLjM0NyAxNjQuNjZIMjQyLjg3M1oiIGZpbGw9IiMwQjBBMEEiLz4KPC9zdmc+Cg==)](https://codeflash.ai)",Human,"analysisWe need to classify whether the primary purpose is to improve performance. The title: ""‚ö°Ô∏è Speed up method `JSONParser.parse_number` by 17%"". The body describes performance improvements, speedup, runtime, optimization notes. So it's definitely performance. So output 'performance'.assistantfinalperformance"
2524313861,115,‚ö°Ô∏è Speed up method `JSONParser.parse_comment` by 29%,codeflash-ai[bot],148906541,closed,2025-05-16T11:31:16Z,2025-05-16T11:35:59Z,,https://api.github.com/repos/mangiucugna/json_repair,https://github.com/mangiucugna/json_repair/pull/115,"### üìÑ 29% (0.29x) speedup for ***`JSONParser.parse_comment` in `src/json_repair/json_parser.py`***

‚è±Ô∏è Runtime :   **`9.00 microseconds`**  **‚Üí** **`7.00 microseconds`** (best of `47` runs)
### üìù Explanation and details

Here's an optimized version of your `JSONParser` program, targeting hot-spots in `parse_comment` and reducing unnecessary work and function calls. The `get_char_at()` calls are now minimized inside tight loops for comment scanning, and the logic around accumulating `termination_characters` is more efficient. All semantics are preserved.



### Optimization Notes

- **Loop minimization**: Instead of repeatedly calling `get_char_at()` for each character in a comment (inefficient for long comments), tight loops work directly on the underlying string, updating the index variable.
- **String slicing**: Builds the comment with one slice instead of repeated string concatenation.
- **Logging unchanged**: Any logging/side-effects remain at exactly the same logical points.
- **No unnecessary dictionary lookups**: The context object is checked only once per function call.
- **Branch ordering**: Strips some redundancy and code path splits for improved readability and performance.
- **No change to function signatures or observable input/output**.

If you have further line profiling data, deeper optimization can be tailored!


‚úÖ **Correctness verification report:**

| Test                        | Status            |
| --------------------------- | ----------------- |
| ‚öôÔ∏è Existing Unit Tests | üîò **None Found** |
| üåÄ Generated Regression Tests | üîò **None Found** |
| ‚è™ Replay Tests | üîò **None Found** |
| üîé Concolic Coverage Tests | ‚úÖ **10 Passed** |
|üìä Tests Coverage       | 93.0% |

To edit these changes `git checkout codeflash/optimize-JSONParser.parse_comment-maqpwq0a` and push.

[![Codeflash](https://img.shields.io/badge/Optimized%20with-Codeflash-yellow?style=flat&color=%23ffc428&logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNDgwIiBoZWlnaHQ9ImF1dG8iIHZpZXdCb3g9IjAgMCA0ODAgMjgwIiBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgo8cGF0aCBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTI4Ni43IDAuMzc4NDE4SDIwMS43NTFMNTAuOTAxIDE0OC45MTFIMTM1Ljg1MUwwLjk2MDkzOCAyODEuOTk5SDk1LjQzNTJMMjgyLjMyNCA4OS45NjE2SDE5Ni4zNDVMMjg2LjcgMC4zNzg0MThaIiBmaWxsPSIjRkZDMDQzIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMzExLjYwNyAwLjM3ODkwNkwyNTguNTc4IDU0Ljk1MjZIMzc5LjU2N0w0MzIuMzM5IDAuMzc4OTA2SDMxMS42MDdaIiBmaWxsPSIjMEIwQTBBIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMzA5LjU0NyA4OS45NjAxTDI1Ni41MTggMTQ0LjI3NkgzNzcuNTA2TDQzMC4wMjEgODkuNzAyNkgzMDkuNTQ3Vjg5Ljk2MDFaIiBmaWxsPSIjMEIwQTBBIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMjQyLjg3MyAxNjQuNjZMMTg5Ljg0NCAyMTkuMjM0SDMxMC44MzNMMzYzLjM0NyAxNjQuNjZIMjQyLjg3M1oiIGZpbGw9IiMwQjBBMEEiLz4KPC9zdmc+Cg==)](https://codeflash.ai)",Human,"analysisWe need to classify as 'performance' or 'non-performance'. The PR title: ""‚ö°Ô∏è Speed up method `JSONParser.parse_comment` by 29%"". The body describes performance optimization. So it's performance. So output 'performance'.assistantfinalperformance"
2617731143,4807,ci: publish to open vsx,benfdking,9087625,closed,2025-06-25T10:19:44Z,2025-06-26T17:50:09Z,2025-06-26T17:50:09Z,https://api.github.com/repos/TobikoData/sqlmesh,https://github.com/TobikoData/sqlmesh/pull/4807,"- also removed redundant steps that slowed the job down

* [x] requires secret to be added
",Human,"analysisWe need to classify the pull request as performance or non-performance. The title: ""ci: publish to open vsx"". Body: ""- also removed redundant steps that slowed the job down"". The mention of ""slowed the job down"" indicates performance improvement. The PR is about CI job speed. So it's performance. So output ""performance"".assistantfinalperformance"
2522826360,4427,Feat: more cores for loading,themisvaltinos,73662635,closed,2025-05-15T18:15:00Z,2025-05-26T18:14:58Z,2025-05-26T18:14:58Z,https://api.github.com/repos/TobikoData/sqlmesh,https://github.com/TobikoData/sqlmesh/pull/4427,"This update adds a `ProcessPoolExecutor` for parallel loading of a project's models. It also adds a mock executor for single-process scenarios, such as when the system doesn‚Äôt support fork. Also, it refactors `optimized_query_cache_pool` to use this unified execution logic, eliminating the need for separate sequential and parallel code paths.",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Feat: more cores for loading"". Body: adds ProcessPoolExecutor for parallel loading of project's models, mock executor for single-process scenarios, refactors optimized_query_cache_pool to use unified execution logic, eliminating separate sequential and parallel code paths.

This is clearly about improving performance by parallelizing loading. So classification: performance.assistantfinalperformance"
2546726071,153,refactor: move some important statements to import_algorithm_packages,johnrachwan123,43934839,closed,2025-05-27T14:41:59Z,2025-05-27T14:57:36Z,2025-05-27T14:57:36Z,https://api.github.com/repos/PrunaAI/pruna,https://github.com/PrunaAI/pruna/pull/153,"## Description
This PR moves some important statements to import_algorithm_packages in order to decrease the overall import time of pruna.
## Related Issue
None.

## Type of Change
<!-- Mark the appropriate option with an ""x"" (no spaces around the ""x"") -->
- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] This change requires a documentation update

## How Has This Been Tested?
None.

## Checklist
<!-- Mark items with ""x"" (no spaces around the ""x"") -->
- [ ] My code follows the style guidelines of this project
- [ ] I have performed a self-review of my own code
- [ ] I have commented my code, particularly in hard-to-understand areas
- [ ] I have made corresponding changes to the documentation
- [ ] I have added tests that prove my fix is effective or that my feature works
- [ ] New and existing unit tests pass locally with my changes

## Additional Notes
None.
",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""refactor: move some important statements to import_algorithm_packages"". Description: moves statements to import_algorithm_packages to decrease overall import time of pruna. That is performance improvement: reduce import time. So classification: performance.assistantfinalperformance"
2449472145,817,fix: Avoid recursive stateEnquiries when latency is high,ramedina86,64783088,closed,2025-04-09T20:00:43Z,2025-04-09T20:17:40Z,2025-04-09T20:17:40Z,https://api.github.com/repos/writer/writer-framework,https://github.com/writer/writer-framework/pull/817,"- When deployed stateEnquiries would take too long and trigger stateEnquiries themselves. Fixed.
- Replaced deprecated Pydantic method",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""fix: Avoid recursive stateEnquiries when latency is high"". Body: ""When deployed stateEnquiries would take too long and trigger stateEnquiries themselves. Fixed. Replaced deprecated Pydantic method"". The description mentions latency, high latency, taking too long, recursion. This is about performance: avoiding recursion when latency is high, so it's a performance fix. So classify as 'performance'.assistantfinalperformance"
2469218203,4192,perf(weave): address data loading perf issue on eval compare,chance-wnb,202173503,closed,2025-04-19T00:27:50Z,2025-04-22T18:11:08Z,2025-04-22T18:11:08Z,https://api.github.com/repos/wandb/weave,https://github.com/wandb/weave/pull/4192,"## Description

<!--
Include reference to internal ticket ""Fixes WB-NNNNN"" and/or GitHub issue ""Fixes #NNNN"" (if applicable)
-->

This addresses a part of the evaluation comparison performance issue. No the customer scenario will no longer crash. 

UI wise, there is no noticeable behavior change.

Here I attach [a video(internal)](https://www.loom.com/share/e07ee35dcd464094bc06aebf97fb8962?sid=691778a9-6156-4b2f-884a-095f3a78d29c) to explain the change.

## Testing

This PR is manually tested against the customer scenario and locally. 
",Human,"analysisWe need to classify as performance or non-performance. The title: ""perf(weave): address data loading perf issue on eval compare"". Contains ""perf"" and ""performance issue"". Body mentions ""performance issue"". So it's performance. So label 'performance'.assistantfinalperformance"
2358030784,4127,slight improvements to user group endpoints,evan-onyx,183385614,open,2025-02-26T01:05:19Z,,,https://api.github.com/repos/onyx-dot-app/onyx,https://github.com/onyx-dot-app/onyx/pull/4127,"## Description

Customer was having long load times from user-group and persona endpoints; these changes improve performance by about ~30% in some cases (worst I saw was equivalent performance to the non-eager loading version). 

## How Has This Been Tested?

This was tested with about 10,000 users, 100 user groups, and 500 personas. 

## Backporting (check the box to trigger backport action)

Note: You have to check that the action passes, otherwise resolve the conflicts manually and tag the patches.

- [ ] This PR should be backported (make sure to check that the backport attempt succeeds)
- [x] [Optional] Override Linear Check
",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""slight improvements to user group endpoints"". Body: mentions ""Customer was having long load times from user-group and persona endpoints; these changes improve performance by about ~30% in some cases"". So it's performance. So output 'performance'.assistantfinalperformance"
2625000241,4956,kg source fix,Orbital-Web,36625832,closed,2025-06-27T17:23:46Z,2025-06-27T20:20:42Z,2025-06-27T20:20:42Z,https://api.github.com/repos/onyx-dot-app/onyx,https://github.com/onyx-dot-app/onyx/pull/4956,"## Description

Fixed multiple row returned in get batch metadata, and made the query (number of docs) / (number of connectors) times faster (just the querying üòÉ ). 

## How Has This Been Tested?

Locally

## Backporting (check the box to trigger backport action)

Note: You have to check that the action passes, otherwise resolve the conflicts manually and tag the patches.

- [ ] This PR should be backported (make sure to check that the backport attempt succeeds)
- [x] [Optional] Override Linear Check
",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""kg source fix"". Body: ""Fixed multiple row returned in get batch metadata, and made the query (number of docs) / (number of connectors) times faster (just the querying üòÉ ).""

It mentions making query faster. That is performance improvement. So classification: performance.assistantfinalperformance"
2504141769,747,optimization: desktop UI,lollipopkit,10864310,closed,2025-05-07T11:43:07Z,2025-05-12T20:57:37Z,2025-05-12T20:57:37Z,https://api.github.com/repos/lollipopkit/flutter_server_box,https://github.com/lollipopkit/flutter_server_box/pull/747,"Fixes #524

## Summary by Sourcery

Optimize desktop UI and routing for the application, introducing more responsive and flexible navigation and page management

New Features:
- Introduced responsive framework for better cross-platform UI
- Added split view support for desktop layouts
- Implemented route-based navigation with type-safe arguments

Enhancements:
- Refactored routing to use more type-safe and explicit route definitions
- Improved desktop and mobile UI responsiveness
- Simplified page navigation and state management

Deployment:
- Updated pubspec.yaml to include new dependencies like responsive_framework and riverpod",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""optimization: desktop UI"". Body: mentions ""Optimize desktop UI and routing for the application, introducing more responsive and flexible navigation and page management"". It says ""responsive framework"", ""improved desktop and mobile UI responsiveness"". That is performance improvement? It says ""responsive"" but could be UI responsiveness, not necessarily performance. But keywords: ""responsive"" is listed as a keyword for performance. Also ""optimization"" is a keyword. So likely performance. The PR is about UI responsiveness, which is performance. So classify as 'performance'.assistantfinalperformance"
2517537659,752,opt.: appbar `scrolledUnderElevation`,lollipopkit,10864310,closed,2025-05-13T20:02:08Z,2025-05-13T20:02:30Z,2025-05-13T20:02:30Z,https://api.github.com/repos/lollipopkit/flutter_server_box,https://github.com/lollipopkit/flutter_server_box/pull/752,"Fixes #751

## Summary by Sourcery

Update project dependencies for improved compatibility and performance

Enhancements:
- Replace the code_text_field package with re_editor
- Bump fl_lib git reference from v1.0.281 to v1.0.283",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""opt.: appbar `scrolledUnderElevation` Fixes #751"". Body: ""## Summary by Sourcery Update project dependencies for improved compatibility and performance Enhancements: - Replace the code_text_field package with re_editor - Bump fl_lib git reference from v1.0.281 to v1.0.283""

Keywords: ""performance"" appears. Also ""improved compatibility and performance"". So it's performance. So label 'performance'.assistantfinalperformance"
2340618989,3341,[management] Add batching support for SaveUsers and SaveGroups,bcmmbaga,8535888,closed,2025-02-17T21:22:59Z,2025-02-17T23:08:03Z,2025-02-17T23:08:03Z,https://api.github.com/repos/netbirdio/netbird,https://github.com/netbirdio/netbird/pull/3341,"## Describe your changes

Introduces batching support for `SaveUsers` and `SaveGroups` to improve performance when handling large datasets.

## Issue ticket number and link

### Checklist
- [ ] Is it a bug fix
- [ ] Is a typo/documentation fix
- [ ] Is a feature enhancement
- [x] It is a refactor
- [ ] Created tests that fail without the change (if possible)
- [ ] Extended the README / documentation, if necessary
",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""[management] Add batching support for SaveUsers and SaveGroups"". Body says ""Introduces batching support for SaveUsers and SaveGroups to improve performance when handling large datasets."" So it's performance improvement. So label 'performance'.assistantfinalperformance"
2414338451,56369,üêõ Source-brevo: Fix/Add incremental on Contacts/Crm deals,KimPlv,67902258,closed,2025-03-24T17:08:25Z,2025-03-24T19:36:01Z,2025-03-24T19:36:00Z,https://api.github.com/repos/airbytehq/airbyte,https://github.com/airbytehq/airbyte/pull/56369,"Issue [56371](https://github.com/airbytehq/airbyte/issues/56371)

## What
fix incremental for Contacts streams 
add incremental for Crm Deals 

increase limit of contacts to retrive faster data 

## How
Contacts: 
update start_time_option field_name
increase page_size

Crm Deals:
add incremental features

## Review guide

1. `manifest.py`
-->

## User Impact
<!--
* What is the end result perceived by the user?
* If there are negative side effects, please list them. 
-->

## Can this PR be safely reverted and rolled back?
<!--
* If unsure, leave it blank.
-->
- [ ] YES üíö
- [ ] NO ‚ùå
",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""üêõ Source-brevo: Fix/Add incremental on Contacts/Crm deals"". Body: mentions ""increase limit of contacts to retrieve faster data"". Also ""increase page_size"". That is performance improvement. Also incremental features. So it's performance. So output 'performance'.assistantfinalperformance"
2369238232,74,Dramatically increase OB speed,joshyattridge,32915015,closed,2025-03-03T22:27:52Z,2025-03-03T22:29:38Z,2025-03-03T22:29:38Z,https://api.github.com/repos/joshyattridge/smart-money-concepts,https://github.com/joshyattridge/smart-money-concepts/pull/74,,Human,"analysisWe need to classify the pull request as performance or non-performance. The title: ""Dramatically increase OB speed"". That indicates performance improvement. So classification: performance.assistantfinalperformance"
2369253951,75,Increased the speed to calculate the liquidity function,joshyattridge,32915015,closed,2025-03-03T22:39:31Z,2025-03-03T22:40:28Z,2025-03-03T22:40:28Z,https://api.github.com/repos/joshyattridge/smart-money-concepts,https://github.com/joshyattridge/smart-money-concepts/pull/75,,Human,"analysisWe need to classify as performance or non-performance. The title: ""Increased the speed to calculate the liquidity function"". That indicates performance improvement. So label 'performance'.assistantfinalperformance"
2354104157,193,Memory optimisations - remove the oninput/-value wrapping,jvanoostveen,239357,closed,2025-02-24T15:56:10Z,2025-05-08T11:07:43Z,2025-05-08T11:07:43Z,https://api.github.com/repos/AFASSoftware/maquette,https://github.com/AFASSoftware/maquette/pull/193,,Human,"analysisWe need to classify whether the primary purpose is to improve performance. The title: ""Memory optimisations - remove the oninput/-value wrapping"". This suggests memory optimizations, removing wrapping. That is performance improvement. So classify as 'performance'.assistantfinalperformance"
2422597275,443,feat: secret cache by label selector to limit memory usage of kratix controller,ChunyiLyu,6786193,closed,2025-03-27T14:03:04Z,2025-03-31T15:46:17Z,2025-03-31T15:46:17Z,https://api.github.com/repos/syntasso/kratix,https://github.com/syntasso/kratix/pull/443,"## Context

GitStateStore and BucketStateStore controller watches Secrets. Controller runtime creates secret cache for the watch and by default, it creates caches for all secrets in the k8s cluster.
This means that in a busy cluster with lots of secrets, the default cache uses a lots of memory.

This PR adds the functionality for users to choose to turn on label selector cache for Secrets to limit memory usage. Default behavior does not change, to use this feature, they can configure the kratix config:
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kratix
  namespace: kratix-platform-system
data:
  config: |
    selectiveCache: true
```
Users must label secrets that's used for kratix objects with the label selector when we create the cache `app.kubernetes.io/part-of=kratix`. Or else kratix controller cannot get the secret even if the secret exists.

For reference, a cluster with ~2000 secrets, before custom cache is used, this is the controller heap:
<img width=""1670"" alt=""Screenshot 2025-03-27 at 14 02 06"" src=""https://github.com/user-attachments/assets/17765607-4422-4e41-9141-c59f17d3e33b"" />

After custom cache is in use:
<img width=""1523"" alt=""Screenshot 2025-03-27 at 14 01 22"" src=""https://github.com/user-attachments/assets/1957acb4-b45d-4d72-ad72-e17080bcbee4"" />
",Human,"analysisWe need to classify as performance or non-performance. The PR is about adding a feature to limit memory usage by using label selector cache. It addresses memory usage, which is a performance concern. The description mentions memory usage, heap usage, etc. So it's performance. The classification: 'performance'.assistantfinalperformance"
2441455457,437,Fix large cache size,Artiu,71273256,closed,2025-04-06T04:11:54Z,2025-04-07T02:58:37Z,2025-04-07T02:58:37Z,https://api.github.com/repos/tommyxchow/frosty,https://github.com/tommyxchow/frosty/pull/437,"There's big problem with caching images in disk (cache on my release app exceeded 4GB). I find out that it's problem with caching library working under the hood of ```cached_network_image``` package (```flutter_cache_manager```). ```flutter_cache_manager``` is only clearing cache from files database, not from file system. So I fixed this in this PR by adding function to remove images which are not in ```flutter_cache_manager```'s database.  NOT TESTED ON IOS, but I think it should be fine.",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Fix large cache size"". Body: describes caching images, cache size exceeded 4GB, issue with caching library, fixed by adding function to remove images not in database. This is about memory/disk usage, performance? It's about cache size, memory usage, disk usage. It's about performance? The guidelines: performance-related PRs may contain keywords like performance, optimization, memory, etc. This PR mentions ""large cache size"" and ""cache on my release app exceeded 4GB"". It's about cleaning up cache to reduce disk usage. That is performance improvement? It's more about resource usage, not necessarily speed. But it's about memory/disk usage, which is performance. The guidelines mention memory, leak, etc. So classify as 'performance'.assistantfinalperformance"
2588611363,520,Expand Vulkan and fakenvapi support,FakeMichau,49685661,closed,2025-06-12T22:20:42Z,2025-06-15T12:13:29Z,2025-06-15T12:13:29Z,https://api.github.com/repos/optiscaler/OptiScaler,https://github.com/optiscaler/OptiScaler/pull/520,"Currently supports only the deprecated nvapi version of vulkan Reflex; not the VK_NV_low_latency2 variant. Streamline only looks at VK_NV_low_latency which we probably can't hook due to how poorly the extension is made. 

Includes:
 - controlling of fps limit when Vulkan nvapi Reflex is used, mostly for fakenvapi
 - showing the currently use low latency tech in the menu
 - a hack to stop Vulkan DLSSG crashing with Opti

New features require fakenvapi 1.3.0 or later.",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Expand Vulkan and fakenvapi support"". Body: mentions controlling fps limit when Vulkan nvapi Reflex is used, showing low latency tech, hack to stop Vulkan DLSSG crashing. This is about adding support for Vulkan low latency, controlling fps limit, etc. It's about performance? The keywords: ""fps limit"", ""low latency"", ""performance"" maybe. The description: ""Expand Vulkan and fakenvapi support"" - not explicitly performance. But controlling fps limit and low latency tech is performance-related. The keywords: ""low latency"", ""fps limit"" are performance. So likely performance. The classification: 'performance'.assistantfinalperformance"
2474497456,175,Performant `merge` and remove test duplication,gustavoguichard,566971,open,2025-04-22T20:33:09Z,,,https://api.github.com/repos/seasonedcc/composable-functions,https://github.com/seasonedcc/composable-functions/pull/175,I'm just removing a duplicated test and adding a `merge` type written by [ahejlsberg himself](https://x.com/colinhacks/status/1912641412669272410).,Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Performant `merge` and remove test duplication"". Body: ""I'm just removing a duplicated test and adding a `merge` type written by [ahejlsberg himself](https://x.com/colinhacks/status/1912641412669272410).""

The title includes ""Performant `merge`"". That suggests performance improvement. The body mentions removing duplicated test and adding a merge type. The test duplication removal might be a small change, but the title says ""Performant merge"". So likely performance. The guidelines: performance-related PRs may contain keywords like performance, optimization, etc. ""Performant"" is a keyword. So classify as 'performance'.assistantfinalperformance"
2416343891,39906,chore: track sql plugins performance,vsvamsi1,121419957,closed,2025-03-25T11:16:28Z,2025-03-26T06:51:32Z,,https://api.github.com/repos/appsmithorg/appsmith,https://github.com/appsmithorg/appsmith/pull/39906,"## Description
> [!TIP]  
> _Add a TL;DR when the description is longer than 500 words or extremely technical (helps the content, marketing, and DevRel team)._
>
> _Please also include relevant motivation and context. List any dependencies that are required for this change. Add links to Notion, Figma or any other documents that might be relevant to the PR._


Fixes #`Issue Number`  
_or_  
Fixes `Issue URL`
> [!WARNING]  
> _If no issue exists, please create an issue first, and check with the maintainers if the issue is valid._

## Automation

/ok-to-test tags=""@tag.Sanity""

### :mag: Cypress test results
<!-- This is an auto-generated comment: Cypress test results  -->
> [!WARNING]
> Workflow run: <https://github.com/appsmithorg/appsmith/actions/runs/14066356252>
> Commit: 1654ffb140f1279b208f4b008e510e998a7f10ab
> <a href=""https://internal.appsmith.com/app/cypress-dashboard/rundetails-65890b3c81d7400d08fa9ee5?branch=master&workflowId=14066356252&attempt=1"" target=""_blank"">Cypress dashboard</a>.
> Tags: @tag.Sanity
> Spec: 
> It seems like **no tests ran** üòî. We are not able to recognize it, please check <a href=""https://github.com/appsmithorg/appsmith/actions/runs/14066356252"" target=""_blank"">workflow here</a>.
> <hr>Tue, 25 Mar 2025 17:49:32 UTC
<!-- end of auto-generated comment: Cypress test results  -->


## Communication
Should the DevRel and Marketing teams inform users about this change?
- [ ] Yes
- [ ] No
",Human,"analysisWe need to classify as performance or non-performance. The title: ""chore: track sql plugins performance"". The description: ""Fixes #Issue Number"" etc. The title indicates tracking performance. The content: ""track sql plugins performance"". That is performance. So classification: performance.assistantfinalperformance"
2417300798,39912,chore: Removing feature flag for app IDE new entity explorer,ankitakinger,28362912,closed,2025-03-25T17:16:22Z,2025-04-03T06:37:24Z,2025-04-03T06:37:24Z,https://api.github.com/repos/appsmithorg/appsmith,https://github.com/appsmithorg/appsmith/pull/39912,"## Description

Removing feature flag for app IDE new entity explorer

Fixes [#39919](https://github.com/appsmithorg/appsmith/issues/39919)

## Automation

/ok-to-test tags=""@tag.All""

### :mag: Cypress test results
<!-- This is an auto-generated comment: Cypress test results  -->
> [!TIP]
> üü¢ üü¢ üü¢ All cypress tests have passed! üéâ üéâ üéâ
> Workflow run: <https://github.com/appsmithorg/appsmith/actions/runs/14125497555>
> Commit: 589ae540cf367d972ba26e4678c8c1210f90c416
> <a href=""https://internal.appsmith.com/app/cypress-dashboard/rundetails-65890b3c81d7400d08fa9ee5?branch=master&workflowId=14125497555&attempt=2"" target=""_blank"">Cypress dashboard</a>.
> Tags: `@tag.All`
> Spec:
> <hr>Fri, 28 Mar 2025 16:27:15 UTC
<!-- end of auto-generated comment: Cypress test results  -->


## Communication
Should the DevRel and Marketing teams inform users about this change?
- [ ] Yes
- [ ] No


<!-- This is an auto-generated comment: release notes by coderabbit.ai -->

## Summary by CodeRabbit

- **Refactor**
  - Streamlined UI rendering for page listings, widget views, and explorer displays by eliminating conditional logic based on legacy feature settings.
- **Chores**
  - Removed outdated interface elements and context menus to simplify navigation and enhance consistency.
- **Tests**
  - Updated configurations to align testing environments with the consolidated UI behavior.

Overall, users will experience a more uniform and efficient interface with improved performance and reduced interface complexity.

<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""chore: Removing feature flag for app IDE new entity explorer"". Body: description, mentions removing feature flag, refactor, streamline UI rendering, eliminate conditional logic, removed outdated interface elements, improved performance and reduced interface complexity. Does it focus on performance? It says ""improved performance"" but is it a performance improvement? The keywords: ""performance"" appears. But is the primary purpose to improve performance? It seems to be a refactor to remove feature flag, simplify UI, maybe performance improvement. The classification: performance. The guidelines: performance-related PRs may contain keywords such as performance. This PR contains ""performance"". So classify as performance.assistantfinalperformance"
2563668066,40849,chore: Adding debounce to `onValueChange` for input widgets,ankitakinger,28362912,closed,2025-06-03T11:48:03Z,2025-06-06T10:21:36Z,2025-06-06T10:21:36Z,https://api.github.com/repos/appsmithorg/appsmith,https://github.com/appsmithorg/appsmith/pull/40849,"## Description

Adding debounce to `onValueChange` for input widgets to fix multiple Execute API calls happening in reactive queries flow.

Fixes [#40813](https://github.com/appsmithorg/appsmith/issues/40813)

## Automation

/ok-to-test tags=""@tag.All""

### :mag: Cypress test results
<!-- This is an auto-generated comment: Cypress test results  -->
> [!TIP]
> üü¢ üü¢ üü¢ All cypress tests have passed! üéâ üéâ üéâ
> Workflow run: <https://github.com/appsmithorg/appsmith/actions/runs/15486342735>
> Commit: 6943ba5d0df915256cf29831df53e9ff9880d617
> <a href=""https://internal.appsmith.com/app/cypress-dashboard/rundetails-65890b3c81d7400d08fa9ee5?branch=master&workflowId=15486342735&attempt=1"" target=""_blank"">Cypress dashboard</a>.
> Tags: `@tag.All`
> Spec:
> <hr>Fri, 06 Jun 2025 09:40:52 UTC
<!-- end of auto-generated comment: Cypress test results  -->


## Communication
Should the DevRel and Marketing teams inform users about this change?
- [ ] Yes
- [ ] No


<!-- This is an auto-generated comment: release notes by coderabbit.ai -->
## Summary by CodeRabbit

- **New Features**
  - Input widgets now update their displayed values instantly while saving changes in the background with a short delay, improving typing responsiveness.
  - Input changes are grouped and saved after a brief pause, reducing unnecessary updates and enhancing performance.
- **Bug Fixes**
  - Input fields now stay in sync with external updates and clear any pending background updates when needed, preventing outdated or duplicate changes.
- **Chores**
  - Improved cleanup of background processes when input widgets are removed, ensuring smoother operation.
- **Tests**
  - Added typing delays in input simulation during Cypress tests to better mimic real user input timing.
<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""chore: Adding debounce to `onValueChange` for input widgets"". Description: adding debounce to onValueChange to fix multiple Execute API calls happening in reactive queries flow. That is performance improvement: reducing API calls, improving responsiveness. Keywords: debounce, performance, responsiveness. So classification: performance.assistantfinalperformance"
2617294066,41033,chore: ce changes related to decoupling webworker,vsvamsi1,121419957,open,2025-06-25T07:45:14Z,,,https://api.github.com/repos/appsmithorg/appsmith,https://github.com/appsmithorg/appsmith/pull/41033,"## Description
We are improving the LCP by reducing the time to reach the first evaluation, aiming for a 1.8 to 2.2 second reduction. To achieve this, we‚Äôve implemented the following changes:

Code Splitting of Widgets: During page load, only the widgets required for the initial evaluation are loaded and registered. The remaining widgets are registered after the first evaluation message is sent. This parallelizes widget loading with evaluation computation, reducing the critical path.

Web Worker Offloading: Macro tasks such as clearCache and JavaScript library installation have been moved to the web worker setup. These are now executed in a separate thread, allowing the firstUnevaluatedTree to be computed in parallel with JS library installation.

Parallel JS Library Loading: All JavaScript libraries are now loaded in parallel within the web worker, instead of sequentially, improving efficiency.

Deferred Rendering of AppViewer: We now render the AppViewer component only after registering the remaining widgets. This ensures that heavy rendering tasks‚Äîsuch as expensive selector computations and loading additional chunks related to the AppViewer‚Äîcan execute in parallel with the first evaluation, further enhancing performance.

## Automation

/ok-to-test tags=""@tag.All""

### :mag: Cypress test results
<!-- This is an auto-generated comment: Cypress test results  -->
> [!CAUTION]
> üî¥ üî¥ üî¥ Some tests have failed.
> Workflow run: <https://github.com/appsmithorg/appsmith/actions/runs/15894953337>
> Commit: 2dc9dbcd6b60cb63ec954713dbf7335d788df9a4
> <a href=""https://internal.appsmith.com/app/cypress-dashboard/rundetails-65890b3c81d7400d08fa9ee5?branch=master&workflowId=15894953337&attempt=1&selectiontype=test&testsstatus=failed&specsstatus=fail"" target=""_blank"">Cypress dashboard</a>.
> Tags: @tag.All
> Spec: 
> The following are new failures, please fix them before merging the PR: <ol>
> <li>cypress/e2e/Regression/ClientSide/OtherUIFeatures/Analytics_spec.js</ol>
> <a href=""https://internal.appsmith.com/app/cypress-dashboard/identified-flaky-tests-65890b3c81d7400d08fa9ee3?branch=master"" target=""_blank"">List of identified flaky tests</a>.
> <hr>Thu, 26 Jun 2025 07:57:26 UTC
<!-- end of auto-generated comment: Cypress test results  -->


## Communication
Should the DevRel and Marketing teams inform users about this change?
- [ ] Yes
- [ ] No


<!-- This is an auto-generated comment: release notes by coderabbit.ai -->
## Summary by CodeRabbit

- **New Features**
  - Added support for deferred loading of JavaScript libraries and improved control over page rendering and first page load behavior.
  - Introduced granular widget registration, allowing partial widget initialization for faster initial rendering.
  - Added new Redux actions and selectors to manage and track evaluation and rendering state.
  - Added explicit cache clearing for widget factory memoization functions.

- **Improvements**
  - Refactored widget loading to be asynchronous and on-demand, reducing initial load time and improving modularity.
  - Enhanced sagas and reducers to better handle first-time evaluations and widget registration.
  - Optimized JS library loading to occur in parallel for improved performance.
  - Modularized theme application and improved conditional rendering in the App Viewer.
  - Reorganized widget registration to initialize widgets individually rather than in bulk.
  - Improved memoization decorator to allow explicit cache clearing globally.
  - Updated evaluation sagas to support partial widget initialization and deferred JS library loading.
  - Updated widget loading utilities and tests to support asynchronous dynamic loading.

- **Bug Fixes**
  - Improved conditional logic to prevent errors when rendering components with missing functions.

- **Tests**
  - Expanded and refactored test suites to cover asynchronous widget loading, partial initialization, and evaluation saga behaviors.
  - Added tests verifying widget factory cache behavior and first evaluation integration.

- **Chores**
  - Updated imports and code structure for clarity and maintainability.
  - Reorganized type imports and moved interface declarations to dedicated modules.
<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,"analysisWe need to classify as 'performance' or 'non-performance'. The PR title: ""chore: ce changes related to decoupling webworker"". Body: describes improving LCP by reducing time to first evaluation, code splitting, web worker offloading, parallel JS library loading, deferred rendering, etc. This is clearly performance improvement. So label 'performance'.assistantfinalperformance"
2398828721,39757,chore: remove analytics execution from the critical path,vsvamsi1,121419957,closed,2025-03-17T19:12:46Z,2025-03-18T11:51:51Z,2025-03-18T11:51:51Z,https://api.github.com/repos/appsmithorg/appsmith,https://github.com/appsmithorg/appsmith/pull/39757,"## Description

- Pushed out the sendExecuteAnalyticsEvent from the critical path of returning action's execution result.
- Improved the critical Path of sendExecuteAnalyticsEvent by running the application mono concurrent to other events.
- Added more telemetry code around the execution flow.


Fixes #`Issue Number`  
_or_  
Fixes `Issue URL`
> [!WARNING]  
> _If no issue exists, please create an issue first, and check with the maintainers if the issue is valid._

## Automation

/ok-to-test tags=""@tag.All""

### :mag: Cypress test results
<!-- This is an auto-generated comment: Cypress test results  -->
> [!TIP]
> üü¢ üü¢ üü¢ All cypress tests have passed! üéâ üéâ üéâ
> Workflow run: <https://github.com/appsmithorg/appsmith/actions/runs/13919689126>
> Commit: ddf93dd06cd4facabdde5898d1cc40ce7dc4765f
> <a href=""https://internal.appsmith.com/app/cypress-dashboard/rundetails-65890b3c81d7400d08fa9ee5?branch=master&workflowId=13919689126&attempt=1"" target=""_blank"">Cypress dashboard</a>.
> Tags: `@tag.All`
> Spec:
> <hr>Tue, 18 Mar 2025 10:28:52 UTC
<!-- end of auto-generated comment: Cypress test results  -->


## Communication
Should the DevRel and Marketing teams inform users about this change?
- [ ] Yes
- [ ] No


<!-- This is an auto-generated comment: release notes by coderabbit.ai -->

## Summary by CodeRabbit

- **New Features**
	- Introduced additional action tracking identifiers to support enhanced analytics and authentication validation.
- **Refactor**
	- Optimized asynchronous operations for data retrieval to improve responsiveness.
	- Enhanced the flow and error handling of action execution, ensuring smoother and more reliable performance.

<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""chore: remove analytics execution from the critical path"". Description: ""Pushed out the sendExecuteAnalyticsEvent from the critical path of returning action's execution result. Improved the critical Path of sendExecuteAnalyticsEvent by running the application mono concurrent to other events. Added more telemetry code around the execution flow."" Also summary: ""Optimized asynchronous operations for data retrieval to improve responsiveness."" So it's about performance improvement: removing analytics from critical path, concurrency, responsiveness. So classification: performance.assistantfinalperformance"
2367540304,111,Accessibility - Add 300ms debounce around the parsing,Niduank,6816972,closed,2025-03-03T08:56:30Z,2025-03-03T09:01:37Z,2025-03-03T09:01:37Z,https://api.github.com/repos/synth-inc/onit,https://github.com/synth-inc/onit/pull/111,,Human,"analysisWe need to classify whether the primary purpose is to improve performance. The PR title: ""Accessibility - Add 300ms debounce around the parsing"". This seems to be adding debounce to parsing, likely to reduce frequency of parsing, maybe to improve performance or responsiveness. Debounce is often used to reduce load, improve performance. The title mentions accessibility, but the change is adding debounce. The question: is the primary purpose to improve performance? Debounce reduces number of operations, so yes, it's performance improvement. So classify as 'performance'.assistantfinalperformance"
2591073348,30693,[freshness] Update instance method to batch queries,OwenKephart,22457492,closed,2025-06-13T18:51:17Z,2025-06-20T16:44:20Z,2025-06-20T16:44:19Z,https://api.github.com/repos/dagster-io/dagster,https://github.com/dagster-io/dagster/pull/30693,"## Summary & Motivation

Follows the pattern of `instance.get_asset_records()` to allow this data to be fetched in a batch.

This enables an efficient `FreshnessStateRecord.gen()` implementation, added in this PR

## How I Tested These Changes

## Changelog

NOCHANGELOG
",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""[freshness] Update instance method to batch queries"". The body: ""Follows the pattern of instance.get_asset_records() to allow this data to be fetched in a batch. This enables an efficient FreshnessStateRecord.gen() implementation, added in this PR"". So it's about batching queries to improve efficiency. That is performance improvement. Keywords: batch, efficient. So classify as 'performance'.assistantfinalperformance"
2489800084,2575,Experiment: Test disable dropping of datasets in tests,sh-rp,1155738,closed,2025-04-29T19:53:51Z,2025-04-30T10:51:02Z,,https://api.github.com/repos/dlt-hub/dlt,https://github.com/dlt-hub/dlt/pull/2575,"<!--
Thank you for submitting a pull request! Please provide a brief description of your changes below.
-->
### Description
This is an experiment to see if we can save CI time by not dropping datasets in a pytest fixtures but maybe in a nightly workflow run. These nightly runs would not be concurrent but only run once, so they could also be executed from some github runner on a small DO droplet to note waste CI minutes.

Things to check:

* Execution time (Athena w/o iceberg is at around 70-80 minutes for essential tests, with iceberg around 170 minutes)
* Test collissions. It is conceivable that some tests use the same dataset name without dev mode and we see collissions, so if tests fail, investigate!",Human,"analysisWe need to classify whether the primary purpose is to improve performance. The PR title: ""Experiment: Test disable dropping of datasets in tests"". Body: It's about saving CI time by not dropping datasets in pytest fixtures, maybe in nightly workflow. They want to check execution time, test collisions. This is about performance: saving CI time, execution time. So it's performance. So label 'performance'.assistantfinalperformance"
2402633884,1752,Switch to FXHash,MeetThePatel,11304993,closed,2025-03-19T04:55:48Z,2025-06-21T07:10:40Z,,https://api.github.com/repos/huggingface/tokenizers,https://github.com/huggingface/tokenizers/pull/1752,"Split PR #1733 into two separate PRs, as per @McPatate 's suggestion. This PR contains the changes for adding FXHash.

The benchmarks (from `cargo bench`) are in [cargo_bench.txt](https://github.com/user-attachments/files/19333143/cargo_bench.txt). Some performance gains, but not as large as the short-string optimization.

Note: The changes in this PR are slightly different than the ones from the other PR. I refactored some things to make this change mostly invisible to the user (this only applies to the Rust library; the Python and Node users shouldn't feel any difference). The only exposure these changes have to a library user are the return types for the ""getters"". For the setters (and other functionality), you can provide a `std::collections::HashMap` and it will convert to a `rustc_hash::FxHashMap` (and the same for HashSet) internally.

I'll work on the PR for the short-string optimization after this PR has been reviewed. I think that would be easier, as changes here would impact the other.",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Switch to FXHash"". Body: mentions adding FXHash, benchmarks, some performance gains. It's about performance improvements. So classification: performance.assistantfinalperformance"
2603553498,1804,Adding multiprocessing for sentencepiece_extractor,AamodThakur,141419512,open,2025-06-19T04:38:54Z,,,https://api.github.com/repos/huggingface/tokenizers,https://github.com/huggingface/tokenizers/pull/1804,"**Goal:** To speedup merges extraction for the Sentence Piece

Adding multiprocessing for sentencepiece_extractor code to speed-up the merges extraction process. 
Since for 128K vocabulary more than an hour of time is consumed.",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Adding multiprocessing for sentencepiece_extractor"". Body: ""Goal: To speedup merges extraction for the Sentence Piece. Adding multiprocessing for sentencepiece_extractor code to speed-up the merges extraction process. Since for 128K vocabulary more than an hour of time is consumed."" This is clearly performance improvement. So label 'performance'.assistantfinalperformance"
2483801280,28,download nltk tokenizer only once,captainst,23187023,closed,2025-04-27T02:13:07Z,2025-05-21T19:55:36Z,2025-05-21T19:55:35Z,https://api.github.com/repos/stair-lab/kg-gen,https://github.com/stair-lab/kg-gen/pull/28,"Currently, the tokenizer is downloaded everytime when calling `from kg_gen import KGGen`. It should be downloaded only once.",Human,"analysisWe need to classify the PR as performance or non-performance. The PR description: ""download nltk tokenizer only once. Currently, the tokenizer is downloaded everytime when calling from kg_gen import KGGen. It should be downloaded only once."" This is about reducing repeated downloads, which is a performance improvement. It reduces overhead, speeds up. So classify as 'performance'.assistantfinalperformance"
2336649960,31868,[IBD] specialize block serialization,l0rinc,1841944,open,2025-02-14T16:48:23Z,,,https://api.github.com/repos/bitcoin/bitcoin,https://github.com/bitcoin/bitcoin/pull/31868,"This change is part of [[IBD] - Tracking PR for speeding up Initial Block Download](https://github.com/bitcoin/bitcoin/pull/32043)

---

This PR is drafted until I remeasure everything after the recent merges and I need to find a way to simplify the 1 byte writes more nicely, I don't like all the specializations.

---

### Summary

This PR contain a few different optimization I found by IBD profiling, and via the newly added block seralization benchmarks. It also takes advantage of the recently merged [`std::span` changes](https://github.com/bitcoin/bitcoin/pull/31519) enabling propagating static extents.

The commits merge similar (de)serialization methods, and separates them internally with  `if constexpr` - similarly to how it has been [done here before](https://github.com/bitcoin/bitcoin/pull/28203). This enabled further `SizeComputer` optimizations as well.

### Context
Other than these, since single byte writes are used very often (used for every `(u)int8_t` or `std::byte` or `bool` and for every `VarInt`'s first byte which is also needed for every `(pre)Vector`), it makes sense to avoid the generalized serialization infrastructure that isn't needed:
* `AutoFile` write doesn't need to allocate 4k buffer for a single byte now;
* `VectorWriter` and `DataStream` avoids memcpy/insert calls;
* `CSHA256::Write` can avoid `memcpy`.

`DeserializeBlock` is dominated by the hash calculations so the optimizations barely affect it.

### Measurements

<details>
<summary>C compiler ............................ AppleClang 16.0.0.16000026</summary>

> Before:

|            ns/block |             block/s |    err% |     total | benchmark
|--------------------:|--------------------:|--------:|----------:|:----------
|          195,610.62 |            5,112.20 |    0.3% |     11.00 | `SerializeBlock`
|           12,061.83 |           82,906.19 |    0.1% |     11.01 | `SizeComputerBlock`

> After:

|            ns/block |             block/s |    err% |     total | benchmark
|--------------------:|--------------------:|--------:|----------:|:----------
|          174,569.19 |            5,728.39 |    0.6% |     10.89 | `SerializeBlock`
|           10,241.16 |           97,645.21 |    0.0% |     11.00 | `SizeComputerBlock`

</details>

> `SerializeBlock` - ~12.% faster
> `SizeComputerBlock` - ~17.7% faster

-----


<details>
<summary>C++ compiler .......................... GNU 13.3.0</summary>

> Before:

|            ns/block |             block/s |    err% |       ins/block |       cyc/block |    IPC |      bra/block |   miss% |     total | benchmark
|--------------------:|--------------------:|--------:|----------------:|----------------:|-------:|---------------:|--------:|----------:|:----------
|          867,857.55 |            1,152.26 |    0.0% |    8,015,883.90 |    3,116,099.08 |  2.572 |   1,517,035.87 |    0.5% |     10.81 | `SerializeBlock`
|           30,928.27 |           32,332.88 |    0.0% |      221,683.03 |      111,055.84 |  1.996 |      53,037.03 |    0.8% |     11.03 | `SizeComputerBlock`

> After:

|            ns/block |             block/s |    err% |       ins/block |       cyc/block |    IPC |      bra/block |   miss% |     total | benchmark
|--------------------:|--------------------:|--------:|----------------:|----------------:|-------:|---------------:|--------:|----------:|:----------
|          615,000.56 |            1,626.01 |    0.0% |    8,015,883.64 |    2,208,340.88 |  3.630 |   1,517,035.62 |    0.5% |     10.56 | `SerializeBlock`
|           25,676.76 |           38,945.72 |    0.0% |      159,390.03 |       92,202.10 |  1.729 |      42,131.03 |    0.9% |     11.00 | `SizeComputerBlock`

</details>

> `SerializeBlock` - ~41.1% faster
> `SizeComputerBlock` - ~20.4% faster

----

While this wasn't the main motivation for the change, IBD on Ubuntu/GCC on SSD with i9 indicates a 2% speedup as well:

<details>
<summary>Details</summary>

```bash
COMMITS=""05314bde0b06b820225f10c6529b5afae128ff81 1cd94ec2511874ec68b92db34ad7ec7d9534fed1""; \
STOP_HEIGHT=880000; DBCACHE=10000; \
C_COMPILER=gcc; CXX_COMPILER=g++; \
hyperfine \
--export-json ""/mnt/my_storage/ibd-${COMMITS// /-}-${STOP_HEIGHT}-${DBCACHE}-${C_COMPILER}.json"" \
--runs 3 \
--parameter-list COMMIT ${COMMITS// /,} \
--prepare ""killall bitcoind || true; rm -rf /mnt/my_storage/BitcoinData/*; git checkout {COMMIT}; git clean -fxd; git reset --hard; cmake -B build -DCMAKE_BUILD_TYPE=Release -DENABLE_WALLET=OFF -DCMAKE_C_COMPILER=$C_COMPILER -DCMAKE_CXX_COMPILER=$CXX_COMPILER && cmake --build build -j$(nproc) --target bitcoind && ./build/bin/bitcoind -datadir=/mnt/my_storage/BitcoinData -stopatheight=1 -printtoconsole=0 || true"" \
--cleanup ""cp /mnt/my_storage/BitcoinData/debug.log /mnt/my_storage/logs/debug-{COMMIT}-$(date +%s).log || true"" \
""COMPILER=$C_COMPILER COMMIT={COMMIT} ./build/bin/bitcoind -datadir=/mnt/my_storage/BitcoinData -stopatheight=$STOP_HEIGHT -dbcache=$DBCACHE -prune=550 -printtoconsole=0""
Benchmark 1: COMPILER=gcc COMMIT=05314bde0b06b820225f10c6529b5afae128ff81 ./build/bin/bitcoind -datadir=/mnt/my_storage/BitcoinData -stopatheight=880000 -dbcache=10000 -prune=550 -printtoconsole=0
  Time (mean ¬± œÉ):     33647.918 s ¬± 508.655 s    [User: 71503.409 s, System: 4404.899 s]
  Range (min ‚Ä¶ max):   33283.439 s ‚Ä¶ 34229.026 s    3 runs
 
Benchmark 2: COMPILER=gcc COMMIT=1cd94ec2511874ec68b92db34ad7ec7d9534fed1 ./build/bin/bitcoind -datadir=/mnt/my_storage/BitcoinData -stopatheight=880000 -dbcache=10000 -prune=550 -printtoconsole=0
  Time (mean ¬± œÉ):     33062.491 s ¬± 183.335 s    [User: 71246.532 s, System: 4318.490 s]
  Range (min ‚Ä¶ max):   32888.211 s ‚Ä¶ 33253.706 s    3 runs
 
Summary
  COMPILER=gcc COMMIT=1cd94ec2511874ec68b92db34ad7ec7d9534fed1 ./build/bin/bitcoind -datadir=/mnt/my_storage/BitcoinData -stopatheight=880000 -dbcache=10000 -prune=550 -printtoconsole=0 ran
    1.02 ¬± 0.02 times faster than COMPILER=gcc COMMIT=05314bde0b06b820225f10c6529b5afae128ff81 ./build/bin/bitcoind -datadir=/mnt/my_storage/BitcoinData -stopatheight=880000 -dbcache=10000 -prune=550 -printtoconsole=0
```

</details>",Human,"analysisWe need to classify as 'performance' or 'non-performance'. The PR title: ""[IBD] specialize block serialization"". Body describes performance improvements: serialization speed, benchmarks, 12% faster, 41% faster, etc. So it's performance. So label 'performance'.assistantfinalperformance"
2593413868,1277,Widget: Remove Title Focus Delay & Default to Title Field By Default,AlexGStapleton,17275120,closed,2025-06-15T10:39:15Z,2025-06-16T11:49:32Z,2025-06-16T11:49:32Z,https://api.github.com/repos/siteorigin/siteorigin-panels,https://github.com/siteorigin/siteorigin-panels/pull/1277,This PR will remove the delay from the Widget Title field. This PR will also improve fallback handling when the title field isn't editable - there was a chance a field other than the title would be focused.,Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Widget: Remove Title Focus Delay & Default to Title Field By Default"". Body: ""This PR will remove the delay from the Widget Title field. This PR will also improve fallback handling when the title field isn't editable - there was a chance a field other than the title would be focused.""

Keywords: ""delay"" is a performance-related term? The instruction says performance-related PRs may contain keywords such as respons, react, wait, responsive, throughput, slow, performance, optimization, latency, compile-time-hog, fast, memory, stuck, leak, hang, perf, minor, speed, perform, lag, latenc, tim, instant, or any other terms referring to how faster and efficiently software runs.

""delay"" is not explicitly listed but ""wait"" is. ""delay"" is similar to wait. The PR removes a delay, which is a performance improvement. So likely performance. The PR is about removing a delay, which improves responsiveness. So classify as 'performance'.

Thus output: performance.assistantfinalperformance"
2496241835,6559,add build:vite script,CoyAce,18475924,closed,2025-05-02T19:11:37Z,2025-05-02T19:54:31Z,,https://api.github.com/repos/mermaid-js/mermaid,https://github.com/mermaid-js/mermaid/pull/6559,"## :bookmark_tabs: Summary

Brief description about the content of your PR.

esbuild output size: 15.5M
vite       output size: 5.1M

esbuild output use ES2022 syntax, not compitable with IOS 15 safari
use vite will transform to ES2018, which is defined in tsconfig.json ""compilerOptions"": {""target"": ""ES2018""}, no static initialization blocks will be generated

Resolves #6529 
Usage: 
pnpm run release
reference mermaid.esm.mjs
```html
  <html lang=""en"">
      <head>
        <meta charset=""utf-8"" />
      </head>
      <body>
        <script type=""module"">
          import mermaid from 'The/Path/In/Your/Package/mermaid.esm.mjs';
          mermaid.initialize({ startOnLoad: true });
        </script>
      </body>
  </html>
```


## :straight_ruler: Design Decisions

Describe the way your implementation works or what design decisions you made if applicable.

### :clipboard: Tasks

Make sure you

- [x] :book: have read the [contribution guidelines](https://mermaid.js.org/community/contributing.html)
- [ ] :computer: have added necessary unit/e2e tests.
- [ ] :notebook: have added documentation. Make sure [`MERMAID_RELEASE_VERSION`](https://mermaid.js.org/community/contributing.html#update-documentation) is used for all new features.
- [ ] :butterfly: If your PR makes a change that should be noted in one or more packages' changelogs, generate a changeset by running `pnpm changeset` and following the prompts. Changesets that add features should be `minor` and those that fix bugs should be `patch`. Please prefix changeset messages with `feat:`, `fix:`, or `chore:`.
",Human,"analysisWe need to classify whether the primary purpose is performance. The PR adds a build script using Vite, reducing output size from 15.5M to 5.1M. That is a performance improvement (size reduction). Also mentions esbuild output size vs vite output size. So it's performance. So label 'performance'.assistantfinalperformance"
2336988355,4336,Minor changes to specific declarations in animation.lua,codecanal,197926377,closed,2025-02-14T20:18:39Z,2025-02-15T17:33:11Z,,https://api.github.com/repos/beyond-all-reason/Beyond-All-Reason,https://github.com/beyond-all-reason/Beyond-All-Reason/pull/4336,"### Work done
I made some slight optimizations for some of the variable declarations in the functions of animation.lua. reduced the number of variables being declared in the beginning of every function, while also eliminating some variables that are being redeclared in infinite loops, leaving only initializations.",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Minor changes to specific declarations in animation.lua"". Body: ""I made some slight optimizations for some of the variable declarations in the functions of animation.lua. reduced the number of variables being declared in the beginning of every function, while also eliminating some variables that are being redeclared in infinite loops, leaving only initializations.""

This is about variable declarations, reducing number of variables, eliminating redeclarations in infinite loops. That is a performance optimization: reducing variable declarations can improve performance, especially in loops. The description mentions ""optimizations"" and ""slight optimizations"". So it's performance. So label 'performance'.assistantfinalperformance"
2283889982,4235,Load dependencies in parallel when updating UI elements,NiklasNeugebauer,68709968,closed,2025-01-17T09:41:48Z,2025-01-20T14:50:29Z,2025-01-20T14:50:29Z,https://api.github.com/repos/zauberzeug/nicegui,https://github.com/zauberzeug/nicegui/pull/4235,"I noticed that adding or changing a large number of elements, especially inside an SVG tag, takes very long.

This changes the update handler to first load all dependencies in parallel and then update the elements.",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Load dependencies in parallel when updating UI elements"". Body: ""I noticed that adding or changing a large number of elements, especially inside an SVG tag, takes very long. This changes the update handler to first load all dependencies in parallel and then update the elements.""

This is about performance: loading dependencies in parallel to speed up UI updates. Keywords: ""takes very long"", ""parallel"", ""update handler"". So it's performance. So label 'performance'.assistantfinalperformance"
2340742349,769,test: lower resource requirements for integration tests,rueian,2727535,closed,2025-02-17T23:19:31Z,2025-02-18T00:11:41Z,,https://api.github.com/repos/redis/rueidis,https://github.com/redis/rueidis/pull/769,,Human,"analysisWe need to classify the PR as performance or non-performance. The title: ""test: lower resource requirements for integration tests"". The body is not provided. The title indicates reducing resource requirements for integration tests. That is about resource usage, not necessarily performance. But ""lower resource requirements"" could be considered performance improvement? The instruction says performance-related PRs may contain keywords like performance, optimization, memory, etc. The title does not contain those. It says ""lower resource requirements for integration tests"". That could be about memory usage or CPU usage. But the instruction says performance-related PRs may contain keywords such as... memory, etc. ""memory"" is a keyword. But the title does not contain ""memory"". It says ""lower resource requirements"". That is about resource usage. But the instruction says performance-related PRs may contain keywords such as... memory. But not necessarily required. The classification is based on primary purpose. Lower resource requirements for integration tests is about reducing resource consumption, which is a performance improvement. But is it considered performance? It might be considered a performance improvement. But the instruction says performance-related PRs may contain keywords such as... memory. But the title does not contain memory. But the instruction says ""may contain keywords"" not ""must contain"". So it's possible to be performance. The primary purpose is to lower resource requirements for integration tests. That is a performance improvement. So classify as 'performance'.assistantfinalperformance"
2470590805,8892,Improve apphost discovery.,mitchdenny,513398,closed,2025-04-21T06:16:21Z,2025-04-22T05:37:56Z,2025-04-22T05:37:56Z,https://api.github.com/repos/dotnet/aspire,https://github.com/dotnet/aspire/pull/8892,"This PR improves usability of the `aspire [run|add|publish] commands by searching for valid app hosts in sub-directories, using the current working directory as a starting point. Here is a walkthrough video showing it in action. The important thing to note here is that we did not change into the `src/HelloAspire.AppHost` sub directory in order for this command to work.

https://github.com/user-attachments/assets/47523b65-0a9d-4d1a-868e-7a9ff3810945

Note that this recursive search behavior can be expensive in projects with a large number of *.csproj files so it is possible to place a `.aspire/settings.json` file in the current directory or any parent directory to short-circuit the discovery process and point directly to a well known apphost.

Included in this PR is a mechanism whereby this configuration file is automatically dropped in the `<workingdir>/.aspire/settings.json` path to avoid subsequent invocations taking as long. The idea is that by the time it becomes a problem in a repository the settings file is present.",Human,"analysisWe need to classify as performance or non-performance. The PR is about improving apphost discovery, adding recursive search, adding config file to avoid expensive search. The mention: ""Note that this recursive search behavior can be expensive in projects with a large number of *.csproj files so it is possible to place a.aspire/settings.json file in the current directory or any parent directory to short-circuit the discovery process and point directly to a well known apphost."" Also ""Included in this PR is a mechanism whereby this configuration file is automatically dropped in the <workingdir>/.aspire/settings.json path to avoid subsequent invocations taking as long."" So it's about performance optimization: reducing expensive search. So classification: performance.assistantfinalperformance"
2299225316,7261,Refactor health check service to be more responsive,JamesNK,303201,closed,2025-01-27T07:21:59Z,2025-01-27T12:05:11Z,2025-01-27T12:05:11Z,https://api.github.com/repos/dotnet/aspire,https://github.com/dotnet/aspire/pull/7261,"## Description

Fixes https://github.com/dotnet/aspire/issues/6112

The health check service immediately checks health when a resource moves to a running state. However, if the intial report is unhealthy then it waits 5 seconds before checking again. This PR refactors the health check service to be more responsive.

* If a health report isn't healthy then the delay interval starts at 1 second and increases by 1 second per failure up to the existing 5 sec. Healthy delay is still 30 seconds.
* Refactored the health check loop to use an instant interrupt instead of polling the state once per second. The `DelayAsync` method is complicated, but the actual health monitoring loop is much simpler.
* Allow setting the health delays from tests so time based tests run a lot faster. No more 30 second tests.
* No more blocking in tests: `AutoResetEvent` üëé 

## Checklist

- Is this feature complete?
  - [x] Yes. Ready to ship.
  - [ ] No. Follow-up changes expected.
- Are you including unit tests for the changes and scenario tests if relevant?
  - [x] Yes
  - [ ] No
- Did you add public API?
  - [ ] Yes
    - If yes, did you have an API Review for it?
      - [ ] Yes
      - [ ] No
    - Did you add `<remarks />` and `<code />` elements on your triple slash comments?
      - [ ] Yes
      - [ ] No
  - [x] No
- Does the change make any security assumptions or guarantees?
  - [ ] Yes
    - If yes, have you done a threat model and had a security review?
      - [ ] Yes
      - [ ] No
  - [x] No
- Does the change require an update in our Aspire docs?
  - [ ] Yes
    - Is this introducing a breaking change?
      - [ ] Yes
        - Link to aspire-docs issue (please use this [`breaking-change` template](https://github.com/dotnet/docs-aspire/issues/new?template=04-breaking-change.yml)):
      - [ ] No
        - Link to aspire-docs issue (please use this [`doc-idea` template](https://github.com/dotnet/docs-aspire/issues/new?template=02-docs-request.yml)):
  - [x] No
",Human,"analysisWe need to classify as 'performance' or 'non-performance'. The PR title: ""Refactor health check service to be more responsive"". The description mentions making health check service more responsive, reducing delays, making tests faster, etc. This is about responsiveness, which is a performance improvement. The keywords: responsive, performance, latency, etc. So it's performance. So output 'performance'.assistantfinalperformance"
2481661719,15509,Swap doc preview and test steps to view the preview faster (#15504),tr33k,196442433,closed,2025-04-25T13:53:09Z,2025-04-25T15:55:39Z,2025-04-25T15:55:39Z,https://api.github.com/repos/mlflow/mlflow,https://github.com/mlflow/mlflow/pull/15509,"<details><summary>&#x1F6E0 DevTools &#x1F6E0</summary>
<p>

[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/tr33k/mlflow/pull/15509?quickstart=1)

#### Install mlflow from this PR

```
# mlflow
pip install git+https://github.com/mlflow/mlflow.git@refs/pull/15509/merge
# mlflow-skinny
pip install git+https://github.com/mlflow/mlflow.git@refs/pull/15509/merge#subdirectory=skinny
```

For Databricks, use the following command:

```
%sh curl -LsSf https://raw.githubusercontent.com/mlflow/mlflow/HEAD/dev/install-skinny.sh | sh -s 15509
```

</p>
</details>

### Related Issues/PRs

<!-- Uncomment 'Resolve' if this PR can close the linked items. -->
Close #15504 

### What changes are proposed in this pull request?

Swaps the steps in .circleci/config.yml to upload the docs artifacts before running example tests, enabling faster doc previews.

<!-- Please fill in changes proposed in this PR. -->

### How is this PR tested?

- [ ] Existing unit/integration tests
- [ ] New unit/integration tests
- [x] Manual tests

<!-- Attach code, screenshot, video used for manual testing here. -->

### Does this PR require documentation update?

- [x] No. You can skip the rest of this section.
- [ ] Yes. I've updated:
  - [ ] Examples
  - [ ] API references
  - [ ] Instructions

### Release Notes

#### Is this a user-facing change?

- [x] No. You can skip the rest of this section.
- [ ] Yes. Give a description of this change to be included in the release notes for MLflow users.

<!-- Details in 1-2 sentences. You can just refer to another PR with a description if this PR is part of a larger change. -->

#### What component(s), interfaces, languages, and integrations does this PR affect?

Components

- [ ] `area/artifacts`: Artifact stores and artifact logging
- [x] `area/build`: Build and test infrastructure for MLflow
- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations
- [x] `area/docs`: MLflow documentation pages
- [ ] `area/examples`: Example code
- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry
- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors
- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates
- [ ] `area/projects`: MLproject format, project running backends
- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs
- [ ] `area/server-infra`: MLflow Tracking server backend
- [ ] `area/tracking`: Tracking Service, tracking client APIs, autologging

Interface

- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server
- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models
- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry
- [ ] `area/windows`: Windows support

Language

- [ ] `language/r`: R APIs and clients
- [ ] `language/java`: Java APIs and clients
- [ ] `language/new`: Proposals for new client languages

Integrations

- [ ] `integrations/azure`: Azure and Azure ML integrations
- [ ] `integrations/sagemaker`: SageMaker integrations
- [ ] `integrations/databricks`: Databricks integrations

<!--
Insert an empty named anchor here to allow jumping to this section with a fragment URL
(e.g. https://github.com/mlflow/mlflow/pull/123#user-content-release-note-category).
Note that GitHub prefixes anchor names in markdown with ""user-content-"".
-->

<a name=""release-note-category""></a>

#### How should the PR be classified in the release notes? Choose one:

- [x] `rn/none` - No description will be included. The PR will be mentioned only by the PR number in the ""Small Bugfixes and Documentation Updates"" section
- [ ] `rn/breaking-change` - The PR will be mentioned in the ""Breaking Changes"" section
- [ ] `rn/feature` - A new user-facing feature worth mentioning in the release notes
- [ ] `rn/bug-fix` - A user-facing bug fix worth mentioning in the release notes
- [ ] `rn/documentation` - A user-facing documentation change worth mentioning in the release notes

#### Should this PR be included in the next patch release?

`Yes` should be selected for bug fixes, documentation updates, and other small changes. `No` should be selected for new features and larger changes. If you're unsure about the release classification of this PR, leave this unchecked to let the maintainers decide.

<details>
<summary>What is a minor/patch release?</summary>

- Minor release: a release that increments the second part of the version number (e.g., 1.2.0 -> 1.3.0).
  Bug fixes, doc updates and new features usually go into minor releases.
- Patch release: a release that increments the third part of the version number (e.g., 1.2.0 -> 1.2.1).
  Bug fixes and doc updates usually go into patch releases.

</details>

<!-- patch -->

- [ ] Yes (this PR will be cherry-picked and included in the next patch release)
- [x] No (this PR will be included in the next minor release)
",Human,"analysisWe need to classify whether the primary purpose is to improve performance. The PR title: ""Swap doc preview and test steps to view the preview faster (#15504)"". The description: ""Swaps the steps in.circleci/config.yml to upload the docs artifacts before running example tests, enabling faster doc previews."" This is about CI steps to speed up doc preview. It's a performance improvement in CI pipeline. The keywords: faster, preview faster. So it's performance. So classification: performance.assistantfinalperformance"
2478962352,15481,Clean up __init__ file for some GenAI flavors,B-Step62,31463517,closed,2025-04-24T12:23:41Z,2025-04-25T01:36:38Z,2025-04-25T01:36:38Z,https://api.github.com/repos/mlflow/mlflow,https://github.com/mlflow/mlflow/pull/15481,"<details><summary>&#x1F6E0 DevTools &#x1F6E0</summary>
<p>

[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/B-Step62/mlflow/pull/15481?quickstart=1)

#### Install mlflow from this PR

```
# mlflow
pip install git+https://github.com/mlflow/mlflow.git@refs/pull/15481/merge
# mlflow-skinny
pip install git+https://github.com/mlflow/mlflow.git@refs/pull/15481/merge#subdirectory=skinny
```

For Databricks, use the following command:

```
%sh curl -LsSf https://raw.githubusercontent.com/mlflow/mlflow/HEAD/dev/install-skinny.sh | sh -s 15481
```

</p>
</details>

### What changes are proposed in this pull request?

The `__init__` file for flavors are massive. This is ok, but becomes a challenge for the lightweight tracing SDK, because doing `mlflow.openai.autolog()` imports everything defined in the `mlflow/openai/__init__.py`, including various model logging dependencies.

On the other hand, a good example is DSPy flavor, which organize code well and keep `__init__.py` clean.

This PR moves model logging code from `__init__.py` to `model.py` so we can later choose to import or not inside the `__init__.py`. This only updates three flavors that are problematic for tracing SDK: `openai`, `langchain`, and `llama_index`, but we can apply same to other flavors later.

**Note**: This PR targets to **mlflow-3** branch rather than the feature branch, because this will cause huge merge conflict otherwise.

### How is this PR tested?

- [x] Existing unit/integration tests
- [ ] New unit/integration tests
- [ ] Manual tests

<!-- Attach code, screenshot, video used for manual testing here. -->

### Does this PR require documentation update?

- [x] No. You can skip the rest of this section.
- [ ] Yes. I've updated:
  - [ ] Examples
  - [ ] API references
  - [ ] Instructions

### Release Notes

#### Is this a user-facing change?

- [ ] No. You can skip the rest of this section.
- [ ] Yes. Give a description of this change to be included in the release notes for MLflow users.

<!-- Details in 1-2 sentences. You can just refer to another PR with a description if this PR is part of a larger change. -->

#### What component(s), interfaces, languages, and integrations does this PR affect?

Components

- [ ] `area/artifacts`: Artifact stores and artifact logging
- [ ] `area/build`: Build and test infrastructure for MLflow
- [ ] `area/deployments`: MLflow Deployments client APIs, server, and third-party Deployments integrations
- [ ] `area/docs`: MLflow documentation pages
- [ ] `area/examples`: Example code
- [ ] `area/model-registry`: Model Registry service, APIs, and the fluent client calls for Model Registry
- [ ] `area/models`: MLmodel format, model serialization/deserialization, flavors
- [ ] `area/recipes`: Recipes, Recipe APIs, Recipe configs, Recipe Templates
- [ ] `area/projects`: MLproject format, project running backends
- [ ] `area/scoring`: MLflow Model server, model deployment tools, Spark UDFs
- [ ] `area/server-infra`: MLflow Tracking server backend
- [x] `area/tracking`: Tracking Service, tracking client APIs, autologging

Interface

- [ ] `area/uiux`: Front-end, user experience, plotting, JavaScript, JavaScript dev server
- [ ] `area/docker`: Docker use across MLflow's components, such as MLflow Projects and MLflow Models
- [ ] `area/sqlalchemy`: Use of SQLAlchemy in the Tracking Service or Model Registry
- [ ] `area/windows`: Windows support

Language

- [ ] `language/r`: R APIs and clients
- [ ] `language/java`: Java APIs and clients
- [ ] `language/new`: Proposals for new client languages

Integrations

- [ ] `integrations/azure`: Azure and Azure ML integrations
- [ ] `integrations/sagemaker`: SageMaker integrations
- [ ] `integrations/databricks`: Databricks integrations

<!--
Insert an empty named anchor here to allow jumping to this section with a fragment URL
(e.g. https://github.com/mlflow/mlflow/pull/123#user-content-release-note-category).
Note that GitHub prefixes anchor names in markdown with ""user-content-"".
-->

<a name=""release-note-category""></a>

#### How should the PR be classified in the release notes? Choose one:

- [x] `rn/none` - No description will be included. The PR will be mentioned only by the PR number in the ""Small Bugfixes and Documentation Updates"" section
- [ ] `rn/breaking-change` - The PR will be mentioned in the ""Breaking Changes"" section
- [ ] `rn/feature` - A new user-facing feature worth mentioning in the release notes
- [ ] `rn/bug-fix` - A user-facing bug fix worth mentioning in the release notes
- [ ] `rn/documentation` - A user-facing documentation change worth mentioning in the release notes

#### Should this PR be included in the next patch release?

`Yes` should be selected for bug fixes, documentation updates, and other small changes. `No` should be selected for new features and larger changes. If you're unsure about the release classification of this PR, leave this unchecked to let the maintainers decide.

<details>
<summary>What is a minor/patch release?</summary>

- Minor release: a release that increments the second part of the version number (e.g., 1.2.0 -> 1.3.0).
  Bug fixes, doc updates and new features usually go into minor releases.
- Patch release: a release that increments the third part of the version number (e.g., 1.2.0 -> 1.2.1).
  Bug fixes and doc updates usually go into patch releases.

</details>

<!-- patch -->

- [ ] Yes (this PR will be cherry-picked and included in the next patch release)
- [x] No (this PR will be included in the next minor release)
",Human,"analysisWe need to classify whether the primary purpose is to improve performance. The PR is about cleaning up __init__ files for some GenAI flavors, moving model logging code from __init__ to model.py to reduce imports for lightweight tracing SDK. This is about reducing import overhead, potentially improving performance of import time. But is that considered performance? The description: ""This PR moves model logging code from __init__.py to model.py so we can later choose to import or not inside the __init__.py. This only updates three flavors that are problematic for tracing SDK: openai, langchain, and llama_index."" The goal is to reduce import overhead for tracing SDK. That is a performance improvement: reduce import time, memory usage. The keywords: ""performance"" not explicitly present, but the intent is to improve performance. The classification guidelines: performance-related PRs may contain keywords such as respons, react, wait, responsive, throughput, slow, performance, optimization, latency, compile-time-hog, fast, memory, stuck, leak, hang, perf, minor, speed, perform, lag, latenc, tim, instant, or any other terms referring to how faster and efficiently software runs. This PR does not contain those keywords. But the description indicates performance improvement. The guidelines say ""may contain keywords"" but not mandatory. We need to decide if it's performance. The primary purpose is to improve performance by reducing import overhead. That is a performance improvement. So classify as 'performance'.

Thus answer: performance.assistantfinalperformance"
2590261382,251382,Optimized concat with reduce,Connormiha,4091305,open,2025-06-13T13:20:06Z,,,https://api.github.com/repos/microsoft/vscode,https://github.com/microsoft/vscode/pull/251382,"<!-- Thank you for submitting a Pull Request. Please:
* Read our Pull Request guidelines:
  https://github.com/microsoft/vscode/wiki/How-to-Contribute#pull-requests
* Associate an issue with the Pull Request.
* Ensure that the code is up-to-date with the `main` branch.
* Include a description of the proposed changes and how to test them.
-->
The `concat` method creates a new collection each time based on the existing one. Using `push` in this case saves resources.",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Optimized concat with reduce"". Body: ""The concat method creates a new collection each time based on the existing one. Using push in this case saves resources."" This is about performance optimization. Keywords: optimized, saves resources. So classification: performance.assistantfinalperformance"
2445560431,245969,ci: cross compile alpine arm64 cli,deepak1556,964386,closed,2025-04-08T10:48:14Z,2025-04-08T12:24:40Z,2025-04-08T12:24:40Z,https://api.github.com/repos/microsoft/vscode,https://github.com/microsoft/vscode/pull/245969,"Verification build https://monacotools.visualstudio.com/Monaco/_build/results?buildId=330675&view=results

CLI stage is now at 9m from previous 17m üöÄ ",Human,"analysisWe need to classify whether the primary purpose of the PR is to improve performance. The title: ""ci: cross compile alpine arm64 cli"". Body: ""Verification build https://monacotools.visualstudio.com/Monaco/_build/results?buildId=330675&view=results

CLI stage is now at 9m from previous 17m üöÄ""

This indicates that the CLI stage time improved from 17m to 9m. So it's a performance improvement. The PR is about cross compile alpine arm64 cli, but the body indicates performance improvement. The classification: 'performance'.assistantfinalperformance"
2488291392,247726,fixes https://github.com/microsoft/vscode/issues/247666,jrieken,1794099,closed,2025-04-29T09:36:49Z,2025-04-29T09:49:37Z,2025-04-29T09:49:37Z,https://api.github.com/repos/microsoft/vscode,https://github.com/microsoft/vscode/pull/247726,,Human,"analysisWe need to classify the PR as performance or non-performance. The title: ""fixes https://github.com/microsoft/vscode/issues/247666"". No body provided. We need to infer from the issue link. Let's check the issue number 247666 on vscode. I don't have internet, but maybe recall? Issue 247666 might be about performance? Let's think: vscode issue 247666. Could be about ""Performance: Editor shows flicker"" or ""Slow opening of large files"". But not sure. We need to decide. The title just says ""fixes https://github.com/microsoft/vscode/issues/247666"". That suggests the PR addresses that issue. We need to determine if the primary purpose is performance improvement. Without more context, we can't be sure. But we can guess: The issue number 247666 might be about ""Performance: Editor is slow when opening large files"" or ""Performance: Search is slow"". But we need to decide. Let's try to recall: I think issue 247666 might be about ""Performance: VS Code is slow when opening large files"" or ""Performance: Editor is slow when scrolling"". But I'm not certain.

Alternatively, maybe the issue is about ""Performance: Editor is stuck when opening large files"" or ""Performance: Editor is slow when opening large files"". The PR might fix that. So likely performance.

But we need to be careful: The classification is based on the primary purpose. If the PR is about fixing a bug, not performance, then it's non-performance. But the issue might be about performance. Let's try to recall: I think issue 247666 is about ""Performance: VS Code is slow when opening large files"" or ""Performance: Editor is slow when opening large files"". Actually, I think I saw an issue about ""Performance: VS Code is slow when opening large files"" with number 247666. But I'm not 100% sure.

Alternatively, maybe it's about ""Performance: VS Code is slow when opening large files"" but the PR might fix that. So classification: performance.

Given the guidelines: performance-related PRs may contain keywords like performance, optimization, etc. The title doesn't contain those. But the issue link might be about performance. But we don't have the body. The classification is based on the primary purpose. If the PR is to fix a performance issue, it's performance. So I'd classify as performance.

Thus answer: performance.assistantfinalperformance"
2476449844,5179,`@remotion/media-parser`: Preload subsequent m3u8 chunks,JonnyBurger,1629785,closed,2025-04-23T14:37:28Z,2025-04-23T14:52:37Z,2025-04-23T14:52:37Z,https://api.github.com/repos/remotion-dev/remotion,https://github.com/remotion-dev/remotion/pull/5179,prefetch cache,Human,"analysisWe need to classify whether the primary purpose is to improve performance. The PR title: ""@remotion/media-parser"": Preload subsequent m3u8 chunks prefetch cache. This seems to be about preloading subsequent m3u8 chunks, likely to improve performance by reducing buffering or latency. The description: ""prefetch cache"". That suggests performance improvement. So classification: performance.assistantfinalperformance"
2282760170,4750,`@remotion/media-parser`: Refactor ISO Base Media and Matroska Parser to be ready for seeking,JonnyBurger,1629785,closed,2025-01-16T21:04:50Z,2025-01-17T15:37:14Z,2025-01-17T15:37:14Z,https://api.github.com/repos/remotion-dev/remotion,https://github.com/remotion-dev/remotion/pull/4750,"TODO: Now tests / CIs are freezing

Array buffer resize is too big",Human,"analysisWe need to classify whether the primary purpose is performance. The title: ""Refactor ISO Base Media and Matroska Parser to be ready for seeking"". Body: ""TODO: Now tests / CIs are freezing Array buffer resize is too big"". The content mentions ""Array buffer resize is too big"". That suggests performance issue? The refactor is to be ready for seeking, maybe performance improvement. The body mentions tests freezing, array buffer resize too big. That could be performance. The keywords: ""performance"" not present. But ""Array buffer resize is too big"" indicates memory usage. The refactor might be to improve performance. The question: primary purpose of the PR is to improve performance? The title: ""Refactor ISO Base Media and Matroska Parser to be ready for seeking"". That is about seeking, maybe performance. The body: ""Array buffer resize is too big"" indicates memory usage. So likely performance. So classify as 'performance'.assistantfinalperformance"
2298019664,4805,`@remotion/media-parser`: Ability to test remote files fast,JonnyBurger,1629785,closed,2025-01-25T07:34:00Z,2025-01-25T08:07:02Z,2025-01-25T08:07:02Z,https://api.github.com/repos/remotion-dev/remotion,https://github.com/remotion-dev/remotion/pull/4805,`@remotion/media-parser`: Ability to test remote files fast,Human,"analysisWe need to classify the pull request as performance or non-performance. The title: ""@remotion/media-parser: Ability to test remote files fast"". The body is repeated same line. The phrase ""fast"" indicates performance. The description: ""Ability to test remote files fast"". That suggests improving performance of testing remote files. So it's performance. So output: performance.assistantfinalperformance"
2511998499,5261,Three.js template: use `useOffthreadVideoTexture()` instead of `useVideoTexture()`,JonnyBurger,1629785,closed,2025-05-11T08:14:22Z,2025-05-11T08:25:27Z,2025-05-11T08:25:27Z,https://api.github.com/repos/remotion-dev/remotion,https://github.com/remotion-dev/remotion/pull/5261,"Fixes hangs with higher concurrency

Fixes #13",Human,"analysisWe need to classify whether the primary purpose is performance improvement. The PR title: ""Three.js template: use `useOffthreadVideoTexture()` instead of `useVideoTexture()` Fixes hangs with higher concurrency"". The body: ""Fixes #13"". The description: using off-thread video texture instead of video texture. That likely improves performance or prevents hangs. The keywords: ""hangs"", ""higher concurrency"". The mention of ""hangs"" indicates performance issue. So it's performance. So classification: performance.assistantfinalperformance"
2495170138,5222,`@remotion/renderer`: Consider `--memory` flag from Docker and warn on unrealistic values,JonnyBurger,1629785,closed,2025-05-02T08:14:20Z,2025-05-02T08:36:26Z,2025-05-02T08:36:26Z,https://api.github.com/repos/remotion-dev/remotion,https://github.com/remotion-dev/remotion/pull/5222,Also allow 50% of memory instead of just 40%,Human,"analysisWe need to classify the pull request as performance or non-performance. The title: ""`@remotion/renderer`: Consider `--memory` flag from Docker and warn on unrealistic values Also allow 50% of memory instead of just 40%"". Body likely about memory flag, warning unrealistic values, allow 50% memory. This is about memory usage, performance? It's about memory usage, but is it performance? The instruction: performance-related PRs may contain keywords such as respons, react, wait, responsive, throughput, slow, performance, optimization, latency, compile-time-hog, fast, memory, stuck, leak, hang, perf, minor, speed, perform, lag, latenc, tim, instant, or any other terms referring to how faster and efficiently software runs.

Memory is included. So this PR is about memory usage, which is a performance aspect. So classify as 'performance'.assistantfinalperformance"
2386598200,2052,Compress SQL-format backups during transmission,otoolep,536312,closed,2025-03-12T00:16:59Z,2025-03-12T00:54:39Z,2025-03-12T00:54:39Z,https://api.github.com/repos/rqlite/rqlite,https://github.com/rqlite/rqlite/pull/2052,https://github.com/rqlite/rqlite/issues/2050,Human,"analysisWe need to classify the pull request as performance or non-performance. The title: ""Compress SQL-format backups during transmission"". This is about compressing backups during transmission. That likely improves performance (network throughput, speed). It's about reducing data size, improving transfer speed. So it's performance. So output ""performance"".assistantfinalperformance"
2601475792,1752,Implement readStrict option for Table configuration,jimmyn,5391775,closed,2025-06-18T11:46:13Z,2025-06-18T11:46:42Z,,https://api.github.com/repos/dynamoose/dynamoose,https://github.com/dynamoose/dynamoose/pull/1752,"This PR implements the readStrict option for Table configuration to control how strictly items are validated when reading from DynamoDB.

**Key Changes:**
- Added readStrict option to TableOptions with default value false
- Modified ItemObjectFromSchemaSettings to include readStrict parameter
- Updated Item.conformToSchema to skip expensive validation when readStrict is false
- Updated ItemRetriever, Model, and Transaction classes to pass readStrict option
- Added tests for both readStrict: true and readStrict: false behaviors

**Behavior:**
- When readStrict: true (maintains backward compatibility): Unknown properties from DynamoDB are filtered out
- When readStrict: false (new behavior): Unknown properties from DynamoDB are preserved in the item

This allows for better performance when reading from DynamoDB by skipping unnecessary validation while still applying essential transformations like getters and custom type conversions.

**Background:**
This feature was previously committed directly to main (commit e383cc73) but was reverted (commit bdb8fc1e) to follow proper PR workflow. This PR contains the same implementation for proper review and testing.",Human,"analysisWe need to classify whether the primary purpose of the PR is to improve performance. The PR implements readStrict option to skip expensive validation when reading from DynamoDB. That is a performance improvement. The description says ""This allows for better performance when reading from DynamoDB by skipping unnecessary validation while still applying essential transformations."" So it's performance. So classification: 'performance'.assistantfinalperformance"
2444973821,2180,"Change NetTrace format version support, eliminate pinning, and improve UnsupportedVersionException",noahfalk,6243776,closed,2025-04-08T06:27:26Z,2025-04-09T21:52:34Z,2025-04-09T21:52:34Z,https://api.github.com/repos/microsoft/perfview,https://github.com/microsoft/perfview/pull/2180,"
- Re-enabling support for parsing format V6 and re-enabling the tests
- Removed support for ancient netperf format version 1 and 2 that were only used during .NET Core 2.0 (not even 2.1).
- The implementation of the Nettrace reader was doing a bunch of pinning because the TraceEvent internal model of events uses unmanaged pointers, however the pinning was creating bad GC performance. TraceEvent builds for .NET standard so we don't have guaranteed access to the pinned heap and I didn't want to start doing lots of copying and/or managing pools of pinned arrays. Instead I switched the stream reading over to using spans backed by native heap allocations and explicit block lifetime tracking. As a side benefit it also brings the V3-5 block iteration code a bit closer to the V6 block iteration and removed all the dependencies on the existing FastSerialization library.
- Add MinSupportedVersion and MaxSupportedVersion to the UnsupportedVersionException




@wiktork - This may fix the GC issues you were seeing in dotnet-monitor
@karpinsn - Exception improvements should help with version error messages in Dev18

@mdh1418, @brianrob, @lateralusX, @beaubelgrave - aside from turning back on V6 support hopefully the behavior should be nearly identical to before despite all the code churn in there.
",Human,"analysisWe need to classify as performance or non-performance. The PR description: ""Change NetTrace format version support, eliminate pinning, and improve UnsupportedVersionException"". It mentions that the implementation of the Nettrace reader was doing a bunch of pinning because the TraceEvent internal model uses unmanaged pointers, but the pinning was creating bad GC performance. They switched to using spans backed by native heap allocations and explicit block lifetime tracking. As a side benefit it also brings the V3-5 block iteration code a bit closer to the V6 block iteration and removed dependencies. So this is about performance: eliminating pinning, GC performance, using spans, etc. So it's performance. Also mention ""bad GC performance"". So classification: performance.assistantfinalperformance"
2311291619,2159,Fixes that repeatedly adding unhandled event handler to TraceLog to real time session conversion,wwh1004,27536652,closed,2025-02-02T15:32:13Z,2025-02-03T22:05:34Z,2025-02-03T22:05:34Z,https://api.github.com/repos/microsoft/perfview,https://github.com/microsoft/perfview/pull/2159,Before this pr the unhandled event handler will be registered repeatedly as long as DispatchClonedEvent is called causing very high cpu usage,Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Fixes that repeatedly adding unhandled event handler to TraceLog to real time session conversion"". Body: ""Before this pr the unhandled event handler will be registered repeatedly as long as DispatchClonedEvent is called causing very high cpu usage"". This is about high CPU usage due to repeated registration. That is a performance issue. So classification: performance.assistantfinalperformance"
2505641263,2197,More efficient build in PRs,rainersigwald,3347530,closed,2025-05-07T23:06:23Z,2025-05-07T23:24:36Z,2025-05-07T23:24:36Z,https://api.github.com/repos/microsoft/perfview,https://github.com/microsoft/perfview/pull/2197,"Enable amd64 MSBuild, in multiproc mode, and include the NuGet restore in a single step.

(this time with a name that shouldn't anger GitHub: https://github.com/microsoft/perfview/pull/2191#issuecomment-2859426506).",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""More efficient build in PRs"". Body: ""Enable amd64 MSBuild, in multiproc mode, and include the NuGet restore in a single step."" This is about build efficiency, compile time. It's about performance of build process. So it's performance. The guidelines mention performance-related keywords like ""performance"", ""optimization"", ""speed"", ""fast"", ""compile-time-hog"". This is about compile time. So classify as 'performance'.assistantfinalperformance"
2454459441,1745,Add MatMulAddGemm Graph Surgery,gnedanur,2918273,closed,2025-04-11T18:04:56Z,2025-04-11T20:04:21Z,,https://api.github.com/repos/microsoft/Olive,https://github.com/microsoft/Olive/pull/1745,"## Describe your changes
Add MatMulAddGemm Graph Surgery to transform Add Op followed by Matmul as Gemm op

## Checklist before requesting a review
- [x] Add unit tests for this change.
- [x] Make sure all tests can pass.
- [ ] Update documents if necessary.
- [ ] Lint and apply fixes to your code by running `lintrunner -a`
- [ ] Is this a user-facing change? If yes, give a description of this change to be included in the release notes.
- [ ] Is this PR including examples changes? If yes, please remember to update [example documentation](https://github.com/microsoft/Olive/blob/main/docs/source/examples.md) in a follow-up PR.

## (Optional) Issue link
",Human,"analysisWe need to classify whether the primary purpose is performance. The PR adds MatMulAddGemm Graph Surgery to transform Add Op followed by Matmul as Gemm op. This is a graph surgery to combine operations, likely for performance optimization. The description: ""Add MatMulAddGemm Graph Surgery to transform Add Op followed by Matmul as Gemm op"". This is a performance improvement: combining add+matmul into gemm reduces ops, improves performance. So classification: performance.assistantfinalperformance"
2508301423,19387,Add inMemoryDataProcessingThreshold for filtering/sorting,cssuh,18150417,closed,2025-05-08T23:30:13Z,2025-05-12T17:07:51Z,2025-05-12T17:07:51Z,https://api.github.com/repos/microsoft/vscode-mssql,https://github.com/microsoft/vscode-mssql/pull/19387,"Adds a threshold to disable filtering/sorting for large data sets.  The default is set at 5000 rows.  If a user tries to filter/sort for data sets larger than this amount, we will display this toast:
<img width=""453"" alt=""Screenshot 2025-05-08 at 4 09 36‚ÄØPM"" src=""https://github.com/user-attachments/assets/7b00eb64-ae4c-4bd7-b197-5f3f9ed33d87"" />

Also cleaned up some code that was no longer needed.",Human,"analysisWe need to classify as performance or non-performance. The PR adds inMemoryDataProcessingThreshold to disable filtering/sorting for large data sets, default 5000 rows. This is to avoid performance issues? It is about disabling filtering/sorting for large data sets to avoid performance problems. The description: ""Adds a threshold to disable filtering/sorting for large data sets. If a user tries to filter/sort for data sets larger than this amount, we will display this toast."" This is about performance: preventing slow operations on large data sets. So it's performance. Also cleaned up code. So classification: performance.assistantfinalperformance"
2269709704,218,"Speed up, reduce memory usage of file reading",jakebailey,5341706,closed,2025-01-10T05:04:13Z,2025-01-10T21:22:00Z,,https://api.github.com/repos/microsoft/typescript-go,https://github.com/microsoft/typescript-go/pull/218,"Our FS abstraction works with strings; rather than reading the file into `[]byte` then copying it into a string, we can read the file directly into a correctly-sized `strings.Builder`.

Also, now that we have a string, we can instead check for BOMs with strings, which is faster as the Go compiler optimizes those reads/comparisons.",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Speed up, reduce memory usage of file reading"". Body describes reading file directly into strings.Builder, reducing memory usage, faster. So it's performance. So output 'performance'.assistantfinalperformance"
2369320781,405,Reduce runtime/allocations during test run,jakebailey,5341706,closed,2025-03-03T23:33:42Z,2025-03-04T00:47:08Z,2025-03-04T00:47:08Z,https://api.github.com/repos/microsoft/typescript-go,https://github.com/microsoft/typescript-go/pull/405,"These are some of the top offenders for memory allocation and runtime. The collection of these brings the runtime of the runner from 6.5s to 6s on my machine, a 7.7% improvement.",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Reduce runtime/allocations during test run"". Body: ""These are some of the top offenders for memory allocation and runtime. The collection of these brings the runtime of the runner from 6.5s to 6s on my machine, a 7.7% improvement.""

Clearly performance improvement. So label 'performance'.assistantfinalperformance"
2555753483,11934,update to stop closures from lazy functions and linq,ttstanley,62913657,closed,2025-05-30T21:45:28Z,2025-06-06T19:07:40Z,2025-06-06T19:07:40Z,https://api.github.com/repos/dotnet/msbuild,https://github.com/dotnet/msbuild/pull/11934,"Fixes : Allocation issue.

### Context
Looking at a trace of allocations. It was shown that some of the allocations were coming from closures. This pr addresses the closures found.

### Changes Made
* Removed lazy from exclude tester function since it was not needed since lifetime of lazy object was within the method itself.
* switched from linq clause for add range to manually adding items, because the linq version caused a closure from a method it did not have context with.

### Testing
Used ILSpy to verify that the closures disappeared. (DisplayClass represents a closure)
Before
![image](https://github.com/user-attachments/assets/123f3b9f-4d2e-4f1d-bcba-2dcbe9144741)

Afterwards
![image](https://github.com/user-attachments/assets/9a7b9f91-1466-4de9-a0ff-579e0d5407bc)



### Notes
",Human,"analysisWe need to classify as performance or non-performance. The PR is about removing lazy functions and linq to stop closures from lazy functions and linq. It addresses allocation issue. It removes closures to reduce allocations. That is a performance improvement: reducing allocations, memory usage. The description: ""Fixes: Allocation issue."" So it's performance. So label 'performance'.assistantfinalperformance"
2461525402,5929,Add the framework required to run component stress tests,hectahertz,24622853,closed,2025-04-15T19:11:08Z,2025-04-22T18:03:31Z,2025-04-22T18:03:31Z,https://api.github.com/repos/primer/react,https://github.com/primer/react/pull/5929,"<!-- Provide the GitHub issue that this issue closes. Start typing the number or name of the issue after the # below. -->

<!-- Provide an overview of the changes, including before/after screenshots, videos, or graphs when helpful -->

### Changelog

<!-- Under the headings below, list out relevant API changes that this Pull Request introduces -->

#### New

This PR adds the framework required to run component stress tests and a couple of examples.

The tests run on Storybook and measure the time it takes for a component to react to a state change.

This is how one of them looks like:

https://github.com/user-attachments/assets/c29c7ad0-ed02-4468-96b5-ab1f6c4634e4

I tried to make the code self explanatory but I'll give a brief overview of how the pieces fit together here.

`StressTest.tsx` is a helper component to be used in Storybook, that takes care of all the performance measuring part of the stress tests. It has a very simple API so that defining the tests is as simple as possible.
It uses `afterframe` and a React state update to do this. We trigger a state update, which the component will react to and rerender appropriately according to the test defined. The afterFrame library calls the function when the next frame starts and measures the elapsed time. The reasoning behind this approach is well explained [in this article](https://3perf.com/blog/react-monitoring/#why-use-afterframe).

`[Component].stress.stories.tsx` contain the stress tests for a few components. The components will react to the state change by interacting with the `count` prop provided in the render function. This reaction is what the stress test will measure.

`stress-tests.yml` defines the workflow that runs the stress tests. It runs the Playwright tests against the Storybook server and stores the results in a file. This file is stored in the cache and compared to the previous results. Each commit will generate a new result in the cache.

If the result of any commit is slower than the alert threshold defined there, the workflow will fail and a comment will be added to the commit with the results.

<img width=""937"" alt=""Screenshot 2025-04-15 at 20 46 39"" src=""https://github.com/user-attachments/assets/2796bb47-2b38-4427-b152-0b1dccb9bb21"" />

`StressTests.test.ts` is where the Playwright tests are defined. The tests just run the Storybook story and grab the mean result from them.

`stress-test-reporter.ts` gets the results from the Playwright tests and stores them in a file, in a format that `benchmark-action/github-action-benchmark` expects. 

### FAQ

#### How much of a stable environment do you need for this to be reliable?

So far, defining `runs-on: ubuntu-latest-4-cores` on the workflow has been very reliable. Running the tests locally is also very consistent from run to run but, as you can imagine, it will only be useful to compare numbers between runs on the same hardware.

#### How accurate is it at detecting regressions?

Very accurate according to my tests. Here's an example of enabling and disabling CSS modules on ActionList:

https://github.com/user-attachments/assets/d67231a3-9a04-4b05-98d7-a942a6da912b

### Rollout strategy

<!-- How do you recommend this change to be rolled out? Refer to [contributor docs on Versioning](https://github.com/primer/react/blob/main/contributor-docs/versioning.md) for details. -->

- [ ] Patch release
- [X] Minor release
- [ ] Major release; if selected, include a written rollout or migration plan
- [ ] None; if selected, include a brief description as to why

### Testing & Reviewing

<!-- Describe any specific details to help reviewers test or review this Pull Request -->

### Merge checklist

- [X] Added/updated tests
- [ ] Added/updated documentation
- [X] Added/updated previews (Storybook)
- [ ] Changes are [SSR compatible](https://github.com/primer/react/blob/main/contributor-docs/CONTRIBUTING.md#ssr-compatibility)
- [X] Tested in Chrome
- [X] Tested in Firefox
- [ ] Tested in Safari
- [ ] Tested in Edge
- [ ] (GitHub staff only) Integration tests pass at github/github ([Learn more about how to run integration tests](https://github.com/github/primer-engineering/blob/main/how-we-work/testing-primer-react-pr-at-dotcom.md))

<!-- Take a look at the [What we look for in reviews](https://github.com/primer/react/blob/main/contributor-docs/CONTRIBUTING.md#what-we-look-for-in-reviews) section of the contributing guidelines for more information on how we review PRs. -->
",Human,"analysisWe need to classify as 'performance' or 'non-performance'. The PR adds framework for component stress tests, measuring time to react to state change, performance measuring. It's clearly performance-related. So label 'performance'.assistantfinalperformance"
2392838218,14428,Enable parallel C++ compilation,JunielKatarn,4507319,closed,2025-03-14T06:53:41Z,2025-03-14T23:50:57Z,2025-03-14T23:50:57Z,https://api.github.com/repos/microsoft/react-native-windows,https://github.com/microsoft/react-native-windows/pull/14428,"## Description

Allow compiling multiple C++ sources in parallel.

### Type of Change
- New feature (non-breaking change which adds functionality)

### Why
Build times in RNW have jumped to between 35 minutes and over 60 minutes in Continuous Integration.

### What
- Default MSBuild property `MultiProcCL` to `true`.

Initial tests (cached NuGet packages, clean build and target directories) showed the following results on a Windows Dev Box:
- `MultiProcCL = false`: ~37 minutes
- `MultiProcCL = true`: ~6 minutes

About **82%** speed-up.

## Note

Should this setting apply to Continuous Integration?\
While it would drastically improve build times, it may compromise predictability and reproducibility of builds.

 ###### Microsoft Reviewers: [Open in CodeFlow](https://microsoft.github.io/open-pr/?codeflow=https://github.com/microsoft/react-native-windows/pull/14428)",Human,analysisWe need to classify as 'performance' or 'non-performance'. The PR is about enabling parallel C++ compilation to reduce build times. That is a performance improvement. So label 'performance'.assistantfinalperformance
2307815455,112016,JIT: Replace `fgMoveHotJumps` with 3-opt utility,amanasifkhalid,58230338,closed,2025-01-30T21:57:18Z,2025-02-20T04:33:34Z,2025-02-20T04:33:34Z,https://api.github.com/repos/dotnet/runtime,https://github.com/dotnet/runtime/pull/112016,"Part of #107749. Based on the plans outlined in #111989 ([comment](https://github.com/dotnet/runtime/pull/111989#issuecomment-2625143840)), we want to remove phases that prematurely tweak the initial layout fed into 3-opt; `fgMoveHotJumps` is one such phase. However, initial attempts to remove it incurred large size increases on x86/x64, suggesting there was some utility in moving blocks closer to their hottest successors to keep the layout compact. To avoid derailing my consolidation plan, I've decided to refactor `fgMoveHotJumps` into a utility for 3-opt to use. For now, we will continue to use this pass to try to keep the layout compact. In the future, this functionality may be useful for churning the initial layout into 3-opt to discover new local-optimal layouts.",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""JIT: Replace `fgMoveHotJumps` with 3-opt utility"". Body: about removing phases that tweak layout, refactor to utility for 3-opt, keep layout compact. This is about code layout optimization, which is performance. It mentions ""size increases"" and ""layout compact"". It's about JIT optimization, likely performance. So classify as 'performance'.assistantfinalperformance"
2452691617,114517,[mono][interp] Avoid doing extra lookups when not needed,lewing,24063,closed,2025-04-11T02:17:22Z,2025-04-11T16:27:32Z,2025-04-11T16:27:32Z,https://api.github.com/repos/dotnet/runtime,https://github.com/dotnet/runtime/pull/114517,micro optimization to reduce the cost of not finding a match for an intrinsic that has a name match but an invalid signature.,Human,"analysisWe need to classify as performance or non-performance. The PR title: ""[mono][interp] Avoid doing extra lookups when not needed"". Body: ""micro optimization to reduce the cost of not finding a match for an intrinsic that has a name match but an invalid signature."" This is a micro optimization to reduce cost of lookups. It's about performance. So classify as 'performance'.assistantfinalperformance"
2622581875,117071,Reduce HTTP headers validation overhead,MihaZupan,25307628,open,2025-06-26T23:02:39Z,,,https://api.github.com/repos/dotnet/runtime,https://github.com/dotnet/runtime/pull/117071,"When adding/reading headers where we don't have a special parser, ""parsing"" only validates that there are no new lines in the value. This change special-cases this (common) case and avoids allocating the `HeaderStoreItemInfo`.

Existing code paths where only non-validating APIs are used stay the same.
Overhead for cases of reading headers with validation, where they were added without validation, and we do have a known parser (still common) is minimal and an acceptable tradeoff IMO (extra branch).

| Method          | Toolchain | Mean      | Error    | Ratio | Allocated | Alloc Ratio |
|---------------- |---------- |----------:|---------:|------:|----------:|------------:|
| Add             | main      |  44.31 ns | 0.386 ns |  1.00 |      32 B |        1.00 |
| Add             | pr        |  21.35 ns | 0.018 ns |  0.48 |         - |        0.00 |
|                 |           |           |          |       |           |             |
| AddEnumerable   | main      |  36.27 ns | 0.592 ns |  1.00 |      32 B |        1.00 |
| AddEnumerable   | pr        |  28.80 ns | 0.265 ns |  0.79 |         - |        0.00 |
|                 |           |           |          |       |           |             |
| GetValues       | main      |  92.39 ns | 0.302 ns |  1.00 |      64 B |        1.00 |
| GetValues       | pr        |  40.35 ns | 0.090 ns |  0.44 |      32 B |        0.50 |
|                 |           |           |          |       |           |             |
| AddAndGetValues | main      |  94.01 ns | 0.192 ns |  1.00 |      64 B |        1.00 |
| AddAndGetValues | pr        |  42.92 ns | 0.229 ns |  0.46 |      32 B |        0.50 |
|                 |           |           |          |       |           |             |
| CloneHeaders    | main      | 905.95 ns | 1.891 ns |  1.00 |    1112 B |        1.00 |
| CloneHeaders    | pr        | 490.74 ns | 1.064 ns |  0.54 |     600 B |        0.54 |

<details>
<summary>Benchmark code</summary>

```c#
BenchmarkRunner.Run<HeadersBench>(args: args);

[MemoryDiagnoser(false)]
public class HeadersBench
{
    private readonly HttpResponseHeaders _headers = new HttpResponseMessage().Headers;
    private readonly HttpRequestHeaders _headersToClone = new HttpRequestMessage().Headers;
    private readonly string[] _fooAsArray = [""Foo""];

    public HeadersBench()
    {
        _headersToClone.TryAddWithoutValidation(""priority"", ""u=0, i"");
        _headersToClone.TryAddWithoutValidation(""sec-ch-ua-mobile"", ""?0"");
        _headersToClone.TryAddWithoutValidation(""sec-ch-ua-platform"", ""\""Windows\"""");
        _headersToClone.TryAddWithoutValidation(""sec-fetch-dest"", ""document"");
        _headersToClone.TryAddWithoutValidation(""sec-fetch-mode"", ""navigate"");
        _headersToClone.TryAddWithoutValidation(""sec-fetch-site"", ""none"");
        _headersToClone.TryAddWithoutValidation(""sec-fetch-user"", ""?1"");
        _headersToClone.TryAddWithoutValidation(""upgrade-insecure-requests"", ""1"");
    }

    [Benchmark]
    public void Add()
    {
        _headers.Add(""X-Custom"", ""Foo"");
        _headers.Clear();
    }

    [Benchmark]
    public void AddEnumerable()
    {
        _headers.Add(""X-Custom"", _fooAsArray);
        _headers.Clear();
    }

    [Benchmark]
    public object GetValues()
    {
        _headers.TryAddWithoutValidation(""X-Custom"", ""Foo"");
        IEnumerable<string> values = _headers.GetValues(""X-Custom"");
        _headers.Clear();
        return values;
    }

    [Benchmark]
    public object AddAndGetValues()
    {
        _headers.Add(""X-Custom"", ""Foo"");
        IEnumerable<string> values = _headers.GetValues(""X-Custom"");
        _headers.Clear();
        return values;
    }

    [Benchmark]
    public HttpRequestHeaders CloneHeaders()
    {
        HttpRequestHeaders newHeaders = new HttpRequestMessage().Headers;

        foreach (KeyValuePair<string, IEnumerable<string>> header in _headersToClone)
        {
            newHeaders.Add(header.Key, header.Value);
        }

        return newHeaders;
    }
}
```

</details>",Human,"analysisWe need to classify as 'performance' or 'non-performance'. The PR title: ""Reduce HTTP headers validation overhead"". Body describes optimization: special-casing common case, avoiding allocation, benchmarks show performance improvements. Keywords: overhead, validation, allocation, performance, speed, etc. So it's performance.assistantfinalperformance"
2309904375,112047,WasmAppBuilder: Remove double computation of a value,filipnavara,1764393,closed,2025-01-31T19:38:16Z,2025-01-31T22:40:48Z,2025-01-31T22:40:48Z,https://api.github.com/repos/dotnet/runtime,https://github.com/dotnet/runtime/pull/112047,Credit goes to https://pvs-studio.com/en/blog/posts/csharp/1216/,Human,"analysisWe need to classify whether the primary purpose is performance improvement. The title: ""WasmAppBuilder: Remove double computation of a value"". That suggests removing redundant computation, likely improving performance. The body: ""Credit goes to https://pvs-studio.com/en/blog/posts/csharp/1216/"". No other details. The change is removing double computation, which is a performance optimization. So classify as 'performance'.assistantfinalperformance"
2281459728,32578,[Cosmos] Query pipeline rewrite,topshot99,113669638,closed,2025-01-16T10:34:13Z,2025-01-29T16:37:57Z,2025-01-29T16:37:56Z,https://api.github.com/repos/Azure/azure-sdk-for-js,https://github.com/Azure/azure-sdk-for-js/pull/32578,"### Packages impacted by this PR
@azure/cosmos

### Issues associated with this PR


### Describe the problem that is addressed by this PR
The current query pipeline lacks optimization for parallel execution. This PR introduces a new flag, enableQueryControl, which, when set to true, allows intermittent results to be sent to client applications. This provides greater control to the clients and eliminates the need to scan the entire container.
Document [Link](https://loop.cloud.microsoft/p/eyJ1IjoiaHR0cHM6Ly9taWNyb3NvZnQuc2hhcmVwb2ludC5jb20vc2l0ZXMvMTNlZjNmMTItMTBjMi00ZmUxLWE1NzItODJmNTM2YTZiZjk2P25hdj1jejBsTWtaemFYUmxjeVV5UmpFelpXWXpaakV5TFRFd1l6SXROR1psTVMxaE5UY3lMVGd5WmpVek5tRTJZbVk1Tmlaa1BXSWhRbVl3U0Y5MlVUZzNWWFZWU0RJd2MwcHpMVk56ZWtVM2RYaENWamhKTlVWdFNGZHhMV2xXUTI1Q1ZETm9NM1IzZFdKU1YxTnlRVTR5ZGpoQ1dFY3phU1ptUFRBeFNFNUZSRVZGVjBoQldGSldSbFJGV2tsV1IwdFRURnBHUWt4U1NVUlFSa2NtWXowbE1rWW1abXgxYVdROU1TWmhQVXh2YjNCQmNIQW1jRDBsTkRCbWJIVnBaSGdsTWtac2IyOXdMWEJoWjJVdFkyOXVkR0ZwYm1WeUpuZzlKVGRDSlRJeWR5VXlNaVV6UVNVeU1sUXdVbFJWU0hoMFlWZE9lV0l6VG5aYWJsRjFZekpvYUdOdFZuZGlNbXgxWkVNMWFtSXlNVGhaYVVaRFdtcENTVmd6V2xKUFJHUldaRlpXU1UxcVFucFRiazEwVlROT05sSlVaREZsUlVwWFQwVnJNVkpYTVVsV00wVjBZVlphUkdKclNsVk5NbWQ2WkVoa01WbHNTbGhWTTBwQ1ZHcEtNazlGU2xsU2VrNXdaa1JCZUZORk5VWlNSVlpHVlZaR1ExRlZkRlZUVlVwWlZsVjBZVkY2VGxWU2VrcEZVMVYwVVZFd2VFTlZSV3NsTTBRbE1qSWxNa01sTWpKcEpUSXlKVE5CSlRJeVlUbGpOakV5TTJFdE9XSmhOeTAwT0RVeUxUazFNelV0TlRJd1lXSTBNVEE1WWpneUpUSXlKVGRFIn0%3D?ct=1735891962018&):

### What are the possible designs available to address the problem? If there are more than one possible design, why was the one in this PR chosen?


### Are there test cases added in this PR? _(If not, why?)_

Yes
### Provide a list of related PRs _(if any)_


### Command used to generate this PR:**_(Applicable only to SDK release request PRs)_

### Checklists
- [ ] Added impacted package name to the issue description
- [ ] Does this PR needs any fixes in the SDK Generator?** _(If so, create an Issue in the [Autorest/typescript](https://github.com/Azure/autorest.typescript) repository and link it here)_
- [ ] Added a changelog (if necessary)
",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""[Cosmos] Query pipeline rewrite"". Body: mentions ""current query pipeline lacks optimization for parallel execution. This PR introduces a new flag, enableQueryControl, which, when set to true, allows intermittent results to be sent to client applications. This provides greater control to the clients and eliminates the need to scan the entire container."" This is about performance: optimization, parallel execution, eliminating need to scan entire container. Keywords: optimization, performance, parallel execution. So classify as 'performance'.assistantfinalperformance"
2484658525,12053,Prevent interface lockups during startup with multiple tabs,droidmonkey,2809491,open,2025-04-28T03:37:53Z,,,https://api.github.com/repos/keepassxreboot/keepassxc,https://github.com/keepassxreboot/keepassxc/pull/12053,"Fixes #11998
Fixes #11870 

Avoids UI lockups by removing several unnecessary mutex blocks  and avoiding redundant key detection calls.

## Testing strategy
[NOTE]: # ( Please describe in detail how you tested your changes. )
[TIP]:  # ( We expect new code to be covered by unit tests and include helpful comments. )
Tested on windows with a Yubikey

## Type of change
[NOTE]: # ( Please remove all lines which don't apply. )
- ‚úÖ Bug fix (non-breaking change that fixes an issue)
",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Prevent interface lockups during startup with multiple tabs"". Body: ""Avoids UI lockups by removing several unnecessary mutex blocks and avoiding redundant key detection calls."" This is about preventing lockups, which is performance improvement? It's about UI responsiveness, removing mutex blocks to avoid lockups. That is performance improvement. The classification: 'performance'.assistantfinalperformance"
2379223800,9893,Ôªø[XABT] Scan for JLO needed wrappers in `LinkAssembliesNoShrink`.,jpobst,179295,closed,2025-03-07T18:39:02Z,2025-03-18T18:28:17Z,2025-03-18T18:28:17Z,https://api.github.com/repos/dotnet/android,https://github.com/dotnet/android/pull/9893,"Today, application builds use Cecil to scan through all Android assemblies at least twice:

- Debug: `_LinkAssembliesNoShrink`, `_GenerateJavaStubs`
- Release: `_ILLink`, `_GenerateJavaStubs`, `_RemoveRegisterAttribute`

This is a costly operation that we would like to only perform once to help speed up application builds.  The long-term goal is to move the (many) steps currently performed by `_GenerateJavaStubs` into ""linker"" steps.

This PR moves the Cecil scanning required for the first step: ""generating Java Callable Wrappers"".

## Implementation

This does not move generating JCW Java code to the linker, it only moves scanning for the JLO information needed to generate JCWs to the linker.  This information is persisted as an XML file in `/obj` next to the processed assembly.

Example:

- `obj/Debug/net10.0-android/android/assets/MyApplication.dll`
- `obj/Debug/net10.0-android/android/assets/MyApplication.jlo.xml`

Doing it this way has two benefits: it helps keep the linker from getting too complex (eg: generating Java code) and it helps incremental builds.  If an assembly has not changed since the last build, the JLO information in the `.jlo.xml` is still valid and will be reused.  The assembly does not need to be re-scanned.  (For example, the ~70 AndroidX assemblies that the MAUI template uses do not need to be re-scanned on every Debug build.)

The process of actually generating the JCWs is done by a new `GenerateJavaCallableWrappers` task in the `_GenerateJavaStubs` target that consumes the `.jlo.xml` files and outputs the `.java` files.

## Release Builds

In an ideal Release build, we would (probably?) run this new step as part of the `ILLink` invocation.  However there are some tricky limitations to running in the linker pipeline, chiefly being that the linker cannot load external assemblies like `Java.Interop.Tool.JavaCallableWrappers.dll` (and dependencies) that we need.  This can be worked around by including the needed source files in the linker assembly, but this will require additional work.

Instead, for now, we will add an invocation a new `_LinkAssembliesAdditionalSteps` target to linked builds that runs after `ILLink`. This calls the existing `LinkAssembliesNoShrink` task to scan for JLOs but does not execute the other linker tasks that have already been run. This allows us to move forward with converting to ""linker steps"" for making Debug builds faster without the extra work required to move these steps into `ILLink`.  A future enhancement can perform this work if desired.  (There is also [some question](https://github.com/dotnet/runtime/issues/107211) as to whether we actually want to create new `ILLink` linker steps.)

Note this temporarily leaves the `$(_AndroidJLOCheckedBuild)` flag that compares the generated Java files to the previous ones.  This will likely be useful ensuring future moved steps are correct as well.",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""[XABT] Scan for JLO needed wrappers in `LinkAssembliesNoShrink`."" The body describes moving scanning to linker to reduce build time. It mentions ""costly operation"" and ""help speed up application builds"" and ""incremental builds"" and ""Debug builds faster"". This is clearly performance improvement. So classification: performance.assistantfinalperformance"
2404898126,9940,[ci] Don't re-run `dotnet test` if there are no auto retry tests.,jpobst,179295,closed,2025-03-19T20:04:03Z,2025-03-20T17:54:54Z,2025-03-20T17:54:53Z,https://api.github.com/repos/dotnet/android,https://github.com/dotnet/android/pull/9940,"Context: https://github.com/dotnet/android/commit/d00b779a5b5f405af967d85e28e570e873631a6b

Commit https://github.com/dotnet/android/commit/d00b779a5b5f405af967d85e28e570e873631a6b added a unit test retry mechanism to address flaky emulator tests.  

This works by:

- Running the unit tests via `dotnet test`
- Running `dotnet-test-slicer` to create a test filter that only contains the failed tests
- Rerunning `dotnet test` with the filtered test list

In the case where there are no failed tests, a ""dummy"" filter is created that does not match any tests.

Most of the time this is fine and the `dotnet test` invocation with the dummy filter takes a couple of seconds to determine it has nothing to do and exits.  However, this ***no-op*** can ***itself*** hang, causing the build to hang for ~3 hours and eventually fail.  ü§¶ 

Instead, add a check to see if the failed test list is empty and fully skip the `dotnet test` invocation if it is, avoiding the potential hang.

Result:

![image](https://github.com/user-attachments/assets/acb84d32-f2d5-4e56-bf63-8290c4777f04)
",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""[ci] Don't re-run `dotnet test` if there are no auto retry tests."" The body describes adding a check to skip re-running dotnet test if no failed tests, to avoid hang. This is about avoiding a hang, but is it performance? It's about preventing a hang, which is a performance issue? The description: ""no-op can itself hang, causing the build to hang for ~3 hours and eventually fail."" They add a check to skip the invocation. This is about avoiding a hang, which is a performance issue? It's more about reliability, but also performance: avoiding unnecessary test run, saving time. The guidelines: performance-related PRs may contain keywords like performance, optimization, latency, etc. This PR mentions ""hang"" and ""3 hours"" and ""skip the invocation"". It is about performance improvement: reducing build time. So classify as 'performance'.assistantfinalperformance"
2609927659,91,chore(v2): release solstatus 2.0.0-beta.0,unibeck,15289783,closed,2025-06-22T19:34:16Z,2025-06-22T22:22:16Z,2025-06-22T22:22:16Z,https://api.github.com/repos/unibeck/solstatus,https://github.com/unibeck/solstatus/pull/91,"## 2.0.0-beta.0 (2025-06-22)

### Updates

* IaC via Alchemy
* Monorepo via pnpm
  * App and Api services now have separate dependency trees and bundles
* UI improvements, especially to the endpoint monitor page
* Performance improvements (which also means reduced costs!)
* Biome v2
* Incremental cache
* Many small bug fixes

### Upgrade from v1.7.x to v2.0.x

It is advisable to read the updated README.md first to get familiar with the new repo structure. That said, updating is an easy two step process

1) Rename the following Cloudflare resources:
        - Worker: `monitor-exec-production` -> `solstatus-prod-monitor-exec`
        - Worker: `monitor-trigger-production` -> `solstatus-prod-monitor-trigger`
        - Worker: `solstatus-production` -> `solstatus-prod-app`
2) Run `pnpm cli --fqdn uptime.example.com --stage prod` (with your actual FQDN)",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""chore(v2): release solstatus 2.0.0-beta.0"". Body includes ""Performance improvements (which also means reduced costs!)"". So it's a performance PR. So label 'performance'.assistantfinalperformance"
2530898021,24818,[MLAS] DequantizeLinear int8/uint8,adrianlizarraga,19691973,open,2025-05-20T09:09:11Z,,,https://api.github.com/repos/microsoft/onnxruntime,https://github.com/microsoft/onnxruntime/pull/24818,"### Description
- Adds multithreaded vectorized implementations of DequantizeLinear for int8 and uint8 inputs:
  - Intel SSE 2
  - ARM NEON
  - All other architectures fallback to a multithreaded scalar reference implementation (previous was not multithreaded).

INT8 DequantizeLinear latency on Intel Core i9-10920X with 4 intra op threads (SSE 2 implementation)

| Number of elements | Baseline latency (us) | Multithreaded+SIMD latency (us) | Speedup |
| ----------------------- | ---------------------- | ------------------------------------ | ---------- |
| 10 K                            | 1                               | 1                                                   | 1              |
| 20 K                            | 2                               | 2                                                   | 1              |
| 40 K                            | 5                               | 5                                                   | 1              |
| 80 K                            | 11                             | 4                                                   | 2.75         |
| 100 K                          | 14                             | 5                                                   | 2.80         |
| 150 K                          | 21                             | 7                                                   | 3.00         |
| 200 K                          | 28                             | 8                                                   | 3.50         |
| 400 K                          | 68                             | 15                                                 | 4.53         |
| 600 K                          | 107                           | 21                                                 | 5.10         |
| 800 K                          | 142                           | 28                                                 | 5.07         |
| 1 M                             | 187                           | 42                                                 | 4.45         |
| 2 M                             | 376                           | 102                                               | 3.69         |
| 4 M                             | 880                           | 236                                               | 3.73         |
| 6 M                             | 1547                         | 557                                               | 2.78         |
| 8 M                             | 2438                         | 1097                                             | 2.22         |
| 10 M                           | 3192                         | 1464                                             | 2.18         |
| 100 M                         | 38718                       | 17733                                           | 2.18         |

INT8 DequantizeLinear latency on Snapdragon 8cx gen 3 @ 3.4GHz with 4 intra op threads (NEON implementation)

| Number of elements | Baseline latency (us) | Multithreaded+SIMD latency (us) | Speedup |
| ----------------------- | ---------------------- | ------------------------------------ | ---------- |
| 10 K                            | 1                               | 1                                                   | 1              |
| 20 K                            | 1                               | 1                                                   | 1              |
| 40 K                            | 3                               | 3                                                   | 1              |
| 80 K                            | 7                               | 4                                                   | 1.75         |
| 100 K                          | 9                               | 3                                                   | 3.00         |
| 150 K                          | 14                             | 5                                                   | 2.80         |
| 200 K                          | 18                             | 6                                                   | 3.00         |
| 400 K                          | 38                             | 10                                                 | 3.80         |
| 600 K                          | 61                             | 15                                                 | 4.07         |
| 800 K                          | 76                             | 19                                                 | 4.00         |
| 1 M                             | 98                             | 24                                                 | 4.08         |
| 2 M                             | 204                           | 48                                                 | 4.25         |
| 4 M                             | 424                           | 112                                               | 3.79         |
| 6 M                             | 677                           | 384                                               | 1.76         |
| 8 M                             | 919                           | 621                                               | 1.48         |
| 10 M                           | 1132                         | 776                                               | 1.46         |
| 100 M                         | 11842                       | 10566                                           | 1.12         |
### Motivation and Context
Improves latency of quantized QDQ models that with large DQs that dominate the inference latency.",Human,"analysisWe need to classify as 'performance' or 'non-performance'. The PR title: ""[MLAS] DequantizeLinear int8/uint8"". Body describes adding multithreaded vectorized implementations, performance improvements, latency tables, speedup. It's clearly performance. So label 'performance'.assistantfinalperformance"
2569040246,24962,Enable VTCM Back Up Buffer Sharing,quic-calvnguy,201653611,closed,2025-06-05T04:09:57Z,2025-06-25T03:27:29Z,2025-06-25T03:27:29Z,https://api.github.com/repos/microsoft/onnxruntime,https://github.com/microsoft/onnxruntime/pull/24962,"### Description
Enabling the VTCM backup buffering feature on QNN EP, assuming all graphs are running sequentially where the input of the next graph is the output of the current graph.

Under these assumptions, rather than allocate buffers for all inputs and outputs, only a single buffer can be shared between all graphs.


### Motivation and Context
This will allow larger LLM models to be run


",Human,"analysisWe need to classify whether the primary purpose is performance improvement. The PR title: ""Enable VTCM Back Up Buffer Sharing"". Description: enabling backup buffering feature on QNN EP, assuming all graphs run sequentially. Instead of allocating buffers for all inputs and outputs, only a single buffer can be shared between all graphs. Motivation: allow larger LLM models to be run. This is about memory usage optimization, reducing buffer allocation, which is a performance improvement (memory usage). It's about performance, specifically memory usage and potentially speed. So classify as 'performance'.assistantfinalperformance"
2348525274,10870,Add Response compression support,kshyju,144469,closed,2025-02-21T02:49:58Z,2025-02-26T21:14:53Z,2025-02-26T21:14:53Z,https://api.github.com/repos/Azure/azure-functions-host,https://github.com/Azure/azure-functions-host/pull/10870,"resolves #7285

Currently, response compression is enabled on platform level for app service windows because Windows workers still use IIS. For Linux use case, Kestrel serves as the web server. This PR adds support for enabling response compression on host by leveraging the ASP.NET response compression middleware. Customers can opt in by setting the feature flag **""EnableResponseCompression""** to enable this behavior.  

For additional context, see [Compression on App Service Linux](https://azureossd.github.io/2022/07/13/Compression-on-App-Service-Linux).

### Pull request checklist

**IMPORTANT**: Currently, changes must be backported to the `in-proc` branch to be included in Core Tools and non-Flex deployments.

* [ ] Backporting to the `in-proc` branch is not required
    * Otherwise: Link to backporting PR - will follow
* [x] My changes **do not** require documentation changes
    * [ ] Otherwise: Documentation issue linked to PR 
* [ ] My changes **should not** be added to the release notes for the next release
    * [x] Otherwise: I've added my notes to `release_notes.md`
* [ ] My changes **do not** need to be backported to a previous version
    * [x] Otherwise: Backport tracked by issue/PR #issue_or_pr  - will follow
* [x] My changes **do not** require diagnostic events changes
    * Otherwise: I have added/updated all related diagnostic events and their documentation (Documentation issue linked to PR)
* [x] I have added all required tests (Unit tests, E2E tests)

<!-- Optional: delete if not applicable  -->
### Additional information

Additional PR information
",Human,"analysisWe need to classify as performance or non-performance. The PR adds response compression support. That is a performance improvement: compression reduces payload size, improves throughput, reduces latency. So it's performance. The keywords: compression, performance, etc. So classification: performance.assistantfinalperformance"
2412640161,35835,Changes to AsyncLocal usage for better lazy loading performance,henriquewr,111216098,closed,2025-03-24T05:11:54Z,2025-03-25T21:40:12Z,2025-03-25T21:40:12Z,https://api.github.com/repos/dotnet/efcore,https://github.com/dotnet/efcore/pull/35835,"Changed AsyncLocal to ThreadId for better performance
Fixes #35832 ",Human,"analysisWe need to classify as performance or non-performance. The title: ""Changes to AsyncLocal usage for better lazy loading performance"". Body: ""Changed AsyncLocal to ThreadId for better performance"". So it's clearly performance improvement. So label 'performance'.assistantfinalperformance"
2320341773,18296,Handle |null types when optimizing away equals/hash/compare from LanguagePrimitves into instance method calls,T-Gro,46543583,closed,2025-02-06T16:23:13Z,2025-02-07T13:42:02Z,2025-02-07T13:42:02Z,https://api.github.com/repos/dotnet/fsharp,https://github.com/dotnet/fsharp/pull/18296,Fixes #18286 ,Human,"analysisWe need to classify the pull request as performance or non-performance. The title: ""Handle |null types when optimizing away equals/hash/compare from LanguagePrimitives into instance method calls"". The body: ""Fixes #18286"". No body content. The title mentions ""optimizing away equals/hash/compare from LanguagePrimitives into instance method calls"". That sounds like an optimization, but is it performance? The phrase ""optimizing away"" suggests performance improvement. The context: ""Handle |null types when optimizing away equals/hash/compare from LanguagePrimitives into instance method calls"". This is about handling null types when optimizing away equals/hash/compare. That seems like a bug fix for null handling, but also mentions optimizing away equals/hash/compare. That could be a performance improvement. But the primary purpose? The title indicates ""Handle |null types when optimizing away equals/hash/compare from LanguagePrimitives into instance method calls"". That is a bug fix for null handling, but also mentions optimization. The question: ""determine whether the primary purpose of the pull request is to improve performance."" The title includes ""optimizing away equals/hash/compare"". That is a performance optimization. But the main issue might be null handling. However, the phrase ""optimizing away"" indicates performance. The classification: performance. The guidelines: performance-related PRs may contain keywords such as optimization, performance, etc. This includes ""optimizing"". So classify as 'performance'.assistantfinalperformance"
2386158448,18377,Test-TP: Reference assembly loading fixes,Thorium,229355,closed,2025-03-11T19:47:24Z,2025-03-12T12:06:13Z,2025-03-12T12:06:13Z,https://api.github.com/repos/dotnet/fsharp,https://github.com/dotnet/fsharp/pull/18377,"I noticed that with the current reference loading of TypeProviders, if I had 139 reference assemblies (in a solution memory), I ended up calling the Assembly.Load for 897 times. This is because so many assemblies have the same references like System.Memory, System.Xml, System.Buffers, System.Threading.Tasks.Extensions, ... And the code said ""load all reference assemblies"". Simple fix: Check already loaded reference assemblies before trying to call the slow Assembly.Load again.

The sourceAssembliesTable_ is a ConcurrentDictionary to ensure thread-safety. However, instead of code using it in thread-safe way, it was used by double-lookup. So that is fixed to actually use it properly. (It's role is to be used as a guard to sourceAssemblies_ array, which is manually lazy-loaded from the queue.)

These changes match the FSharp.TypeProvider.SDK merged PR.
",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Test-TP: Reference assembly loading fixes"". Body: describes that with many reference assemblies, Assembly.Load called many times, causing slow loading. They fix by checking already loaded reference assemblies before calling slow Assembly.Load again. Also fix double-lookup in ConcurrentDictionary. So it's about reducing load calls, improving performance. Keywords: ""slow"", ""Assembly.Load"", ""fixes"", ""performance"". So classification: performance.assistantfinalperformance"
2483117033,18509,"SIMD vectorization of Array.sum<int>, etc",Thorium,229355,open,2025-04-26T12:04:41Z,,,https://api.github.com/repos/dotnet/fsharp,https://github.com/dotnet/fsharp/pull/18509,"## Description

Specific overloads (float, float32, int, int64) of Seq.sum, ~~Seq.average,~~ Array.sum ~~and Array.average~~ to take advantage of vectorization in System.Linq.Enumerable module.

This is potentially a naive first try to solve #16230 by the spirit of @T-Gro comment https://github.com/dotnet/fsharp/issues/16230#issuecomment-2826895557

## Checklist

- [ ] Test cases added
- [x] Performance benchmarks added in case of performance changes
- [x] Release notes entry updated:
    > Please make sure to add an entry with short succinct description of the change as well as link to this pull request to the respective release notes file, if applicable.
",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""SIMD vectorization of Array.sum<int>, etc"". Description: adding SIMD vectorization to Seq.sum, Array.sum, etc. It's about performance improvement. So classification: performance.assistantfinalperformance"
2573225924,61822,"optimization, reduce memory usage",VincentBailly,28921716,closed,2025-06-06T11:20:52Z,2025-06-09T18:48:31Z,2025-06-09T18:48:31Z,https://api.github.com/repos/microsoft/TypeScript,https://github.com/microsoft/TypeScript/pull/61822,"remove the creation of unnecessary lambda wrappers.

Speeds up 11% the project initialization time of tsserver in large repositories.

Fixes #61821

",Human,"analysisWe need to classify as 'performance' or 'non-performance'. The PR title and body: ""optimization, reduce memory usage remove the creation of unnecessary lambda wrappers. Speeds up 11% the project initialization time of tsserver in large repositories. Fixes #61821"". This is clearly performance improvement. So output 'performance'.assistantfinalperformance"
2607158708,13154,Use hard time limit on gradual memory reclaim,Artoria2e5,6459309,open,2025-06-20T11:22:14Z,,,https://api.github.com/repos/microsoft/WSL,https://github.com/microsoft/WSL/pull/13154,"## Summary of the Pull Request
This is a purely theoretical implementation of the fix proposed in https://github.com/microsoft/WSL/issues/10675#issuecomment-2990485856, under the assumption that an incomplete reclaim is causing things to throttle and hang. It is completely untested nor do I have any intention of testing it. For CLA purposes, I disclaim any rights that I may or may not have on this code.

<!-- Please review the items on the PR checklist before submitting-->
## PR Checklist

- [x] **Closes:** Link to issue #10675
- [ ] **Communication:** I've discussed this with core contributors already. If work hasn't been agreed, this work might be rejected
- [ ] **Tests:** Added/updated if needed and all pass
- [ ] **Localization:** All end user facing strings can be localized
- [ ] **Dev docs:** Added/updated if needed
- [ ] **Documentation updated:** If checked, please file a pull request on [our docs repo](https://github.com/MicrosoftDocs/wsl/) and link it here: #xxx

## Detailed Description of the Pull Request / Additional comments

See the comment. This PR is only a tactic to draw attention. It has been made draft to make sure nobody merges it.

Uhhhh @OneBlue notice me please?

## Validation Steps Performed

The vibes are right. There are probably three missing includes.",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Use hard time limit on gradual memory reclaim"". The description: theoretical implementation of fix for issue about incomplete reclaim causing throttle and hang. It's about memory reclaim, throttle, hang. That is performance-related: memory reclaim, throttle, hang. Keywords: throttle, hang. So performance. So output 'performance'.assistantfinalperformance"
2305762055,4774,Add netperf capability to collect CPU traces on Azure,ProjectsByJackHe,43735701,closed,2025-01-29T23:43:23Z,2025-01-30T18:08:04Z,2025-01-30T18:08:04Z,https://api.github.com/repos/microsoft/msquic,https://github.com/microsoft/msquic/pull/4774,"## Description

To improve developer productivity, we enable the option to collect CPU traces in azure for analyzing and identifying bottlenecks.
¬†
## Testing

Netperf CI

## Documentation

N/A
",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Add netperf capability to collect CPU traces on Azure"". Body: ""To improve developer productivity, we enable the option to collect CPU traces in azure for analyzing and identifying bottlenecks."" This is about collecting CPU traces to analyze bottlenecks. That is performance-related. The keywords: ""CPU traces"", ""bottlenecks"", ""performance"". So classification: performance.assistantfinalperformance"
2351099465,614,Finishing up work on AppControl Manager v.1.9.1.0,HotCakeX,118815227,closed,2025-02-22T11:20:19Z,2025-02-22T19:08:45Z,2025-02-22T19:08:45Z,https://api.github.com/repos/HotCakeX/Harden-Windows-Security,https://github.com/HotCakeX/Harden-Windows-Security/pull/614,"* Added [Control Flow Guard](https://learn.microsoft.com/en-us/defender-endpoint/exploit-protection-reference#control-flow-guard-cfg) support to the AppControl Manager. A great security feature that prevents certain exploits.

* Set Intel's Control-flow Enforcement Technology to explicitly enabled in the project.

* The Native Ahead-Of-Time compilation now favors execution speed and performance over package size. (Will be monitoring this one as the app gets bigger when more features are implemented in the future. For now, it only increases the package size by 1MB)

* Enabled a new code analyzer to enforce more code best practices: `dotnet_diagnostic.CA1724`. (Type names should not match namespaces)

* Updated the [AppControl Manager document](https://github.com/HotCakeX/Harden-Windows-Security/wiki/AppControl-Manager).

* Formatted all XAML codes for better readability.

* Adjusted the App Control Simulation code based on the custom serialization/deserialization logic introduced in the previous update.

* Updated `Microsoft.IdentityModel.Abstractions` NuGet dependency.

* Aligned some columns in the new ListViews.
",Human,"analysisWe need to classify as performance or non-performance. The PR description: ""Finishing up work on AppControl Manager v.1.9.1.0"" includes adding Control Flow Guard, enabling Intel CFG, Native Ahead-Of-Time compilation now favors execution speed and performance over package size. That is a performance change. Also mentions ""Native Ahead-Of-Time compilation now favors execution speed and performance over package size."" That is a performance improvement. Also ""Native Ahead-Of-Time compilation now favors execution speed and performance over package size."" So it's performance. Also other changes are security, code analyzer, formatting, etc. But the primary purpose? The title: ""Finishing up work on AppControl Manager v.1.9.1.0"" not explicitly performance. But body includes performance. The instruction: determine whether the primary purpose is to improve performance. The presence of performance keywords: ""performance"", ""speed"". So classify as 'performance'.assistantfinalperformance"
2596620305,2530,Improve JupyterLab extension build time,minestarks,16928427,closed,2025-06-16T19:03:41Z,2025-06-16T23:08:14Z,2025-06-16T23:08:14Z,https://api.github.com/repos/microsoft/qsharp,https://github.com/microsoft/qsharp/pull/2530,"This PR reduces the build time of the JupyterLab package from ~70s/30s/30s (on Windows/Ubuntu/Mac) to ~15s/10s/13s by eliminating the heavyweight `jupyterlab` dependency. Instead, we build and bundle the JavaScript + Python directly using the `@jupyter/builder` npm package. 

Additionally, we get rid of `jlpm` (a.k.a. yarn) in favor of using `npm` which we already use in the rest of our build.

The `jupyterlab-core` dir contains the minimal configuration, that `@jupyter/builder` uses, that previously lived in the `jupyterlab` package. This is really all we needed from there.
¬†
The resulting packages should be identical.

I've ruled out:
- eliminating the webpack dependency in favor of a different bundler, since the JupyterLab extension architecture uses webpack module federation - there's no getting rid of it.
- removing the JupyterLab package completely - this feels very heavyhanded to me, considering how minimal the build now is and how we still get value from the package IMO (Q# syntax highlighting). 
- checking in the built files - the built files are webpack-bundled, and not very readable. Checking in these artifacts directly would again be pretty heavy-handed, sacrificing maintainability to save a few seconds of build time.

Fixes #2482",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Improve JupyterLab extension build time"". Body: describes reducing build time from ~70s to ~15s by eliminating heavyweight dependency, using @jupyter/builder, etc. This is clearly a performance improvement. So classification: 'performance'.assistantfinalperformance"
2394225726,546,Query Optimizations,TheCactusBlue,16204510,closed,2025-03-14T17:22:57Z,2025-05-07T23:44:12Z,,https://api.github.com/repos/stack-auth/stack-auth,https://github.com/stack-auth/stack-auth/pull/546,"
<!-- ELLIPSIS_HIDDEN -->



> [!IMPORTANT]
> Optimizes OAuth provider creation in `crud.tsx` by using `Promise.all` for concurrent execution.
> 
>   - **Optimization**:
>     - Replaces loop with `Promise.all` for concurrent execution of OAuth provider creation in `onCreate` function in `crud.tsx`.
> 
> <sup>This description was created by </sup>[<img alt=""Ellipsis"" src=""https://img.shields.io/badge/Ellipsis-blue?color=175173"">](https://www.ellipsis.dev?ref=stack-auth%2Fstack-auth&utm_source=github&utm_medium=referral)<sup> for f883e1a7362f32c8585b5d571eb4d9a76ac80eef. It will automatically update as commits are pushed.</sup>

<!-- ELLIPSIS_HIDDEN -->",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Query Optimizations"". Body: ""Optimizes OAuth provider creation in crud.tsx by using Promise.all for concurrent execution."" This is about performance optimization. Keywords: ""Optimizes"", ""concurrent execution"". So it's performance. So label 'performance'.assistantfinalperformance"
2450551741,7071,Fix/swap provider manage fix OK-37198 OK-37230 OK-37194 OK-37154,ezailWang,38491708,closed,2025-04-10T09:10:07Z,2025-04-10T09:18:00Z,2025-04-10T09:18:00Z,https://api.github.com/repos/OneKeyHQ/app-monorepo,https://github.com/OneKeyHQ/app-monorepo/pull/7071,"

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->

## Summary by CodeRabbit

- **Bug Fixes**
  - Optimized swap state management for platform-specific environments, ensuring smoother swapping operations.

- **New Features**
  - Enhanced provider information processing to better reflect network support and restrictions.

- **Style**
  - Updated the provider management panel with a transparent background for a cleaner user interface.

<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Fix/swap provider manage fix OK-37198 OK-37230 OK-37194 OK-37154"". Body: summary: Bug Fixes: Optimized swap state management for platform-specific environments, ensuring smoother swapping operations. New Features: Enhanced provider information processing. Style: UI changes.

The summary says ""Optimized swap state management"" which is performance improvement. The keyword ""Optimized"" indicates performance. Also ""smoother swapping operations"" suggests performance. So classify as 'performance'.assistantfinalperformance"
2545005196,7416,feat: Update switch animation to use spring easing for improved perfo‚Ä¶,limichange,1947344,closed,2025-05-27T00:38:51Z,2025-05-29T01:47:31Z,2025-05-29T01:47:31Z,https://api.github.com/repos/OneKeyHQ/app-monorepo,https://github.com/OneKeyHQ/app-monorepo/pull/7416,"‚Ä¶rmance

- Replaced the existing 'easeInOut' animation with a new 'switch' animation using spring dynamics in the Tamagui configuration.
- Updated the Switch component to utilize the new animation for a more responsive user experience.

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->
## Summary by CodeRabbit

- **Style**
  - Updated switch component animation to use a consistent spring-based animation across all platforms, providing a smoother and more unified user experience.
<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""feat: Update switch animation to use spring easing for improved perfo‚Ä¶"". Body mentions improved responsiveness, smoother animation. Keywords: ""performance"", ""responsive"", ""spring easing"". The instruction says performance-related PRs may contain keywords such as respons, react, wait, responsive, throughput, slow, performance, optimization, latency, compile-time-hog, fast, memory, stuck, leak, hang, perf, minor, speed, perform, lag, latenc, tim, instant, or any other terms referring to how faster and efficiently software runs. The PR is about animation, but it's about improving responsiveness, which is a performance improvement. So classify as 'performance'.assistantfinalperformance"
2327882540,6645,fix: Improve async handling in create wallet onboarding flow,limichange,1947344,closed,2025-02-11T07:02:12Z,2025-02-11T13:59:47Z,2025-02-11T13:59:47Z,https://api.github.com/repos/OneKeyHQ/app-monorepo,https://github.com/OneKeyHQ/app-monorepo/pull/6645,"
https://github.com/user-attachments/assets/ef62869b-1e5a-4d30-8b91-c1e51a83cbc6



<!-- This is an auto-generated comment: release notes by coderabbit.ai -->
## Summary by CodeRabbit

- **Refactor**
  - Enhanced the wallet creation process during onboarding, allowing for a more responsive transition to the next step without waiting for password verification.
<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""fix: Improve async handling in create wallet onboarding flow"". Body: mentions ""Enhanced the wallet creation process during onboarding, allowing for a more responsive transition to the next step without waiting for password verification."" The keywords: ""responsive"" is a performance keyword. Also ""async handling"" might improve performance. The description says ""more responsive transition"" - that indicates performance improvement. So classify as 'performance'.assistantfinalperformance"
2400016065,2397,[performance improvements] json_repair.repair_json() improve performance,mangiucugna,4247706,closed,2025-03-18T07:37:59Z,2025-03-21T18:25:20Z,2025-03-21T18:25:19Z,https://api.github.com/repos/crewAIInc/crewAI,https://github.com/crewAIInc/crewAI/pull/2397,"As explained in [the documentation](https://github.com/mangiucugna/json_repair/?tab=readme-ov-file#performance-considerations,) if you already check that the json is valid, you can pass `skip_json_loads=True` to improve performance.",Human,"analysisWe need to classify as 'performance' or 'non-performance'. The PR title: ""[performance improvements] json_repair.repair_json() improve performance"". Body mentions performance improvements. So it's performance.assistantfinalperformance"
2325911874,2086,fix user memory config issue,sharmasundip,59015684,closed,2025-02-10T12:21:30Z,2025-02-18T16:59:31Z,2025-02-18T16:59:31Z,https://api.github.com/repos/crewAIInc/crewAI,https://github.com/crewAIInc/crewAI/pull/2086,,Human,"analysisWe need to classify the pull request as performance or non-performance. The title: ""fix user memory config issue"". Body not provided. The title indicates memory config issue. Could be performance? Memory config issue might be about memory usage, but not necessarily performance. The instruction: performance-related PRs may contain keywords like memory. But memory is ambiguous. The instruction says ""performance-related pull requests may contain keywords such as... memory..."". So memory is a keyword. But does ""fix user memory config issue"" refer to performance? It could be about memory usage, maybe memory leak. But the title says ""memory config issue"" - maybe configuration of memory usage. That could be performance. But it's ambiguous. The instruction says ""performance-related pull requests may contain keywords such as... memory..."". So presence of memory suggests performance. But we need to decide. The primary purpose: fix memory config issue. That could be about memory usage, which is performance. I'd lean performance. But could also be non-performance if it's just a config bug. But memory config is performance. I'd classify as performance.assistantfinalperformance"
2570328421,2964,fix: remove duplicated message about Tool result,lucasgomide,5209129,closed,2025-06-05T13:28:39Z,2025-06-05T13:42:10Z,2025-06-05T13:42:10Z,https://api.github.com/repos/crewAIInc/crewAI,https://github.com/crewAIInc/crewAI/pull/2964,"We are currently inserting tool results into LLM messages twice, which may unnecessarily increase processing costs, especially for longer outputs.

As you can see in this screnshoot the tool result is being added duplicated
<img width=""1722"" alt=""Screenshot 2025-06-05 at 10 12 12‚ÄØAM"" src=""https://github.com/user-attachments/assets/f036fbc2-2dce-4cf9-9149-707cbbf7e161"" />
",Human,"analysisWe need to classify whether the primary purpose is performance improvement. The PR title: ""fix: remove duplicated message about Tool result"". Body: describes removing duplication to reduce processing costs, especially for longer outputs. This is about reducing processing costs, which is performance optimization. It mentions unnecessary duplication, processing costs. That is performance. So label 'performance'.assistantfinalperformance"
2337334370,2136,‚ö°Ô∏è Speed up function `calculate_node_levels` by 48x,misrasaurabh1,1271289,closed,2025-02-15T01:36:40Z,2025-02-25T20:29:22Z,,https://api.github.com/repos/crewAIInc/crewAI,https://github.com/crewAIInc/crewAI/pull/2136,"### üìÑ 4,848% (48.48x) speedup for ***`calculate_node_levels` in `src/crewai/flow/utils.py`***

‚è±Ô∏è Runtime :   **`58.0 milliseconds`**  **‚Üí** **`1.17 millisecond`** (best of `318` runs)
<details>
<summary> üìù Explanation and details</summary>

To optimize the given `calculate_node_levels` function, we can make several changes focusing on reducing the number of nested loops and leveraging data structures more efficiently. Here is the revised function.

1. Replace list `queue` (with `pop(0)`) with `deque` which provides O(1) time complexity for append and pop operations.
2. Precompute method dependencies instead of repeatedly checking conditions inside loops.
3. Organize the steps for better readability and separate route processing into a helper function.



Key optimizations.
1. Using `deque` instead of list `queue` to optimize appending and popping elements.
2. Precomputing listener dependencies reduces the number of checks and allows faster access.
3. Factoring out the router processing logic into the `process_router_paths` function improves readability and maintainability.

</details>

‚úÖ **Correctness verification report:**


| Test                        | Status            |
| --------------------------- | ----------------- |
| ‚öôÔ∏è Existing Unit Tests | üîò **None Found** |
| üåÄ Generated Regression Tests | ‚úÖ **31 Passed** |
| ‚è™ Replay Tests | üîò **None Found** |
| üîé Concolic Coverage Tests | üîò **None Found** |
|üìä Tests Coverage       | 100.0% |
<details>
<summary>üåÄ Generated Regression Tests Details</summary>

```python
from typing import Any, Dict, List, Set

# imports
import pytest  # used for our unit tests
from crewai.flow.utils import calculate_node_levels


# Mock flow class for testing
class MockFlow:
    def __init__(self, methods, listeners, routers, router_paths):
        self._methods = methods
        self._listeners = listeners
        self._routers = routers
        self._router_paths = router_paths

# unit tests
def test_single_start_method_no_listeners_or_routers():
    flow = MockFlow(
        methods={""start"": MockMethod(True)},
        listeners={},
        routers=set(),
        router_paths={}
    )
    expected = {""start"": 0}
    codeflash_output = calculate_node_levels(flow)

def test_multiple_start_methods_no_listeners_or_routers():
    flow = MockFlow(
        methods={""start1"": MockMethod(True), ""start2"": MockMethod(True)},
        listeners={},
        routers=set(),
        router_paths={}
    )
    expected = {""start1"": 0, ""start2"": 0}
    codeflash_output = calculate_node_levels(flow)

def test_single_or_listener():
    flow = MockFlow(
        methods={""start"": MockMethod(True)},
        listeners={""listener"": (""OR"", [""start""])},
        routers=set(),
        router_paths={}
    )
    expected = {""start"": 0, ""listener"": 1}
    codeflash_output = calculate_node_levels(flow)

def test_multiple_or_listeners():
    flow = MockFlow(
        methods={""start1"": MockMethod(True), ""start2"": MockMethod(True)},
        listeners={""listener1"": (""OR"", [""start1""]), ""listener2"": (""OR"", [""start2""])},
        routers=set(),
        router_paths={}
    )
    expected = {""start1"": 0, ""start2"": 0, ""listener1"": 1, ""listener2"": 1}
    codeflash_output = calculate_node_levels(flow)

def test_single_and_listener():
    flow = MockFlow(
        methods={""start1"": MockMethod(True), ""start2"": MockMethod(True)},
        listeners={""listener"": (""AND"", [""start1"", ""start2""])},
        routers=set(),
        router_paths={}
    )
    expected = {""start1"": 0, ""start2"": 0, ""listener"": 1}
    codeflash_output = calculate_node_levels(flow)

def test_multiple_and_listeners():
    flow = MockFlow(
        methods={""start1"": MockMethod(True), ""start2"": MockMethod(True), ""start3"": MockMethod(True)},
        listeners={""listener1"": (""AND"", [""start1"", ""start2""]), ""listener2"": (""AND"", [""start2"", ""start3""])},
        routers=set(),
        router_paths={}
    )
    expected = {""start1"": 0, ""start2"": 0, ""start3"": 0, ""listener1"": 1, ""listener2"": 1}
    codeflash_output = calculate_node_levels(flow)

def test_mixed_conditions():
    flow = MockFlow(
        methods={""start1"": MockMethod(True), ""start2"": MockMethod(True)},
        listeners={""listener1"": (""OR"", [""start1""]), ""listener2"": (""AND"", [""start1"", ""start2""])},
        routers=set(),
        router_paths={}
    )
    expected = {""start1"": 0, ""start2"": 0, ""listener1"": 1, ""listener2"": 1}
    codeflash_output = calculate_node_levels(flow)

def test_single_router():
    flow = MockFlow(
        methods={""start"": MockMethod(True)},
        listeners={""listener"": (""OR"", [""path1""])},
        routers={""start""},
        router_paths={""start"": [""path1""]}
    )
    expected = {""start"": 0, ""listener"": 1}
    codeflash_output = calculate_node_levels(flow)

def test_multiple_routers():
    flow = MockFlow(
        methods={""start1"": MockMethod(True), ""start2"": MockMethod(True)},
        listeners={""listener1"": (""OR"", [""path1""]), ""listener2"": (""OR"", [""path2""])},
        routers={""start1"", ""start2""},
        router_paths={""start1"": [""path1""], ""start2"": [""path2""]}
    )
    expected = {""start1"": 0, ""start2"": 0, ""listener1"": 1, ""listener2"": 1}
    codeflash_output = calculate_node_levels(flow)

def test_no_start_methods():
    flow = MockFlow(
        methods={""method1"": MockMethod(False)},
        listeners={},
        routers=set(),
        router_paths={}
    )
    expected = {}
    codeflash_output = calculate_node_levels(flow)

def test_listeners_with_no_trigger_methods():
    flow = MockFlow(
        methods={""start"": MockMethod(True)},
        listeners={""listener"": (""OR"", [])},
        routers=set(),
        router_paths={}
    )
    expected = {""start"": 0}
    codeflash_output = calculate_node_levels(flow)

def test_empty_flow():
    flow = MockFlow(
        methods={},
        listeners={},
        routers=set(),
        router_paths={}
    )
    expected = {}
    codeflash_output = calculate_node_levels(flow)

def test_large_number_of_methods_and_listeners():
    methods = {f""start{i}"": MockMethod(True) for i in range(100)}
    listeners = {f""listener{i}"": (""OR"", [f""start{i}""]) for i in range(100)}
    flow = MockFlow(
        methods=methods,
        listeners=listeners,
        routers=set(),
        router_paths={}
    )
    expected = {f""start{i}"": 0 for i in range(100)}
    expected.update({f""listener{i}"": 1 for i in range(100)})
    codeflash_output = calculate_node_levels(flow)



def test_deterministic_behavior():
    flow = MockFlow(
        methods={""start"": MockMethod(True)},
        listeners={""listener"": (""OR"", [""start""])},
        routers=set(),
        router_paths={}
    )
    expected = {""start"": 0, ""listener"": 1}
    codeflash_output = calculate_node_levels(flow)
    codeflash_output = calculate_node_levels(flow)

# Mock method class for testing
class MockMethod:
    def __init__(self, is_start_method):
        if is_start_method:
            self.__is_start_method__ = True
# codeflash_output is used to check that the output of the original code is the same as that of the optimized code.

from typing import Any, Dict, List, Set

# imports
import pytest  # used for our unit tests
from crewai.flow.utils import calculate_node_levels


# Helper classes to simulate flow structure
class Method:
    def __init__(self, is_start_method=False):
        self.__is_start_method__ = is_start_method

class Flow:
    def __init__(self):
        self._methods = {}
        self._listeners = {}
        self._routers = set()
        self._router_paths = {}

# unit tests
def test_single_start_method():
    flow = Flow()
    flow._methods['start_method'] = Method(is_start_method=True)
    codeflash_output = calculate_node_levels(flow)

def test_multiple_start_methods():
    flow = Flow()
    flow._methods['start_method_1'] = Method(is_start_method=True)
    flow._methods['start_method_2'] = Method(is_start_method=True)
    codeflash_output = calculate_node_levels(flow)

def test_single_or_listener():
    flow = Flow()
    flow._methods['start_method'] = Method(is_start_method=True)
    flow._listeners['listener_method'] = ('OR', ['start_method'])
    codeflash_output = calculate_node_levels(flow)

def test_single_and_listener():
    flow = Flow()
    flow._methods['start_method'] = Method(is_start_method=True)
    flow._listeners['listener_method'] = ('AND', ['start_method'])
    codeflash_output = calculate_node_levels(flow)

def test_multiple_or_listeners():
    flow = Flow()
    flow._methods['start_method'] = Method(is_start_method=True)
    flow._listeners['listener_method_1'] = ('OR', ['start_method'])
    flow._listeners['listener_method_2'] = ('OR', ['start_method'])
    codeflash_output = calculate_node_levels(flow)

def test_multiple_and_listeners():
    flow = Flow()
    flow._methods['start_method'] = Method(is_start_method=True)
    flow._listeners['listener_method_1'] = ('AND', ['start_method'])
    flow._listeners['listener_method_2'] = ('AND', ['start_method'])
    codeflash_output = calculate_node_levels(flow)

def test_mixed_or_and_listeners():
    flow = Flow()
    flow._methods['start_method'] = Method(is_start_method=True)
    flow._listeners['or_listener'] = ('OR', ['start_method'])
    flow._listeners['and_listener'] = ('AND', ['start_method'])
    codeflash_output = calculate_node_levels(flow)

def test_single_router():
    flow = Flow()
    flow._methods['start_method'] = Method(is_start_method=True)
    flow._routers.add('start_method')
    flow._router_paths['start_method'] = ['router_path']
    codeflash_output = calculate_node_levels(flow)

def test_router_with_listeners():
    flow = Flow()
    flow._methods['start_method'] = Method(is_start_method=True)
    flow._routers.add('start_method')
    flow._router_paths['start_method'] = ['router_path']
    flow._listeners['listener_method'] = ('OR', ['router_path'])
    codeflash_output = calculate_node_levels(flow)

def test_multiple_routers():
    flow = Flow()
    flow._methods['start_method'] = Method(is_start_method=True)
    flow._routers.add('start_method')
    flow._routers.add('router_method_2')
    flow._router_paths['start_method'] = ['router_path_1']
    flow._router_paths['router_method_2'] = ['router_path_2']
    codeflash_output = calculate_node_levels(flow)

def test_empty_flow():
    flow = Flow()
    codeflash_output = calculate_node_levels(flow)

def test_cycle_in_flow():
    flow = Flow()
    flow._methods['start_method'] = Method(is_start_method=True)
    flow._listeners['cycle_method'] = ('OR', ['start_method'])
    flow._listeners['start_method'] = ('OR', ['cycle_method'])
    codeflash_output = calculate_node_levels(flow)

def test_disconnected_nodes():
    flow = Flow()
    flow._methods['start_method'] = Method(is_start_method=True)
    flow._methods['disconnected_method'] = Method()
    codeflash_output = calculate_node_levels(flow)

def test_listeners_with_no_trigger_methods():
    flow = Flow()
    flow._methods['start_method'] = Method(is_start_method=True)
    flow._listeners['listener_method'] = ('OR', [])
    codeflash_output = calculate_node_levels(flow)

def test_large_number_of_methods():
    flow = Flow()
    for i in range(1000):
        flow._methods[f'method_{i}'] = Method(is_start_method=(i == 0))
        if i > 0:
            flow._listeners[f'method_{i}'] = ('OR', [f'method_{i-1}'])
    codeflash_output = calculate_node_levels(flow)

def test_deep_hierarchical_structure():
    flow = Flow()
    flow._methods['start_method'] = Method(is_start_method=True)
    for i in range(1, 1000):
        flow._listeners[f'method_{i}'] = ('OR', [f'method_{i-1}' if i > 1 else 'start_method'])
    codeflash_output = calculate_node_levels(flow)
# codeflash_output is used to check that the output of the original code is the same as that of the optimized code.
```

</details>



[![Codeflash](https://img.shields.io/badge/Optimized%20with-Codeflash-yellow?style=flat&color=%23ffc428&logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNDgwIiBoZWlnaHQ9ImF1dG8iIHZpZXdCb3g9IjAgMCA0ODAgMjgwIiBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgo8cGF0aCBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTI4Ni43IDAuMzc4NDE4SDIwMS43NTFMNTAuOTAxIDE0OC45MTFIMTM1Ljg1MUwwLjk2MDkzOCAyODEuOTk5SDk1LjQzNTJMMjgyLjMyNCA4OS45NjE2SDE5Ni4zNDVMMjg2LjcgMC4zNzg0MThaIiBmaWxsPSIjRkZDMDQzIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMzExLjYwNyAwLjM3ODkwNkwyNTguNTc4IDU0Ljk1MjZIMzc5LjU2N0w0MzIuMzM5IDAuMzc4OTA2SDMxMS42MDdaIiBmaWxsPSIjMEIwQTBBIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMzA5LjU0NyA4OS45NjAxTDI1Ni41MTggMTQ0LjI3NkgzNzcuNTA2TDQzMC4wMjEgODkuNzAyNkgzMDkuNTQ3Vjg5Ljk2MDFaIiBmaWxsPSIjMEIwQTBBIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMjQyLjg3MyAxNjQuNjZMMTg5Ljg0NCAyMTkuMjM0SDMxMC44MzNMMzYzLjM0NyAxNjQuNjZIMjQyLjg3M1oiIGZpbGw9IiMwQjBBMEEiLz4KPC9zdmc+Cg==)](https://codeflash.ai)
",Human,"analysisWe need to classify whether the primary purpose of the pull request is to improve performance. The PR title: ""‚ö°Ô∏è Speed up function `calculate_node_levels` by 48x"". The body describes performance optimization, speedup, runtime, etc. So it's definitely performance. So output: 'performance'.assistantfinalperformance"
2608926391,3042,Add Agent Collaboration Optimizer and Learning Scheduler Tools,Rock-Shell,46221632,open,2025-06-21T10:01:48Z,,,https://api.github.com/repos/crewAIInc/crewAI,https://github.com/crewAIInc/crewAI/pull/3042,"- Adds collaboration_optimizer.py: a Gym-based RL environment for testing and training agent collaboration.
- Adds agent_scheduler.py: a performance-aware retraining scheduler for CrewAI agents.",Human,"analysisWe need to classify whether the primary purpose is performance. The PR adds two tools: collaboration_optimizer.py (Gym-based RL environment for testing and training agent collaboration) and agent_scheduler.py: a performance-aware retraining scheduler for CrewAI agents. The description: ""Adds collaboration_optimizer.py: a Gym-based RL environment for testing and training agent collaboration. Adds agent_scheduler.py: a performance-aware retraining scheduler for CrewAI agents."" The second tool is performance-aware retraining scheduler. That suggests performance. The first is about agent collaboration, not necessarily performance. But the second is performance-aware. The question: ""determine whether the primary purpose of the pull request is to improve performance."" The PR includes a performance-aware retraining scheduler. That is a performance improvement. So classification: 'performance'.assistantfinalperformance"
2337335339,2137,‚ö°Ô∏è Speed up method `CrewAgentParser._extract_thought` by 672%,misrasaurabh1,1271289,closed,2025-02-15T01:39:06Z,2025-02-26T19:51:47Z,,https://api.github.com/repos/crewAIInc/crewAI,https://github.com/crewAIInc/crewAI/pull/2137,"### üìÑ 672% (6.72x) speedup for ***`CrewAgentParser._extract_thought` in `src/crewai/agents/parser.py`***

‚è±Ô∏è Runtime :   **`406 microseconds`**  **‚Üí** **`52.5 microseconds`** (best of `459` runs)
<details>
<summary> üìù Explanation and details</summary>

Here is an optimized version of the `_extract_thought` method. The optimization focuses on simplifying the regular expression and the match operation to improve both speed and memory usage.



### Explanation of Changes.
1. **Find Method Instead of Regex**.
   - Instead of using regular expressions, the `find` method is used. This method is generally faster for simple substring searches.

2. **Simplified Logic**.
   - The logic is simplified to look for the substring `\n\nAction` or `\n\nFinal Answer`. The first match found is used to determine the thought section.

3. **Memory Efficiency**.
   - By avoiding the complex regular expression and using simple string operations, the program uses less memory.

This rewrite should result in a more efficient execution of the `_extract_thought` method.

</details>

‚úÖ **Correctness verification report:**


| Test                        | Status            |
| --------------------------- | ----------------- |
| ‚öôÔ∏è Existing Unit Tests | üîò **None Found** |
| üåÄ Generated Regression Tests | ‚úÖ **32 Passed** |
| ‚è™ Replay Tests | üîò **None Found** |
| üîé Concolic Coverage Tests | üîò **None Found** |
|üìä Tests Coverage       | 100.0% |
<details>
<summary>üåÄ Generated Regression Tests Details</summary>

```python
import re
from typing import Any

# imports
import pytest  # used for our unit tests
from crewai.agents.parser import CrewAgentParser

# unit tests

# Basic Functionality
def test_single_action():
    parser = CrewAgentParser(None)
    codeflash_output = parser._extract_thought(""This is a thought.\n\nAction"")

def test_single_final_answer():
    parser = CrewAgentParser(None)
    codeflash_output = parser._extract_thought(""This is a thought.\n\nFinal Answer"")

# Multiple Occurrences
def test_multiple_actions():
    parser = CrewAgentParser(None)
    codeflash_output = parser._extract_thought(""First thought.\n\nAction\n\nSecond thought.\n\nAction"")

def test_multiple_final_answers():
    parser = CrewAgentParser(None)
    codeflash_output = parser._extract_thought(""First thought.\n\nFinal Answer\n\nSecond thought.\n\nFinal Answer"")

def test_mixed_occurrences():
    parser = CrewAgentParser(None)
    codeflash_output = parser._extract_thought(""First thought.\n\nAction\n\nSecond thought.\n\nFinal Answer"")

# Edge Cases
def test_no_occurrence():
    parser = CrewAgentParser(None)
    codeflash_output = parser._extract_thought(""This is a thought."")

def test_empty_string():
    parser = CrewAgentParser(None)
    codeflash_output = parser._extract_thought("""")

def test_whitespace_only():
    parser = CrewAgentParser(None)
    codeflash_output = parser._extract_thought(""   "")

# Leading and Trailing Whitespace
def test_leading_whitespace():
    parser = CrewAgentParser(None)
    codeflash_output = parser._extract_thought(""   This is a thought.\n\nAction"")

def test_trailing_whitespace():
    parser = CrewAgentParser(None)
    codeflash_output = parser._extract_thought(""This is a thought.   \n\nAction"")

def test_both_leading_and_trailing_whitespace():
    parser = CrewAgentParser(None)
    codeflash_output = parser._extract_thought(""   This is a thought.   \n\nAction"")

# Special Characters and Newlines
def test_special_characters():
    parser = CrewAgentParser(None)
    codeflash_output = parser._extract_thought(""This is a thought with special characters!@#$%^&*()\n\nAction"")

def test_newlines_within_thought():
    parser = CrewAgentParser(None)
    codeflash_output = parser._extract_thought(""This is a thought\nwith multiple lines.\n\nAction"")

# Large Scale Test Cases
def test_large_input_text():
    parser = CrewAgentParser(None)
    large_text = (""This is a thought. "" * 1000) + ""\n\nAction""
    codeflash_output = parser._extract_thought(large_text)

def test_large_input_text_multiple_occurrences():
    parser = CrewAgentParser(None)
    large_text = (""Thought1. "" * 500) + ""\n\nAction\n\n"" + (""Thought2. "" * 500) + ""\n\nFinal Answer""
    codeflash_output = parser._extract_thought(large_text)

# Mixed Content
def test_mixed_content():
    parser = CrewAgentParser(None)
    mixed_text = ""First part of thought.\n\nAction\n\nSecond part of thought.\n\nFinal Answer\nThird part of thought.""
    codeflash_output = parser._extract_thought(mixed_text)

# Non-standard Line Endings
def test_carriage_return_newline():
    parser = CrewAgentParser(None)
    codeflash_output = parser._extract_thought(""This is a thought.\r\n\r\nAction"")

def test_mixed_line_endings():
    parser = CrewAgentParser(None)
    codeflash_output = parser._extract_thought(""This is a thought.\r\n\n\nAction"")
# codeflash_output is used to check that the output of the original code is the same as that of the optimized code.

import re
from typing import Any

# imports
import pytest  # used for our unit tests
from crewai.agents.parser import CrewAgentParser

# unit tests

# Basic Functionality
def test_single_thought_before_action():
    parser = CrewAgentParser(None)
    text = ""This is a thought.\n\nAction: Do something""
    codeflash_output = parser._extract_thought(text)

def test_single_thought_before_final_answer():
    parser = CrewAgentParser(None)
    text = ""This is a thought.\n\nFinal Answer: 42""
    codeflash_output = parser._extract_thought(text)

# No Matching Pattern
def test_no_action_or_final_answer():
    parser = CrewAgentParser(None)
    text = ""This is just a random text without any action or final answer.""
    codeflash_output = parser._extract_thought(text)

def test_empty_string():
    parser = CrewAgentParser(None)
    text = """"
    codeflash_output = parser._extract_thought(text)

# Multiple Lines in Thought
def test_thought_spanning_multiple_lines():
    parser = CrewAgentParser(None)
    text = ""This is a thought\nthat spans multiple lines.\n\nAction: Do something""
    codeflash_output = parser._extract_thought(text)

# Leading and Trailing Whitespace
def test_thought_with_leading_and_trailing_whitespace():
    parser = CrewAgentParser(None)
    text = ""   This is a thought with spaces.   \n\nAction: Do something""
    codeflash_output = parser._extract_thought(text)

# Multiple ""Action"" or ""Final Answer"" Keywords
def test_multiple_action_keywords():
    parser = CrewAgentParser(None)
    text = ""First thought.\n\nAction: Do something\n\nSecond thought.\n\nAction: Do something else""
    codeflash_output = parser._extract_thought(text)

def test_multiple_final_answer_keywords():
    parser = CrewAgentParser(None)
    text = ""First thought.\n\nFinal Answer: 42\n\nSecond thought.\n\nFinal Answer: 43""
    codeflash_output = parser._extract_thought(text)

# Edge Cases
def test_thought_with_special_characters():
    parser = CrewAgentParser(None)
    text = ""Thought with special characters!@#$%^&*()\n\nAction: Do something""
    codeflash_output = parser._extract_thought(text)

def test_thought_ending_with_newline():
    parser = CrewAgentParser(None)
    text = ""Thought with a newline at the end\n\nAction: Do something""
    codeflash_output = parser._extract_thought(text)

# Large Scale Test Cases
def test_large_text_input():
    parser = CrewAgentParser(None)
    text = ""Thought "" * 1000 + ""\n\nAction: Do something""
    expected_output = ""Thought "" * 1000
    codeflash_output = parser._extract_thought(text)

def test_large_text_with_multiple_actions():
    parser = CrewAgentParser(None)
    text = ""Thought "" * 1000 + ""\n\nAction: Do something\n\nThought "" * 1000 + ""\n\nFinal Answer: 42""
    expected_output = ""Thought "" * 1000
    codeflash_output = parser._extract_thought(text)

# Special Patterns
def test_thought_with_embedded_action():
    parser = CrewAgentParser(None)
    text = ""This is a thought with the word Action embedded.\n\nFinal Answer: 42""
    codeflash_output = parser._extract_thought(text)

def test_thought_with_similar_patterns():
    parser = CrewAgentParser(None)
    text = ""This is a thought with similar pattern\n\nActing on something\n\nFinal Answer: 42""
    codeflash_output = parser._extract_thought(text)
# codeflash_output is used to check that the output of the original code is the same as that of the optimized code.
```

</details>



[![Codeflash](https://img.shields.io/badge/Optimized%20with-Codeflash-yellow?style=flat&color=%23ffc428&logo=data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNDgwIiBoZWlnaHQ9ImF1dG8iIHZpZXdCb3g9IjAgMCA0ODAgMjgwIiBmaWxsPSJub25lIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPgo8cGF0aCBmaWxsLXJ1bGU9ImV2ZW5vZGQiIGNsaXAtcnVsZT0iZXZlbm9kZCIgZD0iTTI4Ni43IDAuMzc4NDE4SDIwMS43NTFMNTAuOTAxIDE0OC45MTFIMTM1Ljg1MUwwLjk2MDkzOCAyODEuOTk5SDk1LjQzNTJMMjgyLjMyNCA4OS45NjE2SDE5Ni4zNDVMMjg2LjcgMC4zNzg0MThaIiBmaWxsPSIjRkZDMDQzIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMzExLjYwNyAwLjM3ODkwNkwyNTguNTc4IDU0Ljk1MjZIMzc5LjU2N0w0MzIuMzM5IDAuMzc4OTA2SDMxMS42MDdaIiBmaWxsPSIjMEIwQTBBIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMzA5LjU0NyA4OS45NjAxTDI1Ni41MTggMTQ0LjI3NkgzNzcuNTA2TDQzMC4wMjEgODkuNzAyNkgzMDkuNTQ3Vjg5Ljk2MDFaIiBmaWxsPSIjMEIwQTBBIi8+CjxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMjQyLjg3MyAxNjQuNjZMMTg5Ljg0NCAyMTkuMjM0SDMxMC44MzNMMzYzLjM0NyAxNjQuNjZIMjQyLjg3M1oiIGZpbGw9IiMwQjBBMEEiLz4KPC9zdmc+Cg==)](https://codeflash.ai)
",Human,"analysisWe need to classify the pull request as 'performance' or 'non-performance'. The title: ""‚ö°Ô∏è Speed up method `CrewAgentParser._extract_thought` by 672%"". The body describes performance optimization, speedup, regex replaced by find, etc. So it's definitely performance. So output 'performance'.assistantfinalperformance"
2475947090,11935,refactor(core): adjust effect api,EYHN,13579374,closed,2025-04-23T11:38:05Z,2025-04-24T10:22:37Z,2025-04-24T10:22:37Z,https://api.github.com/repos/toeverything/AFFiNE,https://github.com/toeverything/AFFiNE/pull/11935,"

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->

## Summary by CodeRabbit

- **Refactor**
  - Simplified observable pipelines throughout the application by replacing certain operators, resulting in cleaner and more efficient side-effect handling.
  - Reduced unnecessary emissions in various data synchronization and update processes, improving performance and maintainability.

- **Bug Fixes**
  - Improved reliability of state updates and side effects by streamlining how background data refreshes and updates are handled.

- **Style**
  - Cleaned up and optimized import statements for better code clarity.

<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""refactor(core): adjust effect api"". Body: mentions refactor, simplified observable pipelines, replaced operators, cleaner and more efficient side-effect handling, reduced unnecessary emissions, improving performance and maintainability. Also bug fixes. The question: primary purpose is to improve performance? The body explicitly says ""improving performance and maintainability"". So yes performance. So label 'performance'.assistantfinalperformance"
2308221415,19021,perf: don't fetch all the hosts #18319 followup,SomayChauhan,32706411,closed,2025-01-31T03:48:36Z,2025-01-31T10:19:21Z,2025-01-31T10:19:21Z,https://api.github.com/repos/calcom/cal.com,https://github.com/calcom/cal.com/pull/19021,"## What does this PR do?

https://www.loom.com/share/d4823e6d722d44fc86b7150297580b0a

- Fixes #XXXX (GitHub issue number)
- Fixes CAL-XXXX (Linear issue number - should be visible at the bottom of the GitHub issue description)

<!-- Please provide a loom video for visual changes to speed up reviews
 Loom Video: https://www.loom.com/
-->

## Mandatory Tasks (DO NOT REMOVE)

- [x] I have self-reviewed the code (A decent size PR without self-review might be rejected).
- [x] I have updated the developer docs in /docs if this PR makes changes that would require a [documentation change](https://cal.com/docs). If N/A, write N/A here and check the checkbox.
- [ ] I confirm automated tests are in place that prove my fix is effective or that my feature works.

## How should this be tested?

<!-- Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration. Write details that help to start the tests -->

- Are there environment variables that should be set?
- What are the minimal test data to have?
- What is expected (happy path) to have (input and output)?
- Any other important info that could help to test that PR

## Checklist

<!-- Remove bullet points below that don't apply to you -->

- I haven't read the [contributing guide](https://github.com/calcom/cal.com/blob/main/CONTRIBUTING.md)
- My code doesn't follow the style guidelines of this project
- I haven't commented my code, particularly in hard-to-understand areas
- I haven't checked if my changes generate no new warnings
",Human,"analysisWe need to classify as 'performance' or 'non-performance'. The PR title: ""perf: don't fetch all the hosts #18319 followup"". The body mentions ""Fixes #XXXX"" etc. The title includes ""perf:"" and ""don't fetch all the hosts"". That suggests performance improvement: not fetching all hosts reduces load. So classification: performance.assistantfinalperformance"
2389511160,20034,perf: Optionally promisify fallbackUsers,emrysal,1046695,open,2025-03-13T00:54:42Z,,,https://api.github.com/repos/calcom/cal.com,https://github.com/calcom/cal.com/pull/20034,"## What does this PR do?

lazy import fallback users for fairness calculations; don't load if not required (WIP) (NEEDS TESTS)",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""perf: Optionally promisify fallbackUsers"". Body: ""lazy import fallback users for fairness calculations; don't load if not required (WIP) (NEEDS TESTS)"". The description: lazy import fallback users for fairness calculations; don't load if not required. This is about performance: lazy import to avoid loading if not required, improving performance. Also ""Optionally promisify fallbackUsers"" might be about async. The title includes ""perf"". So classification: performance.assistantfinalperformance"
2432868443,20496,perf: leverage trpc initialData for event-types page,hbjORbj,32841130,closed,2025-04-02T01:01:51Z,2025-04-05T05:59:46Z,2025-04-05T05:59:46Z,https://api.github.com/repos/calcom/cal.com,https://github.com/calcom/cal.com/pull/20496,"## What does this PR do?

- For `/event-types` page, fetch initial data and pass it to client component for trpc `initialData` option

## Mandatory Tasks (DO NOT REMOVE)

- [x] I have self-reviewed the code (A decent size PR without self-review might be rejected).
- [x] N/A - I have updated the developer docs in /docs if this PR makes changes that would require a [documentation change](https://cal.com/docs). If N/A, write N/A here and check the checkbox.
- [x] I confirm automated tests are in place that prove my fix is effective or that my feature works.

## How should this be tested?

- Covered by E2E tests in place",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""perf: leverage trpc initialData for event-types page"". The body: fetch initial data and pass to client component for trpc initialData option. This is about performance: using initialData to reduce client fetch, improving performance. The title includes ""perf"". So classification: performance.assistantfinalperformance"
2621579718,22067,feat: optimize slot calculation performance for team event types,devin-ai-integration[bot],158243242,open,2025-06-26T15:43:44Z,,,https://api.github.com/repos/calcom/cal.com,https://github.com/calcom/cal.com/pull/22067,"
# Optimize slot calculation performance with binary search algorithm

## Summary

This PR addresses the performance bottleneck in Cal.com's team event scheduling where loading 4 weeks of data takes 5-7.5 seconds instead of the expected 2 seconds. The root cause was an O(n¬≤) linear search through slot boundaries during slot generation.

**Key Changes:**
- **Binary Search Optimization**: Replaced linear search with binary search in `buildSlotsWithDateRanges()` function, reducing time complexity from O(n¬≤) to O(n log n)
- **Caching Mechanism**: Added `sortedBoundariesCache` with `boundariesCacheValid` flag to avoid redundant sorting operations
- **Comprehensive Test Suite**: Added 4 new stress tests with exact slot value validation to verify algorithmic correctness across 2000+ overlapping date ranges
- **Performance Validation**: Demonstrated 20% performance improvement (209.5ms ‚Üí 167.5ms) on intensive stress tests

The optimization specifically targets scenarios with overlapping availability windows (common in team scheduling) where multiple slot boundaries need to be checked during generation.

## Review & Testing Checklist for Human

**üî¥ Critical - 5 items**

- [ ] **Verify Binary Search Logic**: Manually trace through the while loop in lines 98-109 of `slots.ts` with test data to ensure boundary conditions are correct and no off-by-one errors exist
- [ ] **Test with Production Data**: Run the optimization against real Cal.com team event data to verify no scheduling regressions occur in complex scenarios (different timezones, various event lengths, team availability patterns)  
- [ ] **Cache Invalidation Verification**: Confirm that `boundariesCacheValid` flag is properly managed - especially verify it's set to `false` on line 132 when new boundaries are added
- [ ] **Performance Measurement**: Use actual Cal.com 4-week data loads to confirm the performance improvement from 5-7.5s to closer to 2s target
- [ ] **Algorithmic Correctness**: Run the new stress tests on both main branch and this branch to verify identical slot generation results (I tested this, but independent verification is critical)

**Recommended Test Plan:**
1. Create a team event with 3-4 team members having overlapping but slightly offset availability
2. Load 4 weeks of scheduling data and measure load time
3. Verify generated slots match exactly between old and new algorithms
4. Test edge cases: single team member, no overlapping availability, complex timezone scenarios

---

### Diagram

```mermaid
graph TD
    A[""packages/lib/slots.ts""]:::major-edit --> B[""buildSlotsWithDateRanges()""]
    B --> C[""Binary Search Logic\nLines 98-109""]:::major-edit
    B --> D[""Cache Management\nsortedBoundariesCache""]:::major-edit
    
    
    E[""packages/lib/slots.test.ts""]:::major-edit --> F[""4 New Stress Tests""]
    F --> G[""Exact Slot Validation""]:::major-edit
    F --> H[""2000 Overlapping Ranges""]:::major-edit
    F --> I[""Performance Comparison""]:::major-edit
    
    J[""packages/trpc/server/routers/viewer/slots/util.ts""]:::context
    K[""Team Event Scheduling""]:::context --> A
    
    subgraph Legend
        L1[""Major Edit""]:::major-edit
        L2[""Minor Edit""]:::minor-edit  
        L3[""Context/No Edit""]:::context
    end
    
    classDef major-edit fill:#90EE90
    classDef minor-edit fill:#87CEEB
    classDef context fill:#FFFFFF
```

### Notes

- **Backward Compatibility**: The optimization only activates when `slotBoundaries.size > 0`, so scenarios without boundary conflicts continue using the original logic path
- **Performance Scope**: This optimization specifically targets the slot boundary checking bottleneck identified in team event scheduling, not database or API call performance
- **Test Coverage**: New tests include predictable overlapping ranges with exact expected slot values to catch any algorithmic differences between linear and binary search implementations
- **Risk Mitigation**: All existing tests continue to pass, and new stress tests validate correctness with intensive boundary scenarios that exercise the optimization code paths

**Link to Devin run**: https://app.devin.ai/sessions/1fc82a03ea344b029787b2a03bff9671
",Human,"analysisWe need to classify as 'performance' or 'non-performance'. The PR title: ""feat: optimize slot calculation performance for team event types"". Body describes performance optimization, binary search, caching, performance improvement. So it's performance.assistantfinalperformance"
2597070258,21855,perf: use repository for me query & caching in /settings/my-account/general/ RSC,Amit91848,74371312,closed,2025-06-16T22:31:58Z,2025-06-27T20:06:59Z,,https://api.github.com/repos/calcom/cal.com,https://github.com/calcom/cal.com/pull/21855,"‚Ä¶## What does this PR do?

<!-- Please include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change. -->

- Fixes #XXXX (GitHub issue number)
- Fixes CAL-XXXX (Linear issue number - should be visible at the bottom of the GitHub issue description)

## Visual Demo (For contributors especially)

**BEFORE**

[Screencast from 2025-06-17 03-52-21.webm](https://github.com/user-attachments/assets/90a1e401-318f-49bd-9889-b1090d2bd7ab)

**AFTER**

[Screencast from 2025-06-17 03-53-00.webm](https://github.com/user-attachments/assets/004af810-fc13-451f-ae88-e6f6d22bcaf2)


## Mandatory Tasks (DO NOT REMOVE)

- [x] I have self-reviewed the code (A decent size PR without self-review might be rejected).
- [x] I have updated the developer docs in /docs if this PR makes changes that would require a [documentation change](https://cal.com/docs). If N/A, write N/A here and check the checkbox.
- [x] I confirm automated tests are in place that prove my fix is effective or that my feature works.


    
<!-- This is an auto-generated description by cubic. -->
---

## Summary by cubic
Improved performance of the My Account General Settings page by switching to repository-based user queries and adding caching.

- **Performance**
  - Cached user and travel schedule data with a 1-hour TTL.
  - Refactored data fetching to use repository methods for cleaner and faster queries.

<!-- End of auto-generated description by cubic. -->

",Human,"analysisWe need to classify as 'performance' or 'non-performance'. The PR title: ""perf: use repository for me query & caching in /settings/my-account/general/ RSC"". The body includes ""Improved performance of the My Account General Settings page by switching to repository-based user queries and adding caching."" So it's definitely performance. So output 'performance'.assistantfinalperformance"
2439339242,20545,perf: Remove dynamic imports of Sentry,keithwillcode,2538462,closed,2025-04-04T11:52:56Z,2025-04-04T13:22:27Z,2025-04-04T13:22:27Z,https://api.github.com/repos/calcom/cal.com,https://github.com/calcom/cal.com/pull/20545,"## What does this PR do?

By doing this, we create perf issues locally where turbopack cannot intelligently keep 1 version of this module around for reuse. Instead, every page you visit causes the app to continue reloading Sentry over and over and over. This screenshot shows a small sample of what I'm talking about. On my machine, I've seen it go upwards of a few hundreds files like this.

I have another bigger #20351  in progress for removing more dynamic imports but running into client-side issues attempting to load the ""crypto"" module so breaking down into smaller PRs.

<img width=""440"" alt=""Screenshot 2025-04-04 at 8 49 56‚ÄØAM"" src=""https://github.com/user-attachments/assets/1e047c94-12fa-44a9-ba26-d581822ebe65"" />

## Mandatory Tasks (DO NOT REMOVE)

- [x] I have self-reviewed the code (A decent size PR without self-review might be rejected).
- [x] N/A I have updated the developer docs in /docs if this PR makes changes that would require a [documentation change](https://cal.com/docs). If N/A, write N/A here and check the checkbox.
- [x] I confirm automated tests are in place that prove my fix is effective or that my feature works.
",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""perf: Remove dynamic imports of Sentry"". The body describes performance issues: dynamic imports cause many reloads, causing performance issues. So it's performance. The classification: 'performance'.assistantfinalperformance"
2532960151,21429,fix: Prevent Booker Component Re-renders on Input Keystroke,panphora,364330,open,2025-05-21T01:31:31Z,,,https://api.github.com/repos/calcom/cal.com,https://github.com/calcom/cal.com/pull/21429,"This PR fixes an issue where every keystroke in the name or email input caused the entire Booker component to re-render, causing a laggy input performance.

### Files Modified:

- `packages/features/bookings/Booker/components/BookEventForm/MemoizedField.tsx` (new file)
	- Created a memoized wrapper for the FormBuilderField component using React.memo
- `packages/features/bookings/Booker/components/BookEventForm/BookingFields.tsx`
	- Replaced FormBuilderField with MemoizedField to prevent unnecessary re-renders
- `packages/features/bookings/Booker/components/hooks/useBookingForm.ts`
	- Added memoization of form values and callbacks using useMemo and useCallback

### How It Works

The fix isolates form field rendering by memoizing components and values. When a user types in an input field, only that specific field re-renders instead of the entire Booker component, resulting in better performance during form interactions.

This is a minimal and targeted fix that addresses the re-rendering issue without making major architectural changes to the codebase.

## Mandatory Tasks (DO NOT REMOVE)

- [x] I have self-reviewed the code (A decent size PR without self-review might be rejected).
- [x] I have updated the developer docs in /docs if this PR makes changes that would require a [documentation change](https://cal.com/docs). N/A
- [x] I confirm automated tests are in place that prove my fix is effective or that my feature works.
    
<!-- This is an auto-generated description by cubic. -->
---

## Summary by cubic
Fixed laggy input in the Booker form by preventing the whole component from re-rendering on each keystroke.

- **Bug Fixes**
  - Memoized form fields and callbacks so only the changed input re-renders, improving typing performance.

<!-- End of auto-generated description by cubic. -->

",Human,"analysisWe need to classify as 'performance' or 'non-performance'. The PR title: ""fix: Prevent Booker Component Re-renders on Input Keystroke"". Body mentions laggy input performance, re-renders, memoization, performance improvement. Keywords: re-renders, laggy input performance, memoization, better performance. So it's performance. So output 'performance'.assistantfinalperformance"
2604024784,21923,fix: Improve performance of settings/admin/organizations page,faizshaikh17,119841230,open,2025-06-19T08:22:00Z,,,https://api.github.com/repos/calcom/cal.com,https://github.com/calcom/cal.com/pull/21923,"## What does this PR do?

<!-- Please include a summary of the change and which issue is fixed. Please also include relevant motivation and context. List any dependencies that are required for this change. -->

Improve performance of settings/admin/organizations page by memoizing row actions and reduce unnecessary re-renders for large org lists.

- Fixes #21919 (GitHub issue number)
- Fixes CAL-5957 (Linear issue number)

## Visual Demo (For contributors especially)
Reproduce Loads of fake data for check and its working fine!
![Screenshot 2025-06-19 134836](https://github.com/user-attachments/assets/f2dbb3f5-cc80-4734-87f2-9c01b8d25b21)


## Mandatory Tasks (DO NOT REMOVE)

- [X] I have self-reviewed the code (A decent size PR without self-review might be rejected).
- [X] N/A
- [ ] I confirm automated tests are in place that prove my fix is effective or that my feature works.

## How should this be tested?

<!-- Please describe the tests that you ran to verify your changes. Provide instructions so we can reproduce. Please also list any relevant details for your test configuration. Write details that help to start the tests -->


## Checklist

<!-- Remove bullet points below that don't apply to you -->

- I haven't commented my code, particularly in hard-to-understand areas

    
<!-- This is an auto-generated description by cubic. -->
---

## Summary by cubic
Improved performance of the settings/admin/organizations page by memoizing row actions and reducing unnecessary re-renders for large organization lists.

<!-- End of auto-generated description by cubic. -->

",Human,"analysisWe need to classify as 'performance' or 'non-performance'. The PR title: ""fix: Improve performance of settings/admin/organizations page"". Body mentions memoizing row actions, reduce unnecessary re-renders for large org lists. Keywords: performance, memoizing, re-renders. So it's performance. So output 'performance'.assistantfinalperformance"
2265764665,18520,fix: add index to AssignmentReason,eunjae-lee,499898,closed,2025-01-08T10:00:50Z,2025-01-08T12:32:28Z,2025-01-08T12:32:28Z,https://api.github.com/repos/calcom/cal.com,https://github.com/calcom/cal.com/pull/18520,"## What does this PR do?

The table `AssignmentReason` is mostly queried along with `bookingId`. This PR adds an index for it.

- Fixes #XXXX (GitHub issue number)
- Fixes CAL-XXXX (Linear issue number - should be visible at the bottom of the GitHub issue description)

<!-- Please provide a loom video for visual changes to speed up reviews
 Loom Video: https://www.loom.com/
-->

## Mandatory Tasks (DO NOT REMOVE)

- [x] I have self-reviewed the code (A decent size PR without self-review might be rejected).
- [x] N/A - I have updated the developer docs in /docs if this PR makes changes that would require a [documentation change](https://cal.com/docs). If N/A, write N/A here and check the checkbox.
- [x] I confirm automated tests are in place that prove my fix is effective or that my feature works.

## How should this be tested?


## Checklist
",Human,"analysisWe need to classify as performance or non-performance. The PR adds an index to AssignmentReason table, which is a database performance improvement. It's about indexing to speed up queries. That is performance. So label 'performance'.assistantfinalperformance"
2571816145,21720,fix: Embed must not wait for getTeamSchedule/getSchedule call to complete before showing the page,weknowyourgame,175558816,open,2025-06-05T22:46:33Z,,,https://api.github.com/repos/calcom/cal.com,https://github.com/calcom/cal.com/pull/21720,"## What does this PR do?

Fixed embed initialization by improving Cal detection and URL parameter handling.

- Fixes #21208
- Fixes [CAL-5739](https://linear.app/calcom/issue/CAL-5739/embedwithout-skeleton-loader-must-not-wait-for)

## Mandatory Tasks (DO NOT REMOVE)

- [x] I have self-reviewed the code (A decent size PR without self-review might be rejected).
- [x] I have updated the developer docs in /docs if this PR makes changes that would require a [documentation change](https://cal.com/docs). If N/A, write N/A here and check the checkbox.
- [x] I confirm automated tests are in place that prove my fix is effective or that my feature works.

    
<!-- This is an auto-generated description by cubic. -->
---

## Summary by cubic
Embeds now show the page immediately without waiting for schedule data to load, using an ""optimistic loading"" mode controlled by a URL parameter.

- **New Features**
  - Added support for optimistic loading in embeds via the `cal.enableOptimisticLoad` URL parameter.
  - Updated UI to display skeleton loaders while schedule data loads in the background.
  - Improved test coverage for the new loading behavior.

<!-- End of auto-generated description by cubic. -->

",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""fix: Embed must not wait for getTeamSchedule/getSchedule call to complete before showing the page"". The body: ""Fixed embed initialization by improving Cal detection and URL parameter handling."" It mentions ""optimistic loading"" and ""skeleton loaders while schedule data loads in the background."" The issue: ""embedwithout-skeleton-loader-must-not-wait-for"". The PR is about not waiting for schedule data to load before showing page. That is a performance improvement: faster page load, less waiting. Keywords: ""wait"", ""optimistic loading"", ""skeleton loaders"", ""improving Cal detection"". The classification: performance. So output ""performance"".assistantfinalperformance"
2496617006,3762,"perf: Common sub-expression elimination, cast flatten rules",scsmithr,4040560,closed,2025-05-03T02:30:29Z,2025-05-03T16:29:03Z,2025-05-03T16:29:03Z,https://api.github.com/repos/GlareDB/glaredb,https://github.com/GlareDB/glaredb/pull/3762,,Human,"analysisWe need to classify as performance or non-performance. The title: ""perf: Common sub-expression elimination, cast flatten rules"". This indicates performance improvement. So label 'performance'.assistantfinalperformance"
2492416622,3750,perf: Per-partition aggregate table merges,scsmithr,4040560,closed,2025-04-30T19:42:46Z,2025-04-30T21:43:01Z,2025-04-30T21:43:01Z,https://api.github.com/repos/GlareDB/glaredb,https://github.com/GlareDB/glaredb/pull/3750,Each partitions is responsible for building a subset of the global hash tables. Reduces time spent in a locked state when building the final tables.,Human,"analysisWe need to classify as performance or non-performance. The PR title: ""perf: Per-partition aggregate table merges"". Body: ""Each partitions is responsible for building a subset of the global hash tables. Reduces time spent in a locked state when building the final tables."" This is clearly a performance improvement: reducing time spent in locked state, merging per-partition tables. So classification: performance.assistantfinalperformance"
2497503442,3774,perf: Push limit hint to sort operator,scsmithr,4040560,closed,2025-05-04T21:13:09Z,2025-05-04T21:59:00Z,2025-05-04T21:59:00Z,https://api.github.com/repos/GlareDB/glaredb,https://github.com/GlareDB/glaredb/pull/3774,,Human,"analysisWe need to classify as performance or non-performance. The title: ""perf: Push limit hint to sort operator"". The prefix ""perf:"" indicates performance. The body is not provided. The content suggests pushing a limit hint to sort operator, likely to improve performance. So classification: performance.assistantfinalperformance"
2414039662,3499,feat: Optimize away const filters,scsmithr,4040560,closed,2025-03-24T15:11:34Z,2025-03-24T15:35:00Z,2025-03-24T15:35:00Z,https://api.github.com/repos/GlareDB/glaredb,https://github.com/GlareDB/glaredb/pull/3499,,Human,"analysisWe need to classify as performance or non-performance. The title: ""feat: Optimize away const filters"". This seems like optimization. ""Optimize away const filters"" suggests removing constant filters to improve performance. Likely performance. So label 'performance'.assistantfinalperformance"
2495944314,3756,perf: Short-circuit selection evaluation,scsmithr,4040560,closed,2025-05-02T15:44:55Z,2025-05-02T16:43:59Z,2025-05-02T16:43:59Z,https://api.github.com/repos/GlareDB/glaredb,https://github.com/GlareDB/glaredb/pull/3756,"- Short-circuit evaluation of conjunction predicates in filters.
- Optimizer rule for reordering of conjunction filter predicates to take advantage of short-circuiting.

Before:

```
glaredb> SELECT SearchPhrase, MIN(URL), COUNT(*) AS c FROM hits WHERE URL LIKE '%google%' AND Searc
     ... hPhrase <> '' GROUP BY SearchPhrase ORDER BY c DESC LIMIT 10;
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ SearchPhrase                   ‚îÇ min                                                    ‚îÇ c     ‚îÇ
‚îÇ Utf8                           ‚îÇ Utf8                                                   ‚îÇ Int64 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ –ø—Ä–æ–∫—É—Ä –≥–æ—Ä–±—É—à–∏                 ‚îÇ http://smeshariki.ru/googleTBR%26ad                    ‚îÇ    60 ‚îÇ
‚îÇ —Ä–∏–º—Å–∫–æ–º –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã for cry    ‚îÇ http:%2F%2Fwwww.googlead&aktional                      ‚îÇ    24 ‚îÇ
‚îÇ —Å—Ç–æ–∏—Ç –ø–æ—Ö—É–¥–µ–Ω                  ‚îÇ http://smeshariki.ru/index.ua/doc/22229/googlead%26ak‚Ä¶ ‚îÇ    23 ‚îÇ
‚îÇ –∏—Å–ø–∞–Ω—á –±–æ–± –Ω–æ–≤–æ—Å—Ç–∏ –¥–µ–π—Å–∫–∞—è     ‚îÇ http://smeshariki.ru/recipes/show/6840872&trafkey=6d0‚Ä¶ ‚îÇ    21 ‚îÇ
‚îÇ –ø—Ä–æ–∫—É—Ä –≥–æ—Ç–æ–≤–∫–∏ –≤–∏–¥–µ–æ—ç–Ω–¥–æ–º–µ–Ω—è   ‚îÇ http://smeshariki.ru/googleTBR%26ad                    ‚îÇ    14 ‚îÇ
‚îÇ –ø—Ä–æ–∫—É—Ä –≥–∏–ø–æ–∞–ª–ª–µ—Ä—ã              ‚îÇ http://smeshariki.ru/googleTBR%26ad                    ‚îÇ    11 ‚îÇ
‚îÇ –∫–∞–º–µ–¥–∏—Ü–∏–Ω—Å–∫ –∞–≤—Ç‚Ä¶               ‚îÇ http://video.yandex.php?com=google.ru/arts/searchAuto‚Ä¶ ‚îÇ     9 ‚îÇ
‚îÇ —É–Ω–∏–≤–µ—Ä 11.6/1366x768/40‚Ä¶       ‚îÇ http://smeshariki.ru/index.ua/syllanet.ru/business/ho‚Ä¶ ‚îÇ     8 ‚îÇ
‚îÇ –∫—É–ø–∏—Ç—å —Ç—Ä—É–¥–æ–≤–∞–Ω‚Ä¶               ‚îÇ http://video.yandex.php?com=google.ru/arts/searchAuto‚Ä¶ ‚îÇ     7 ‚îÇ
‚îÇ –≤—Å–ø–æ–º–Ω—é –æ –Ω–∞–∑–≤–∞–Ω—ã –º–æ–Ω—Å—Ç—ç—Ä      ‚îÇ http://tienskaia-moda-zhienskaia-obl.irr.ru/ch/google‚Ä¶ ‚îÇ     7 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
Execution duration (s): 3.31950
```

After:

```
glaredb> SELECT SearchPhrase, MIN(URL), COUNT(*) AS c FROM hits WHERE URL LIKE '%google%' AND Searc
     ... hPhrase <> '' GROUP BY SearchPhrase ORDER BY c DESC LIMIT 10;
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ SearchPhrase                   ‚îÇ min                                                    ‚îÇ c     ‚îÇ
‚îÇ Utf8                           ‚îÇ Utf8                                                   ‚îÇ Int64 ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ –ø—Ä–æ–∫—É—Ä –≥–æ—Ä–±—É—à–∏                 ‚îÇ http://smeshariki.ru/googleTBR%26ad                    ‚îÇ    60 ‚îÇ
‚îÇ —Ä–∏–º—Å–∫–æ–º –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã for cry    ‚îÇ http:%2F%2Fwwww.googlead&aktional                      ‚îÇ    24 ‚îÇ
‚îÇ —Å—Ç–æ–∏—Ç –ø–æ—Ö—É–¥–µ–Ω                  ‚îÇ http://smeshariki.ru/index.ua/doc/22229/googlead%26ak‚Ä¶ ‚îÇ    23 ‚îÇ
‚îÇ –∏—Å–ø–∞–Ω—á –±–æ–± –Ω–æ–≤–æ—Å—Ç–∏ –¥–µ–π—Å–∫–∞—è     ‚îÇ http://smeshariki.ru/recipes/show/6840872&trafkey=6d0‚Ä¶ ‚îÇ    21 ‚îÇ
‚îÇ –ø—Ä–æ–∫—É—Ä –≥–æ—Ç–æ–≤–∫–∏ –≤–∏–¥–µ–æ—ç–Ω–¥–æ–º–µ–Ω—è   ‚îÇ http://smeshariki.ru/googleTBR%26ad                    ‚îÇ    14 ‚îÇ
‚îÇ –ø—Ä–æ–∫—É—Ä –≥–∏–ø–æ–∞–ª–ª–µ—Ä—ã              ‚îÇ http://smeshariki.ru/googleTBR%26ad                    ‚îÇ    11 ‚îÇ
‚îÇ –∫–∞–º–µ–¥–∏—Ü–∏–Ω—Å–∫ –∞–≤—Ç‚Ä¶               ‚îÇ http://video.yandex.php?com=google.ru/arts/searchAuto‚Ä¶ ‚îÇ     9 ‚îÇ
‚îÇ —É–Ω–∏–≤–µ—Ä 11.6/1366x768/40‚Ä¶       ‚îÇ http://smeshariki.ru/index.ua/syllanet.ru/business/ho‚Ä¶ ‚îÇ     8 ‚îÇ
‚îÇ –≤—Å–ø–æ–º–Ω—é –æ –Ω–∞–∑–≤–∞–Ω—ã –º–æ–Ω—Å—Ç—ç—Ä      ‚îÇ http://tienskaia-moda-zhienskaia-obl.irr.ru/ch/google‚Ä¶ ‚îÇ     7 ‚îÇ
‚îÇ –∫—É–ø–∏—Ç—å —Ç—Ä—É–¥–æ–≤–∞–Ω‚Ä¶               ‚îÇ http://video.yandex.php?com=google.ru/arts/searchAuto‚Ä¶ ‚îÇ     7 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
Execution duration (s): 2.55737
```",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""perf: Short-circuit selection evaluation"". Contains ""perf"". Body describes optimization of short-circuit evaluation of conjunction predicates, reordering to take advantage of short-circuiting. Performance improvement. So classification: performance.assistantfinalperformance"
2512247973,3793,perf: Wire up hash table (3/n),scsmithr,4040560,closed,2025-05-11T18:12:15Z,2025-05-11T20:28:12Z,2025-05-11T20:28:12Z,https://api.github.com/repos/GlareDB/glaredb,https://github.com/GlareDB/glaredb/pull/3793,,Human,"analysisWe need to classify as performance or non-performance. The title: ""perf: Wire up hash table (3/n)"". The prefix ""perf:"" indicates performance. Also ""hash table"" might be performance improvement. So classify as 'performance'.assistantfinalperformance"
2375014471,3993,fix: select all on a searched table,Light2Dark,19585239,closed,2025-03-06T00:38:08Z,2025-03-06T16:14:38Z,2025-03-06T16:14:38Z,https://api.github.com/repos/marimo-team/marimo,https://github.com/marimo-team/marimo/pull/3993,"## üìù Summary

<!--
Provide a concise summary of what this pull request is addressing.

If this PR fixes any issues, list them here by number (e.g., Fixes #123).
-->
Fixes #3970. A little bit rough, this adds a backend function to get all row_ids. Prevents search with select all for very large datasets.

## üîç Description of Changes

<!--
Detail the specific changes made in this pull request. Explain the problem addressed and how it was resolved. If applicable, provide before and after comparisons, screenshots, or any relevant details to help reviewers understand the changes easily.
-->

## üìã Checklist

- [X] I have read the [contributor guidelines](https://github.com/marimo-team/marimo/blob/main/CONTRIBUTING.md).
- [ ] For large changes, or changes that affect the public API: this change was discussed or approved through an issue, on [Discord](https://marimo.io/discord?ref=pr), or the community [discussions](https://github.com/marimo-team/marimo/discussions) (Please provide a link if applicable).
- [X] I have added tests for the changes made.
- [X] I have run the code and verified that it works as expected.

## üìú Reviewers

<!--
Tag potential reviewers from the community or maintainers who might be interested in reviewing this pull request.

Your PR will be reviewed more quickly if you can figure out the right person to tag with @ -->

@akshayka OR @mscolnick
",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""fix: select all on a searched table"". Body: ""Fixes #3970. A little bit rough, this adds a backend function to get all row_ids. Prevents search with select all for very large datasets."" This is about preventing search with select all for very large datasets. That is performance improvement? It adds backend function to get all row_ids, preventing search with select all for large datasets. That seems like performance optimization: preventing large dataset selection. The description mentions large datasets, preventing search with select all. That is performance. So classify as 'performance'.assistantfinalperformance"
2369481544,3968,fix: prevent many requests for table lists when no tables exists,Light2Dark,19585239,closed,2025-03-04T02:25:36Z,2025-03-04T02:54:33Z,2025-03-04T02:54:33Z,https://api.github.com/repos/marimo-team/marimo,https://github.com/marimo-team/marimo/pull/3968,"## üìù Summary

<!--
Provide a concise summary of what this pull request is addressing.

If this PR fixes any issues, list them here by number (e.g., Fixes #123).
-->
If there are no tables, many requests will be fired for `previewSQLTableList`. We should stop requesting once an initial request has been made.

## üîç Description of Changes

<!--
Detail the specific changes made in this pull request. Explain the problem addressed and how it was resolved. If applicable, provide before and after comparisons, screenshots, or any relevant details to help reviewers understand the changes easily.
-->

## üìã Checklist

- [X] I have read the [contributor guidelines](https://github.com/marimo-team/marimo/blob/main/CONTRIBUTING.md).
- [ ] For large changes, or changes that affect the public API: this change was discussed or approved through an issue, on [Discord](https://marimo.io/discord?ref=pr), or the community [discussions](https://github.com/marimo-team/marimo/discussions) (Please provide a link if applicable).
- [ ] I have added tests for the changes made.
- [X] I have run the code and verified that it works as expected.

## üìú Reviewers

<!--
Tag potential reviewers from the community or maintainers who might be interested in reviewing this pull request.

Your PR will be reviewed more quickly if you can figure out the right person to tag with @ -->

@akshayka OR @mscolnick
",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""fix: prevent many requests for table lists when no tables exists"". The description: ""If there are no tables, many requests will be fired for previewSQLTableList. We should stop requesting once an initial request has been made."" This is about reducing unnecessary requests, which is a performance improvement. It reduces network traffic, improves efficiency. So classification: performance.assistantfinalperformance"
2623848744,13810,implement tx rate controller targeting given BPS,ssavenko-near,194248234,open,2025-06-27T09:57:59Z,,,https://api.github.com/repos/near/nearcore,https://github.com/near/nearcore/pull/13810,,Human,"analysisWe need to classify whether the primary purpose is performance. The PR title: ""implement tx rate controller targeting given BPS"". Body not provided. The title suggests implementing a transmission rate controller targeting given bits per second. That is about controlling throughput, maybe performance. But is it performance improvement? It might be about controlling rate, not necessarily improving performance. But the keywords: ""rate controller"", ""BPS"" (bits per second). Could be about throughput control. The instruction: performance-related PR may contain keywords like throughput, performance, etc. This one has ""BPS"" which is throughput. So likely performance. The primary purpose: implement a rate controller to target a given BPS. That is about controlling throughput, maybe to improve performance or to meet performance constraints. It's likely a performance-related change. So classify as 'performance'.assistantfinalperformance"
2406858651,13168,read the CHUNK_TRANSACTIONS_TOTAL metrics in tx injection benchmark,ssavenko-near,194248234,closed,2025-03-20T12:42:11Z,2025-03-20T14:02:59Z,2025-03-20T14:02:58Z,https://api.github.com/repos/near/nearcore,https://github.com/near/nearcore/pull/13168,This will provide somewhat more relevant metric for the single node benchmarks,Human,"analysisWe need to classify the pull request as performance or non-performance. The title and body: ""read the CHUNK_TRANSACTIONS_TOTAL metrics in tx injection benchmark This will provide somewhat more relevant metric for the single node benchmarks"". The content mentions metrics, benchmarks, performance metrics. It seems to be about reading metrics to provide more relevant metric for benchmarks. That is performance-related. So classify as 'performance'.assistantfinalperformance"
2485594584,13431,ci: use warp windows builder for publishable package check,nagisa,679122,closed,2025-04-28T11:39:39Z,2025-04-29T10:19:16Z,,https://api.github.com/repos/near/nearcore,https://github.com/near/nearcore/pull/13431,"This job takes the longest time to run by far (15 minutes) with the next slowest job (publishable check on linux & large pytest checks) running in 8.5~9 minutes.

This is going to result in having to pay for this job, but 15 minutes is way outside of our self-imposed end-to-end CI limit of 10 minutes.",Human,"analysisWe need to classify whether the primary purpose of the PR is to improve performance. The PR title: ""ci: use warp windows builder for publishable package check"". Body: ""This job takes the longest time to run by far (15 minutes) with the next slowest job (publishable check on linux & large pytest checks) running in 8.5~9 minutes. This is going to result in having to pay for this job, but 15 minutes is way outside of our self-imposed end-to-end CI limit of 10 minutes.""

So the PR is about CI job, using warp windows builder for publishable package check. It's about reducing CI time, which is performance improvement. The body mentions job takes 15 minutes, wants to reduce to under 10 minutes. So it's performance improvement. So classification: performance.assistantfinalperformance"
2517584822,13536,randomize order of chunk part forwarding,saketh-are,3241341,open,2025-05-13T20:26:54Z,,,https://api.github.com/repos/near/nearcore,https://github.com/near/nearcore/pull/13536,"Improves fairness and performance; refer to comments in the code change. Possibly worth cherry-picking to the next release?

Also, see [this comment](https://github.com/near/nearcore/pull/13460/files#r2073135962).",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""randomize order of chunk part forwarding"". Body: ""Improves fairness and performance; refer to comments in the code change. Possibly worth cherry-picking to the next release? Also, see [this comment]"". The description explicitly says ""Improves fairness and performance"". So it's performance. So label 'performance'.assistantfinalperformance"
2482362021,13425,wip: bench new analysis,Longarithm,8607261,closed,2025-04-25T19:59:48Z,2025-05-05T05:41:23Z,,https://api.github.com/repos/near/nearcore,https://github.com/near/nearcore/pull/13425,,Human,"analysisWe need to classify the pull request as performance or non-performance. The title and body: ""wip: bench new analysis"". That suggests a work in progress, bench new analysis. Likely a benchmark. The content is minimal. The title includes ""bench"" which is a keyword for performance. So likely performance. There's no other context. So classify as 'performance'.assistantfinalperformance"
2445328278,13324,feat: `SimpleNightshadeV6`: shard 0 split,Wiezzel,9394821,closed,2025-04-08T09:13:15Z,2025-04-11T08:30:54Z,2025-04-11T08:30:54Z,https://api.github.com/repos/near/nearcore,https://github.com/near/nearcore/pull/13324,"Shard `0` is currently the biggest one.
```
    total_size: 18.8 GB,
    total_count: 78866834,
    average_size: 238 B,
    middle_account: StateStatsAccount {
        account_id: ""6318411008.tg"",
        size: 143 B,
    },
    split_size: ""9.4 GB : 143 B : 9.4 GB"",
    split_percent: ""49:0:49"",
```

The split account providing most even split would be `6318411008.tg`, but `600` was chosen instead as it is easier to memorize.",Human,"analysisWe need to classify whether the primary purpose is performance. The PR title: ""feat: `SimpleNightshadeV6`: shard 0 split"". Body describes splitting shard 0 to balance size. This is about sharding, likely to improve performance by balancing load. The description mentions splitting to even out shard size. That is a performance improvement. Keywords: ""split"", ""shard"", ""biggest"", ""size"". Not explicit performance keywords but it's about balancing shards to improve performance. The instruction says performance-related PR may contain keywords such as respons, react, wait, responsive, throughput, slow, performance, optimization, latency, compile-time-hog, fast, memory, stuck, leak, hang, perf, minor, speed, perform, lag, latenc, tim, instant, or any other terms referring to how faster and efficiently software runs. This PR doesn't contain those keywords. But the content indicates it's about splitting shards to improve performance. The question: ""determine whether the primary purpose of the pull request is to improve performance."" The PR is about splitting shard 0 to balance load, which is a performance improvement. So classification: 'performance'.assistantfinalperformance"
2326688624,12902,refactor: use Cow for Account contract fn,pugachAG,3171838,closed,2025-02-10T17:18:12Z,2025-02-10T21:03:06Z,2025-02-10T21:03:06Z,https://api.github.com/repos/near/nearcore,https://github.com/near/nearcore/pull/12902,Currently we return owned instance of `AccountContract` which forces cloning of `AccountId` for `GlobalByAccount` variant. We cannot return ref since for `AccountV1` we need to create instance of `AccountContract` to wrap `code_hash`. Using `Cow` solves the issue.,Human,"analysisWe need to classify whether the primary purpose is performance. The PR title: ""refactor: use Cow for Account contract fn"". Body: explanation about returning owned instance causing cloning, using Cow solves issue. This is about reducing cloning overhead, memory usage, performance. It's a refactor to improve performance. So classification: 'performance'.assistantfinalperformance"
2402300777,4042,fix: recommended wallet listing when opening modal with namespace filter,enesozturk,19428358,closed,2025-03-18T23:42:17Z,2025-03-19T14:43:37Z,2025-03-19T14:43:36Z,https://api.github.com/repos/reown-com/appkit,https://github.com/reown-com/appkit/pull/4042,"# Description

- Fixes recommended wallets listing logics when opening the modal with the namespace filter: We've been showing EVM wallets when opening the Bitcoin modal. This was due to logic issues on how we are re-fetching the wallets when setting the namespace filter.
- Optimizes fetching wallets requests when not changing the namespace filter: We've been fetching wallets every time when we switch between namespace buttons on open / close modal, which is not good. Optimises calls to fetch the wallets only when it's necessary


## Type of change

- [ ] Chore (non-breaking change that addresses non-functional tasks, maintenance, or code quality improvements)
- [x] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)

# Associated Issues

For Linear issues: Closes APKT-xxx
For GH issues: closes #...

# Showcase (Optional)

If there is a UI change include the screenshots with before and after state.
If new feature is being introduced, include the link to demo recording.

# Checklist

- [x] Code in this PR is covered by automated tests (Unit tests, E2E tests)
- [x] My changes generate no new warnings
- [x] I have reviewed my own code
- [x] I have filled out all required sections
- [x] I have tested my changes on the preview link
- [ ] Approver of this PR confirms that the changes are tested on the preview link
",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""fix: recommended wallet listing when opening modal with namespace filter"". Body: describes bug fix, logic issues, optimizes fetching wallets requests when not changing namespace filter. It says ""Optimizes fetching wallets requests when not changing the namespace filter: We've been fetching wallets every time when we switch between namespace buttons on open / close modal, which is not good. Optimises calls to fetch the wallets only when it's necessary"". This is about reducing unnecessary network calls, which is performance improvement. But is it primarily performance? It's a bug fix that also optimizes. The classification: performance. The guidelines: performance-related PRs may contain keywords like optimization, performance, etc. This PR contains ""Optimizes fetching wallets requests"". So it's performance. So label 'performance'.assistantfinalperformance"
2624049871,475,Deduplicated calls to recalculate plaintext description,daniellockyer,964245,closed,2025-06-27T11:13:27Z,2025-06-27T11:46:37Z,2025-06-27T11:46:37Z,https://api.github.com/repos/antiwork/gumroad,https://github.com/antiwork/gumroad/pull/475,"- we don't need to do this twice, which means we can avoid heavy calls to sanitize the plaintext description

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->

## Summary by CodeRabbit

* **Refactor**
  * Improved the handling of meta tag descriptions for product pages, ensuring clearer and more maintainable code without changing existing behavior.

<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Deduplicated calls to recalculate plaintext description"". Body: mentions avoiding heavy calls to sanitize plaintext description. That is performance improvement: reducing heavy calls. So classify as 'performance'.assistantfinalperformance"
2597669019,353,Removed scheduling gap between user balance stats workers,daniellockyer,964245,closed,2025-06-17T06:15:27Z,2025-06-17T06:55:45Z,2025-06-17T06:55:45Z,https://api.github.com/repos/antiwork/gumroad,https://github.com/antiwork/gumroad/pull/353,"- UpdateUserBalanceStatsCacheWorker has a lock on it, which prevent multiple jobs from being scheduled at the same time
- there is a bug where we don't update the user balance status for up to 6 hours because the cron'd update jobs prevent us from re-scheduling newer cache update
- we can workaround that by just adding all jobs to the update queue and allowing Sidekiq to process them as fast as it wants

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->

## Summary by CodeRabbit

- **Refactor**
  - Improved the scheduling of balance stats update jobs for large sellers by enqueuing all jobs immediately, resulting in faster processing.

<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,"analysisWe need to classify as performance or non-performance. The PR removes scheduling gap, enqueues all jobs immediately, allowing Sidekiq to process them as fast as it wants. This is about improving performance (faster processing). Keywords: faster, performance. So classify as 'performance'.assistantfinalperformance"
2623769975,471,Preloaded thumbnail variants to avoid n+1 SQL queries,daniellockyer,964245,closed,2025-06-27T09:29:53Z,2025-06-27T10:01:51Z,2025-06-27T10:01:51Z,https://api.github.com/repos/antiwork/gumroad,https://github.com/antiwork/gumroad/pull/471,"- we can avoid a bunch of DB queries by preloading them in advance

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->

## Summary by CodeRabbit

* **Bug Fixes**
  * Improved loading performance for product thumbnails and their image variants, ensuring faster and more reliable display of images.

<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Preloaded thumbnail variants to avoid n+1 SQL queries"". Body mentions improved loading performance for product thumbnails, ensuring faster and more reliable display. This is performance improvement. So label 'performance'.assistantfinalperformance"
2604162624,361,Improved Sidekiq scheduling efficiency for `LargeSellersUpdateUserBal‚Ä¶,daniellockyer,964245,closed,2025-06-19T09:17:02Z,2025-06-20T14:49:46Z,2025-06-20T14:49:46Z,https://api.github.com/repos/antiwork/gumroad,https://github.com/antiwork/gumroad/pull/361,"‚Ä¶anceStatsCacheWorker`

- instead of scheduling each sub-job individually, we can use `push_bulk` to do it much more efficiently

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->

## Summary by CodeRabbit

- **Refactor**
  - Improved job processing efficiency by switching to bulk enqueuing for user balance stats updates. End users may experience faster and more reliable updates.

<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Improved Sidekiq scheduling efficiency for `LargeSellersUpdateUserBal‚Ä¶`"". Body: ""instead of scheduling each sub-job individually, we can use `push_bulk` to do it much more efficiently"". This is about improving scheduling efficiency, using bulk enqueuing. That is performance improvement. So label 'performance'.assistantfinalperformance"
2583712142,315,Reduced caching threshold for user balance stats service,daniellockyer,964245,closed,2025-06-11T10:00:52Z,2025-06-11T11:58:39Z,2025-06-11T11:58:39Z,https://api.github.com/repos/antiwork/gumroad,https://github.com/antiwork/gumroad/pull/315,"refs https://github.com/antiwork/gumroad/issues/234

- sites even as small as 50k can still have slow dashboards, so we should enable caching for them too

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->

## Summary by CodeRabbit

- **Chores**
  - Updated the default sales threshold for user caching from 100,000 to 50,000.

<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Reduced caching threshold for user balance stats service"". Body: mentions that sites even as small as 50k can still have slow dashboards, so we should enable caching for them too. It's about caching threshold to improve performance. Keywords: slow, caching, performance. So it's performance. So label 'performance'.assistantfinalperformance"
2614077759,416,chore: remove unused files identified by knip.dev,eRaz00r,4405235,closed,2025-06-24T08:23:01Z,2025-06-24T08:36:33Z,,https://api.github.com/repos/antiwork/gumroad,https://github.com/antiwork/gumroad/pull/416,"Remove 12 unused utility functions and legacy files that have no imports or dependencies in the codebase. Each file was carefully analyzed to ensure safe removal without breaking functionality.

Files removed with detailed reasoning:

1. app/assets/javascripts/application.js
   - Legacy Rails asset pipeline manifest using deprecated //= require syntax
   - Project uses modern Shakapacker (webpack) instead of asset pipeline
   - Only referenced in config comments, not actively used
   - Contains obsolete jQuery dependencies handled differently in modern setup

2. app/javascript/utils/apply.ts
   - Functional programming utility for partial function application
   - Complex TypeScript generics that would be obvious if used elsewhere
   - No imports found for 'partiallyApplyAllButFirst' function
   - Modern JavaScript has better alternatives for function composition

3. app/javascript/utils/cookies.ts
   - Custom cookie read/write utility with JSON serialization
   - No imports found for 'readCookie' or 'writeCookie' functions
   - Modern apps typically use specialized cookie libraries or browser APIs
   - Custom serialization approach not used anywhere in codebase

4. app/javascript/utils/delay.ts
   - Simple debounce utility function
   - No imports found in codebase search
   - Standalone function with no side effects
   - Modern React apps typically use useCallback/useMemo for debouncing

5. app/javascript/utils/formSerializerUtils.ts
   - Complex query parameter serialization with custom encoding
   - Advanced TypeScript types but no imports found
   - Modern alternatives exist in URLSearchParams or form libraries
   - Custom encoding logic appears to be superseded

6. app/javascript/utils/isNotNull.ts
   - Simple TypeScript type guard for null checking
   - No imports found for this utility
   - Modern TypeScript has improved null checking built-in
   - One-liner function easily recreated if needed

7. app/javascript/utils/local_storage_utils.ts
   - Browser localStorage feature detection utility
   - No imports found for 'isLocalStorageAccessible' function
   - Defensive programming utility apparently not needed in current codebase
   - Modern browsers have consistent localStorage support

8. app/javascript/utils/reorderCollection.ts
   - Sophisticated array sorting utility with multiple strategies
   - Well-documented but no imports found in codebase
   - Complex logic that would be central to app if used
   - Appears to be planned for use but never implemented

9. app/javascript/utils/set.ts
   - Simple Set manipulation utility with 'toggleItem' function
   - No imports or references found in codebase
   - One-liner function that's easily recreated if needed
   - Standard Set operations typically used inline

10. app/javascript/utils/stateStore.ts
    - Custom Redux-like state management implementation
    - Complex store with action callbacks but no imports found
    - Would be central to app architecture if actually used
    - App likely uses React hooks or other state solutions instead

11. app/javascript/utils/whitespace.ts
    - Simple string manipulation function for removing whitespace
    - No imports found for 'removeWhitespaces' function
    - One-liner utility easily recreated if needed
    - Standard string methods typically used inline

12. app/javascript/utils/with_social_connect.js
    - Legacy Facebook/Twitter OAuth integration utility
    - jQuery-based social login using older patterns
    - No imports found, appears replaced by modern OAuth flows
    - Uses deprecated $vendor/jquery.oauthpopup dependency

Verification process:
- Performed comprehensive grep searches for imports and function names
- Checked for dynamic imports and string-based references
- Verified no runtime dependencies or side effects
- Confirmed files are not entry points in webpack configuration

Impact:
- Reduces dead code by 44% (27 ‚Üí 15 unused files)
- Removes 275 lines of unused code
- Improves build performance with fewer files to process
- Simplifies codebase maintenance
- Potential bundle size reduction

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->

## Summary by CodeRabbit

- **Chores**
  - Removed several legacy utility scripts and JavaScript libraries, including cookie handling, local storage checks, form serialization, state management, debounce, set operations, whitespace removal, and social media connection utilities.
  - Discontinued automatic inclusion of older jQuery, jQuery UI, Facebook SDK, and related plugins.
  - Some features dependent on these utilities or libraries may no longer be available.

<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,"analysisWe need to classify as performance or non-performance. The PR is about removing unused files, reducing dead code, improving build performance. The description says ""Improves build performance with fewer files to process"". That is a performance improvement. The keywords: performance, build performance. So classify as 'performance'.assistantfinalperformance"
2608906245,397,Cached repetitive data lookups for creator analytics,daniellockyer,964245,closed,2025-06-21T09:21:17Z,2025-06-21T09:38:34Z,,https://api.github.com/repos/antiwork/gumroad,https://github.com/antiwork/gumroad/pull/397,"- because of how the code is structured, we create a separate CreatorAnalytics::Web instance for every missing date range
- this then calls the `products_for_creator_analytics` method on a user, which returns a different relation each time, so query caching doesn't work
- instead, we can just calculate this once in the caching proxy and then pass it to the web instance
- I'll refactor this properly in the future once the fix is confirmed good

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->

## Summary by CodeRabbit

- **Performance Improvements**
  - Enhanced analytics performance by caching user‚Äôs first sale date and product data, resulting in faster loading times for analytics features.

<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Cached repetitive data lookups for creator analytics"". Body describes caching repetitive data lookups, caching user‚Äôs first sale date and product data, resulting in faster loading times. This is clearly performance improvement. So label 'performance'.assistantfinalperformance"
2572635660,300,Fixed external PayPal API request when loading dashboard,daniellockyer,964245,closed,2025-06-06T06:50:23Z,2025-06-06T07:20:25Z,2025-06-06T07:20:24Z,https://api.github.com/repos/antiwork/gumroad,https://github.com/antiwork/gumroad/pull/300,"fix https://github.com/antiwork/gumroad/issues/274

- when loading the dashboard, if the user has PayPal payouts configured, we'd make a call to the PayPal API
- however, the code never used the value returned from the API because a different codepath gets triggered first
- as a result, we can re-arrange the code to skip that API request and save 500ms from dashboard load time

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->

## Summary by CodeRabbit

- **Refactor**
  - Improved internal logic for displaying payout method details to ensure more accurate and relevant information is shown to users. No changes to visible features or user experience.

<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Fixed external PayPal API request when loading dashboard"". Body: mentions skipping API request to save 500ms from dashboard load time. That is a performance improvement. So label 'performance'.assistantfinalperformance"
2600560710,356,Removed unnecessary API requests to fetch balances,daniellockyer,964245,closed,2025-06-18T05:30:41Z,2025-06-19T07:49:16Z,2025-06-19T07:49:16Z,https://api.github.com/repos/antiwork/gumroad,https://github.com/antiwork/gumroad/pull/356,"refs https://github.com/antiwork/gumroad-old/pull/28712/files

- since the PR above, we've been including the balance/stats directly into the page instead of relying on loading them via secondary API requests
- as a result, we're currently loading 4 different pages and not even using the result
- some of these requests are even just returning HTML because they don't have JSON equivalents
- we can do away with that and simplify the code in the process

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->
## Summary by CodeRabbit

- **Refactor**
  - Simplified the statistics display by removing asynchronous fetching and progress indicators. The statistic value is now displayed directly from the provided value.
  - Removed clickable links and URL-based navigation from the statistics components on the dashboard.
  - Updated font sizing and tooltip support for improved readability and usability.
<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,"analysisWe need to classify as performance or non-performance. The PR removes unnecessary API requests to fetch balances, simplifying code. This reduces network requests, potentially improves performance. The description: ""Removed unnecessary API requests to fetch balances"" and ""we're currently loading 4 different pages and not even using the result"" and ""some of these requests are even just returning HTML because they don't have JSON equivalents"". So it's about removing redundant API calls, which is a performance improvement. The keywords: ""performance"" not explicitly but it's about reducing API calls. The instruction says performance-related PR may contain keywords such as respons, react, wait, responsive, throughput, slow, performance, optimization, latency, compile-time-hog, fast, memory, stuck, leak, hang, perf, minor, speed, perform, lag, latenc, tim, instant, or any other terms referring to how faster and efficiently software runs. This PR doesn't contain those keywords, but the content indicates performance improvement. The instruction says ""Performance-related pull requests may contain keywords such as... or any other terms referring to how faster and efficiently software runs."" It doesn't say that if it doesn't contain those keywords it's non-performance. But the content is about removing unnecessary API requests, which is a performance improvement. So classify as 'performance'.

Thus answer: performance.assistantfinalperformance"
2577421996,307,Fixed duplicate context lookups across app,daniellockyer,964245,closed,2025-06-09T05:40:16Z,2025-06-09T06:14:39Z,2025-06-09T06:14:39Z,https://api.github.com/repos/antiwork/gumroad,https://github.com/antiwork/gumroad/pull/307,"refs https://github.com/antiwork/gumroad/issues/234

- because we were calling the custom_context function twice, we were doing all the DB queries twice
- this saves ~4 DB queries per page load when logged in, resulting in a ~2% win

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->

## Summary by CodeRabbit

- **Refactor**
  - Improved performance and maintainability by optimizing how custom context data is accessed when embedding design settings and user agent information in the layout. No visible changes to users.

<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Fixed duplicate context lookups across app"". Body: mentions calling custom_context twice, doing DB queries twice, saves ~4 DB queries per page load, ~2% win. Also summary says ""Refactor - Improved performance and maintainability by optimizing how custom context data is accessed"". So it's performance. So label 'performance'.assistantfinalperformance"
2560305820,289,Added Typhoeus client for HTTP connection pooling + re-use,daniellockyer,964245,closed,2025-06-02T11:32:52Z,2025-06-02T12:32:02Z,2025-06-02T12:32:02Z,https://api.github.com/repos/antiwork/gumroad,https://github.com/antiwork/gumroad/pull/289,"refs https://github.com/elastic/elasticsearch-ruby#usage refs https://github.com/antiwork/gumroad/issues/234

- as suggested by the Elasticsearch Ruby docs, we should be using a HTTP library that supports persistent HTTP connections
- by adding the `typhoeus` gem, it'll automatically be used by Faraday
- this should improve performance by reusing HTTP connections instead of invoking the overhead of setting up and closing

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->

## Summary by CodeRabbit

- **Chores**
  - Added a new dependency to improve application capabilities. No user-facing changes.

<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,analysisWe need to classify as performance or non-performance. The PR adds Typhoeus client for HTTP connection pooling + re-use. It mentions improving performance by reusing HTTP connections. So it's performance.assistantfinalperformance
2441809617,56,Index `followers` to better support our query patterns,xrav3nz,6376558,closed,2025-04-06T19:16:30Z,2025-04-06T19:36:36Z,2025-04-06T19:36:36Z,https://api.github.com/repos/antiwork/gumroad,https://github.com/antiwork/gumroad/pull/56,"We frequently query followers filtered by followed_id and ordered by
confirmed_at. (e.g. on the /followers page)

This could take 80+ seconds for sellers with a large amount of
following.

I am hoping that this composite index on (followed_id, confirmed_at) can
help more efficiently perform range scans without a filesort, improving
query performances.

I left these out of the composite index:

- `deleted_at`: vast majority of the records should have `deleted_at is
  NULL`
- `id`: I think this is only going to help if we switch to cursor-based
  pagination (unlikely to be prioritized)

I had tested this with a local table of 1M records. Will further
bench this once it's rolled out and make adjustments if needed.",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Index `followers` to better support our query patterns"". Body describes adding composite index to improve query performance. It's clearly performance improvement. So label 'performance'.assistantfinalperformance"
2441961215,74,Enable Playwright parallelism,MayaRainer,19721695,closed,2025-04-07T00:21:49Z,2025-04-10T21:38:57Z,2025-04-10T21:38:57Z,https://api.github.com/repos/antiwork/flexile,https://github.com/antiwork/flexile/pull/74,Closes #45.,Human,"analysisWe need to classify whether the primary purpose is to improve performance. The PR title: ""Enable Playwright parallelism"". That likely improves test execution speed. It's about enabling parallelism in Playwright tests, which is a performance improvement. So classification: performance.assistantfinalperformance"
2510652202,290,Don't generate S3 download URLs ahead of time,MayaRainer,19721695,closed,2025-05-09T22:19:33Z,2025-05-09T23:55:49Z,2025-05-09T23:55:49Z,https://api.github.com/repos/antiwork/flexile,https://github.com/antiwork/flexile/pull/290,"Currently, the `/documents` page is slow in part because we pre-generate S3 download URLs for every single row. This PR removes this generation and instead adds an endpoint to explicitly redirect to the download.

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->

## Summary by CodeRabbit

- **New Features**
  - Added a secure download route for file attachments, allowing users to download files via a new URL format.

- **Improvements**
  - Updated all download links for attachments and documents to use the new secure download route, enhancing consistency and security.
  - Improved the display and conditional rendering of download buttons across billing, cap table uploads, documents, invoices, and tender offers.
  - Adjusted form integration and display logic on the tender offers page for better usability.

- **Bug Fixes**
  - Ensured download links are only shown when attachments exist.

- **Tests**
  - Extended end-to-end tests to verify the presence and correctness of download links for document attachments.

- **Chores**
  - Updated TypeScript configuration to target ECMAScript 2024 for improved language support.

<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Don't generate S3 download URLs ahead of time"". The body: mentions that /documents page is slow because pre-generating S3 URLs for every row. This PR removes generation and adds endpoint to redirect. So it's about performance improvement. Keywords: slow, performance, optimization. So classification: performance.assistantfinalperformance"
2430102297,47,Mock Docuseal in tests,MayaRainer,19721695,closed,2025-03-31T23:24:49Z,2025-04-03T14:59:33Z,2025-04-03T14:59:33Z,https://api.github.com/repos/antiwork/flexile,https://github.com/antiwork/flexile/pull/47,"A part of #37.

Ensures a DocuSeal token is no longer required to run tests locally or on CI. One less thing for OSS contributors to set up, also speeds up tests and ensures we don't create dummy data in DocuSeal.",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Mock Docuseal in tests"". Body: ""Ensures a DocuSeal token is no longer required to run tests locally or on CI. One less thing for OSS contributors to set up, also speeds up tests and ensures we don't create dummy data in DocuSeal.""

Keywords: ""speeds up tests"" indicates performance improvement. Also ""mock"" to avoid external calls. So it's performance. So label 'performance'.assistantfinalperformance"
2618583894,413,Optimize invoices page to load only invoices needing approval by default,devin-ai-integration[bot],158243242,open,2025-06-25T15:21:36Z,,,https://api.github.com/repos/antiwork/flexile,https://github.com/antiwork/flexile/pull/413,"# Optimize invoices page to load only invoices needing approval by default

## Summary
This PR optimizes the invoices page performance by implementing default status filtering, similar to how the documents page loads only ""awaiting signature"" documents by default. The invoices page now loads only invoices with ""received"" and ""approved"" status by default, significantly reducing the initial dataset and improving page load times.

## Changes Made
- **Added localStorage-based status filtering** using the same pattern as the documents page
- **Default filter** shows only `received` and `approved` invoices (both map to ""Awaiting approval"" in the UI)
- **Toggle functionality** allows users to switch between ""Show awaiting approval only"" and ""Show all invoices""
- **Type-safe implementation** using proper TypeScript types and zod schema validation
- **Preserved existing functionality** including admin/contractor access controls

## Technical Details
- Modified `frontend/app/invoices/page.tsx` to add status filtering logic
- Added `invoiceStatusFilterSchema` using zod with proper enum validation
- Implemented localStorage persistence for user filter preferences
- Added toggle button in the DataTable actions for administrators
- Used existing `trpc.invoices.list` query with status parameter (no backend changes needed)

## Performance Impact
- **Reduced initial load time** by limiting the default dataset to only invoices needing approval
- **Maintained user flexibility** by allowing access to all invoices via toggle
- **Improved user experience** by showing the most relevant invoices first (similar to documents page)

## Testing Transparency

### What I Actually Checked
‚úÖ **Code linting passed** - All TypeScript and ESLint checks passed successfully  
‚úÖ **Code patterns verified** - Implementation follows the exact same pattern used in the documents page  
‚úÖ **Type safety confirmed** - Proper TypeScript types and zod schema validation implemented  
‚úÖ **Import validation** - All required imports (`invoiceStatuses` from `@/db/enums`) are correctly added  
‚úÖ **Query structure verified** - The existing `trpc.invoices.list` query already supports status filtering  
‚úÖ **UI integration confirmed** - Toggle button properly integrated into existing DataTable actions  

### What I Did Not Check
‚ùå **Browser functionality** - Could not test the actual page behavior due to database migration issues in local dev environment  
‚ùå **Filter toggle behavior** - Could not verify the toggle button works correctly in the browser  
‚ùå **localStorage persistence** - Could not test that filter preferences are properly saved and restored  
‚ùå **Performance improvement** - Could not measure actual load time improvements  

### Environment Issues Encountered
- Local development server (`bin/dev`) failed due to database migration error: `PG::UndefinedTable: ERROR: table ""board_consents"" does not exist`
- This prevented browser testing but does not affect the frontend code changes

## Review Checklist for Human Reviewer
Please verify the following when testing:

üîç **Critical functionality to test:**
- [ ] Navigate to `/invoices` and verify only ""received"" and ""approved"" invoices are shown by default
- [ ] Click the toggle button and verify it switches between filtered and all invoices views
- [ ] Refresh the page and verify the filter preference is remembered via localStorage
- [ ] Test both admin and contractor user roles to ensure access controls still work
- [ ] Verify existing invoice actions (approve, reject, etc.) still function correctly

üîç **Performance to verify:**
- [ ] Page loads faster with fewer invoices initially displayed
- [ ] No regressions in existing invoice functionality

## Link to Devin run
https://app.devin.ai/sessions/c2ab3f9b34d3433aa5b91d0809bcebc6

**Requested by:** sahil.lavingia@gmail.com

## Notes
This change addresses the performance issues mentioned in the Slack thread where the invoices page was ""super slow to resolve"" by reducing the initial dataset size, similar to the successful approach used on the documents page.
",Human,"analysisWe need to classify as 'performance' or 'non-performance'. The PR title: ""Optimize invoices page to load only invoices needing approval by default"". Body mentions performance improvements, reducing load time, etc. Contains keywords: optimize, performance, load, reduce, etc. So it's performance.assistantfinalperformance"
2479345712,1497,Use `_async_monitor_job_status` in `AsyncFirecrawlApp`,jmbledsoe,776738,closed,2025-04-24T14:37:10Z,2025-04-24T15:51:47Z,,https://api.github.com/repos/mendableai/firecrawl,https://github.com/mendableai/firecrawl/pull/1497,"Closes https://github.com/mendableai/firecrawl/issues/1496

This PR updates `scrape_url` and `batch_scrape_urls` in `AsyncFirecrawlApp` to use `_async_monitor_job_status` for asynchronous monitoring of job statuses.",Human,"analysisWe need to classify whether the primary purpose is performance improvement. The PR title: ""Use `_async_monitor_job_status` in `AsyncFirecrawlApp`"". Body: ""This PR updates `scrape_url` and `batch_scrape_urls` in `AsyncFirecrawlApp` to use `_async_monitor_job_status` for asynchronous monitoring of job statuses."" It seems to change the monitoring to asynchronous, likely improving performance or responsiveness. The keywords: ""async"" indicates asynchronous monitoring, which may improve performance. The description: ""for asynchronous monitoring of job statuses."" That likely improves performance by not blocking. So classification: 'performance'.assistantfinalperformance"
2587604734,1662,feat(index): remove unused columns,mogery,66118807,closed,2025-06-12T14:36:19Z,2025-06-12T14:51:40Z,2025-06-12T14:51:40Z,https://api.github.com/repos/mendableai/firecrawl,https://github.com/mendableai/firecrawl/pull/1662,,Human,"analysisWe need to classify whether the primary purpose is to improve performance. The PR title: ""feat(index): remove unused columns"". Body not provided. Removing unused columns could be for performance (e.g., reduce memory usage, speed up queries). But could also be for code cleanliness. The instruction: performance-related PRs may contain keywords like performance, optimization, etc. This title doesn't contain those. But the action ""remove unused columns"" could be performance improvement. But the instruction says to classify based on primary purpose. Without body, we guess. The title suggests removing unused columns, likely to reduce memory usage or improve performance. But could also be cleaning up. The guidelines: performance-related PRs may contain keywords such as... or any other terms referring to how faster and efficiently software runs. The title doesn't contain those. But the action itself is a performance improvement. The question: do we classify as performance? The guidelines: ""Given a text combining the title and body of a pull request, determine whether the primary purpose of the pull request is to improve performance."" So we need to decide if the primary purpose is performance. Removing unused columns is likely to improve performance. So classify as 'performance'. There's no contradictory info. So answer: performance.assistantfinalperformance"
2479526440,1498,Use `_async_monitor_job_status` in `AsyncFirecrawlApp`,jmbledsoe,776738,closed,2025-04-24T15:51:08Z,2025-04-24T16:28:30Z,2025-04-24T16:28:30Z,https://api.github.com/repos/mendableai/firecrawl,https://github.com/mendableai/firecrawl/pull/1498,"Closes https://github.com/mendableai/firecrawl/issues/1496

This PR updates `scrape_url` and `batch_scrape_urls` in `AsyncFirecrawlApp` to use `_async_monitor_job_status` for asynchronous monitoring of job statuses.",Human,"analysisWe need to classify whether the primary purpose is performance improvement. The PR title: ""Use `_async_monitor_job_status` in `AsyncFirecrawlApp`"". Body: ""This PR updates `scrape_url` and `batch_scrape_urls` in `AsyncFirecrawlApp` to use `_async_monitor_job_status` for asynchronous monitoring of job statuses."" It seems to change the monitoring to asynchronous, likely improving performance or responsiveness. The keywords: ""async"" indicates asynchronous monitoring, which may improve performance. The description: ""for asynchronous monitoring of job statuses."" That likely improves performance by not blocking. So classification: 'performance'.assistantfinalperformance"
2297969098,309,feat(pack): Performance Optimization for Large Repositories,yamadashy,5019072,closed,2025-01-25T04:56:39Z,2025-01-25T05:32:13Z,2025-01-25T05:32:13Z,https://api.github.com/repos/yamadashy/repomix,https://github.com/yamadashy/repomix/pull/309,"## Performance Improvement

### yamadashy/repomix
- Before: `868.73 millis (usr: 1.11 secs, sys: 0.14 secs)`
- After: `671.26 millis (usr: 1.42 secs, sys: 0.22 secs)`

No significant change

### facebook/react

- Before: `123.31 secs (usr: 118.64 secs, sys: 1.60 secs)`
- After: `4.19 secs (usr: 22.66 secs, sys: 2.49 secs)`

29x faster

### vercel/next.js
- Before: `17.85 mins (usr: 16.66 mins, sys: 0.18 mins)`
- After: `17.27 secs (usr: 52.93 secs, sys: 7.11 secs)`

58x faster

## Changes
- Replace `p-map` with Piscina worker threads for parallel processing
- Implement dedicated workers for file collection, processing, and metrics
- Optimize file search with single worker implementation
- Add proper resource cleanup for worker pools
- Move file manipulation logic to worker threads

## Checklist

- [x] Run `npm run test`
- [x] Run `npm run lint`
",Human,"analysisWe need to classify as 'performance' or 'non-performance'. The PR title: ""feat(pack): Performance Optimization for Large Repositories"". Body includes performance metrics, mentions speed improvements, etc. So it's definitely performance. So output 'performance'.assistantfinalperformance"
2298027795,313,feat(website): Improve performance of large repository output using Ace Editor,yamadashy,5019072,closed,2025-01-25T08:04:24Z,2025-01-25T13:55:54Z,2025-01-25T13:55:54Z,https://api.github.com/repos/yamadashy/repomix,https://github.com/yamadashy/repomix/pull/313,"<!-- Please include a summary of the changes -->

related: #312 

## Checklist

- [x] Run `npm run test`
- [x] Run `npm run lint`
",Human,"analysisWe need to classify as performance or non-performance. The title: ""feat(website): Improve performance of large repository output using Ace Editor"". That clearly indicates performance improvement. So label 'performance'.assistantfinalperformance"
2531991252,4109,fix(boxai-sidebar): refactor sidebar content to reduce re-render ,fpan225,97700841,closed,2025-05-20T15:56:03Z,2025-05-21T22:27:04Z,,https://api.github.com/repos/box/box-ui-elements,https://github.com/box/box-ui-elements/pull/4109,"Use` React.useCallback `to reduce component  re-rendering
<!--
Please add the `ready-to-merge` label when the pull request has received the appropriate approvals.
Using the `ready-to-merge` label adds your approved pull request to the merge queue where it waits to be merged.
Mergify will merge your pull request based on the queue assuming your pull request is still in a green state after the previous merge.

What to do when the `ready-to-merge` label is not working:

- Do you have two approvals?
  - At least two approvals are required in order to merge to the master branch.
- Are there any reviewers that are still requested for review?
  - If the pull request has received the necessary approvals, remove any additional reviewer requests that are pending.
    - e.g.
      - Three reviewers added comments but you already have two necessary approvals and the third reviewer's comments are no longer applicable. You can remove the third person as a reviewer or have them approve the pull request.
      - A team was added as a reviewer because of a change to a file but the file change has been undone. At this point, it should be safe to remove the team as a reviewer.
- Are there other pull requests at the front of the merge queue?
  - Mergify handles the queueing, your pull request will eventually get merged.

When to contact someone for assistance when trying to merge via `ready-to-merge` label:

- There are no other pull requests in the merge queue and your pull request has been sitting there with the `ready-to-merge` label for longer than a couple of hours.
- If you are unable to remove unnecessary reviewers from the pull request.
- If you are unable to add the `ready-to-merge` label.
  -->


<!-- This is an auto-generated comment: release notes by coderabbit.ai -->
## Summary by CodeRabbit

## Summary by CodeRabbit

- **Performance Improvements**
  - Enhanced sidebar responsiveness by optimizing content update checks to reduce unnecessary refreshes.
<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""fix(boxai-sidebar): refactor sidebar content to reduce re-render"". Body mentions using React.useCallback to reduce component re-rendering. Also auto-generated comment says ""Performance Improvements: Enhanced sidebar responsiveness by optimizing content update checks to reduce unnecessary refreshes."" So it's performance. So label 'performance'.assistantfinalperformance"
2333934931,3928,feat(boxai-sidebar): Add cache for Agent Selector,MateuszMamczarz,23643342,closed,2025-02-13T14:25:28Z,2025-02-14T18:14:05Z,2025-02-14T18:14:05Z,https://api.github.com/repos/box/box-ui-elements,https://github.com/box/box-ui-elements/pull/3928,"Add logic to cache agent selector to not load it again after sidebar is switched to another tab.

https://github.com/user-attachments/assets/03907f4a-8afb-4659-9833-ca341201af43



<!--
Please add the `ready-to-merge` label when the pull request has received the appropriate approvals.
Using the `ready-to-merge` label adds your approved pull request to the merge queue where it waits to be merged.
Mergify will merge your pull request based on the queue assuming your pull request is still in a green state after the previous merge.

What to do when the `ready-to-merge` label is not working:

- Do you have two approvals?
  - At least two approvals are required in order to merge to the master branch.
- Are there any reviewers that are still requested for review?
  - If the pull request has received the necessary approvals, remove any additional reviewer requests that are pending.
    - e.g.
      - Three reviewers added comments but you already have two necessary approvals and the third reviewer's comments are no longer applicable. You can remove the third person as a reviewer or have them approve the pull request.
      - A team was added as a reviewer because of a change to a file but the file change has been undone. At this point, it should be safe to remove the team as a reviewer.
- Are there other pull requests at the front of the merge queue?
  - Mergify handles the queueing, your pull request will eventually get merged.

When to contact someone for assistance when trying to merge via `ready-to-merge` label:

- There are no other pull requests in the merge queue and your pull request has been sitting there with the `ready-to-merge` label for longer than a couple of hours.
- If you are unable to remove unnecessary reviewers from the pull request.
- If you are unable to add the `ready-to-merge` label.
  -->
",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""feat(boxai-sidebar): Add cache for Agent Selector"". Body: ""Add logic to cache agent selector to not load it again after sidebar is switched to another tab."" This is about caching to avoid reloading. That is a performance improvement? It reduces load time, improves responsiveness. It's a performance optimization. The keywords: ""cache"", ""not load again"", ""sidebar switched"". It's about performance. So classify as 'performance'.assistantfinalperformance"
2272409158,10359,Start with minimal prefetch distance to minimize prefetch overhead for exact or limited index scans,knizhnik,156922,closed,2025-01-12T07:18:27Z,2025-01-13T14:27:02Z,2025-01-13T14:27:02Z,https://api.github.com/repos/neondatabase/neon,https://github.com/neondatabase/neon/pull/10359,"## Problem

See https://neondb.slack.com/archives/C04DGM6SMTM/p1736526089437179

In case of queries index scan with LIMIT clause, multiple backends can concurrently send larger number of duplicated prefetch requests which are not stored in LFC and so actually do useless job.

Current implementation of index prefetch starts with maximal prefetch distance (10 by default now) when there are no key bounds, so in queries with LIMIT clause like `select * from T order by pk limit 1` compute can send a lot of useless prefetch requests to page server.

## Summary of changes

Always start with minimal prefetch distance even if there are not key boundaries.

Related Postgres PRs:
https://github.com/neondatabase/postgres/pull/552
https://github.com/neondatabase/postgres/pull/551
https://github.com/neondatabase/postgres/pull/550
https://github.com/neondatabase/postgres/pull/549
",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""Start with minimal prefetch distance to minimize prefetch overhead for exact or limited index scans"". The body describes reducing prefetch overhead, avoiding useless prefetch requests, improving performance. Keywords: ""prefetch overhead"", ""useless job"", ""performance"". So it's performance. So label 'performance'.assistantfinalperformance"
2311607019,8412,ML-KEM/Kyber: small memory usage,SparkiDev,17982558,closed,2025-02-03T02:36:12Z,2025-02-04T19:45:02Z,2025-02-04T19:45:02Z,https://api.github.com/repos/wolfSSL/wolfssl,https://github.com/wolfSSL/wolfssl/pull/8412,"# Description

Options to compile ML-KEM/Kyber to use less dynamic memory.
Only available with C code and has small performance trade-off.

# Testing

Regression tested ML-KEM/Kyber.

# Checklist

 - [ ] added tests
 - [ ] updated/added doxygen
 - [ ] updated appropriate READMEs
 - [ ] Updated manual and documentation
",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""ML-KEM/Kyber: small memory usage"". Body: ""Options to compile ML-KEM/Kyber to use less dynamic memory. Only available with C code and has small performance trade-off."" So primary purpose: reduce memory usage. That is a performance improvement? Memory usage is a performance metric, but the primary focus is memory usage, not speed. The description says small performance trade-off. But the main goal is memory usage. The classification: performance or non-performance? The guidelines: performance-related PRs may contain keywords like performance, optimization, etc. This PR mentions ""performance trade-off"" but the main goal is memory usage. Memory usage is a performance metric, but the PR is about memory usage, not speed. The guidelines say ""performance-related pull requests may contain keywords such as..."". The presence of ""performance trade-off"" might be enough. But the primary purpose is memory usage. The question: is memory usage considered performance? Usually memory usage is a performance metric. But the classification is binary: performance or non-performance. The guidelines: ""Performance-related pull requests may contain keywords such as respons, react, wait, responsive, throughput, slow, performance, optimization, latency, compile-time-hog, fast, memory, stuck, leak, hang, perf, minor, speed, perform, lag, latenc, tim, instant, or any other terms referring to how faster and efficiently software runs."" The list includes ""memory"". So memory is a keyword. So this PR is performance. So classify as 'performance'.assistantfinalperformance"
2281495670,1202,feat: add helicone for caching openai response,plxity,28510494,closed,2025-01-16T10:47:54Z,2025-01-16T13:13:46Z,2025-01-16T13:13:46Z,https://api.github.com/repos/ComposioHQ/composio,https://github.com/ComposioHQ/composio/pull/1202,"Added Helicone for caching OpenAI responses while running tests
<!-- ELLIPSIS_HIDDEN -->


----

> [!IMPORTANT]
> Integrate Helicone for caching OpenAI responses in tests, update workflows, and modify JavaScript examples to use Helicone parameters.
> 
>   - **Workflows**:
>     - Add `HELICONE_API_KEY` to environment variables in `examples_js.yml`.
>     - Modify `run_examples_js.yml` to trigger on changes in `js/examples/**`.
>   - **JavaScript Examples**:
>     - Update `demo.mjs` in `lead_outreach_agent`, `portfolio-generator`, and `reddit-research` to use `heliconeParams` from `utils.mjs`.
>     - Change model to `gpt-3.5-turbo` in `lead_outreach_agent/demo.mjs` and `portfolio-generator/demo.mjs`.
>   - **Utilities**:
>     - Add `utils.mjs` to define `heliconeParams` and `openai` with Helicone configuration.
>   - **Dependencies**:
>     - Update `langchain` to `^0.3.11` in `lead_outreach_agent/package.json`.
>     - Update `@ai-sdk/openai` and `ai` versions in `portfolio-generator/package.json` and `reddit-research/package.json`.
> 
> <sup>This description was created by </sup>[<img alt=""Ellipsis"" src=""https://img.shields.io/badge/Ellipsis-blue?color=175173"">](https://www.ellipsis.dev?ref=ComposioHQ%2Fcomposio&utm_source=github&utm_medium=referral)<sup> for ff237ffba9e50542bc34c00c96c03f17873c4081. It will automatically update as commits are pushed.</sup>


<!-- ELLIPSIS_HIDDEN -->",Human,"analysisWe need to classify as performance or non-performance. The PR adds Helicone for caching OpenAI responses while running tests. Caching responses improves performance? It might reduce latency, but the primary purpose is to add caching for tests, not necessarily performance improvement. The description mentions caching OpenAI responses, adding environment variable, updating examples. It's about caching to speed up tests, but the main goal is to integrate Helicone for caching. That is a performance improvement? Caching reduces latency, but the PR is about adding caching for tests, not necessarily performance. The classification: performance if it's about improving performance. Caching is a performance optimization. The keywords: ""caching"" is not in the list but ""performance"" is. The description: ""add helicone for caching openai response"". That is a performance improvement. So classify as performance.assistantfinalperformance"
2317932349,24,Devin/1738769793 python fastapi migration,YWoooooo,193873018,closed,2025-02-05T15:38:36Z,2025-02-05T15:40:10Z,,https://api.github.com/repos/jina-ai/node-DeepResearch,https://github.com/jina-ai/node-DeepResearch/pull/24,,Human,"analysisWe need to classify the pull request as performance or non-performance. The title: ""Devin/1738769793 python fastapi migration"". Body not provided. The title suggests migrating to FastAPI, likely for performance improvements? But migration could be for other reasons. The instruction: performance-related PRs may contain keywords like fast, performance, etc. The title contains ""fastapi"" and ""fast"". ""fastapi"" includes ""fast"". But is that enough? The guidelines: performance-related PRs may contain keywords such as... fast. So ""fastapi"" contains ""fast"". But is that enough? The title also includes ""migration"". Could be a migration to FastAPI for better performance. But the classification is ambiguous. The guidelines say performance-related PRs may contain keywords such as... fast. So presence of ""fast"" might be enough. But we need to decide. The title ""Devin/1738769793 python fastapi migration"" includes ""fastapi"" which includes ""fast"". So likely performance. But could be just a migration for other reasons. But the presence of ""fast"" is a keyword. So classify as 'performance'.assistantfinalperformance"
2319710038,32,minimize the docker image size after building,ArnoChenFx,24894259,closed,2025-02-06T11:43:33Z,2025-02-06T13:39:37Z,2025-02-06T13:39:37Z,https://api.github.com/repos/jina-ai/node-DeepResearch,https://github.com/jina-ai/node-DeepResearch/pull/32,Shrinking the image size from 1G+ to 200M+,Human,"analysisWe need to classify whether the primary purpose is to improve performance. The PR title: ""minimize the docker image size after building"". Body: ""Shrinking the image size from 1G+ to 200M+"". This is about reducing image size, which is a performance improvement? It's about reducing size, which can improve performance (download time, storage). But the classification guidelines: performance-related PRs may contain keywords like performance, optimization, etc. This one is about image size. It's a performance improvement? The guidelines say performance-related PRs may contain keywords such as... or any other terms referring to how faster and efficiently software runs. Reducing image size is a performance improvement. So classify as 'performance'.assistantfinalperformance"
2469761206,360,Feature/fix handlers,tegnike,35606144,closed,2025-04-19T20:35:32Z,2025-04-23T10:36:37Z,2025-04-23T10:36:37Z,https://api.github.com/repos/tegnike/aituber-kit,https://github.com/tegnike/aituber-kit/pull/360,"

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->
## Summary by CodeRabbit

- **Êñ∞Ê©üËÉΩ**
  - „ÉÅ„É£„ÉÉ„Éà„É°„ÉÉ„Çª„Éº„Ç∏„Å´‰∏ÄÊÑè„ÅÆID„Åå‰ªò‰∏é„Åï„Çå„Çã„Çà„ÅÜ„Å´„Å™„Çä„Åæ„Åó„Åü„ÄÇ
  - „É°„ÉÉ„Çª„Éº„Ç∏„ÅÆËøΩÂä†„ÉªÊõ¥Êñ∞„ÇíIDÂçò‰Ωç„ÅßÊüîËªü„Å´Ë°å„Åà„ÇãÊñ∞Ê©üËÉΩ„ÅåËøΩÂä†„Åï„Çå„Åæ„Åó„Åü„ÄÇ
  - „É°„ÉÉ„Çª„Éº„Ç∏IDÁîüÊàê„ÅÆ„É¶„Éº„ÉÜ„Ç£„É™„ÉÜ„Ç£Èñ¢Êï∞„ÇíËøΩÂä†„Åó„Åæ„Åó„Åü„ÄÇ

- **ÊîπÂñÑ**
  - AIÂøúÁ≠î„ÇÑ„Ç≥„Éº„Éâ„Éñ„É≠„ÉÉ„ÇØ„ÅÆÂá¶ÁêÜ„ÅåÊñáÁ´†Âçò‰Ωç„ÅßÁ¥∞„Åã„Åè„É™„Ç¢„É´„Çø„Ç§„É†„Å´ÂèçÊò†„Åï„Çå„Çã„Çà„ÅÜ„Å´„Å™„Çä„Åæ„Åó„Åü„ÄÇ
  - „É°„ÉÉ„Çª„Éº„Ç∏‰øùÂ≠ò„ÅåË§áÊï∞‰ª∂„Åæ„Å®„ÇÅ„Å¶ÂäπÁéáÁöÑ„Å´Ë°å„Çè„Çå„Çã„Çà„ÅÜ„Å´ÊîπÂñÑ„Åï„Çå„Åæ„Åó„Åü„ÄÇ
  - „É°„ÉÉ„Çª„Éº„Ç∏ÁµêÂêà„ÅÆÊù°‰ª∂„ÅåÂé≥ÂØÜÂåñ„Åï„Çå„ÄÅÂÜÖÂÆπÁÆ°ÁêÜ„Åå„Çà„ÇäÊ≠£Á¢∫„Å´„Å™„Çä„Åæ„Åó„Åü„ÄÇ
  - „ÉÅ„É£„ÉÉ„Éà„É≠„Ç∞„ÅÆ‰øùÂ≠òÂá¶ÁêÜ„Åå„Éê„ÉÉ„ÉÅÂåñ„Åï„Çå„ÄÅÁÑ°ÈßÑ„Å™‰øùÂ≠ò„ÇíÂâäÊ∏õ„Åó„Åæ„Åó„Åü„ÄÇ
  - „Çπ„Éà„É™„Éº„Éü„É≥„Ç∞Âá¶ÁêÜ„ÅÆÈü≥Â£∞ÂêàÊàê„ÇÑÁä∂ÊÖãÁÆ°ÁêÜ„ÇíÊîπÂñÑ„ÅóÂÆâÂÆöÊÄß„ÇíÂêë‰∏ä„Åó„Åæ„Åó„Åü„ÄÇ
  - YouTube„Ç≥„É°„É≥„ÉàÂá¶ÁêÜ„ÅÆ„Éï„É≠„Éº„ÇíÁ∞°Á¥†Âåñ„Åó„Åæ„Åó„Åü„ÄÇ

- **„Éê„Ç∞‰øÆÊ≠£**
  - ‰øùÂ≠òAPI„ÅÆ„É≠„Ç∞„Éï„Ç°„Ç§„É´Ë™≠„ÅøËæº„ÅøÊôÇ„ÅÆ„Ç®„É©„ÉºÂá¶ÁêÜ„Å®„Éá„Éº„ÇøÊï¥ÂêàÊÄß„ÇíÂº∑Âåñ„Åó„Åæ„Åó„Åü„ÄÇ

- **„É™„Éï„Ç°„ÇØ„Çø**
  - „ÉÅ„É£„ÉÉ„ÉàÂá¶ÁêÜ„ÅÆ„ÉÜ„Ç≠„Çπ„ÉàËß£Êûê„ÇÑ„É°„ÉÉ„Çª„Éº„Ç∏ÁÆ°ÁêÜ„Çí„É¢„Ç∏„É•„Éº„É´Âåñ„Åó„ÄÅ‰øùÂÆàÊÄß„ÇíÂêë‰∏ä„Åó„Åæ„Åó„Åü„ÄÇ
  - ‰∏çË¶Å„Å™Â§âÊï∞„ÇÑÈñ¢Êï∞„ÇíÂâäÈô§„Åó„Ç≥„Éº„Éâ„ÇíÊï¥ÁêÜ„Åó„Åæ„Åó„Åü„ÄÇ
<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,"analysisWe need to classify whether the primary purpose is performance. The PR description: ""Feature/fix handlers"" includes new features, improvements, bug fixes, refactoring. Improvements mention ""„É°„ÉÉ„Çª„Éº„Ç∏‰øùÂ≠ò„ÅåË§áÊï∞‰ª∂„Åæ„Å®„ÇÅ„Å¶ÂäπÁéáÁöÑ„Å´Ë°å„Çè„Çå„Çã„Çà„ÅÜ„Å´ÊîπÂñÑ„Åï„Çå„Åæ„Åó„Åü"" (message saving improved to be efficient), ""„ÉÅ„É£„ÉÉ„Éà„É≠„Ç∞„ÅÆ‰øùÂ≠òÂá¶ÁêÜ„Åå„Éê„ÉÉ„ÉÅÂåñ„Åï„Çå„ÄÅÁÑ°ÈßÑ„Å™‰øùÂ≠ò„ÇíÂâäÊ∏õ„Åó„Åæ„Åó„Åü"" (batching to reduce waste), ""„Çπ„Éà„É™„Éº„Éü„É≥„Ç∞Âá¶ÁêÜ„ÅÆÈü≥Â£∞ÂêàÊàê„ÇÑÁä∂ÊÖãÁÆ°ÁêÜ„ÇíÊîπÂñÑ„ÅóÂÆâÂÆöÊÄß„ÇíÂêë‰∏ä„Åó„Åæ„Åó„Åü"" (improve streaming processing). These are performance improvements. Also ""AIÂøúÁ≠î„ÇÑ„Ç≥„Éº„Éâ„Éñ„É≠„ÉÉ„ÇØ„ÅÆÂá¶ÁêÜ„ÅåÊñáÁ´†Âçò‰Ωç„ÅßÁ¥∞„Åã„Åè„É™„Ç¢„É´„Çø„Ç§„É†„Å´ÂèçÊò†„Åï„Çå„Çã„Çà„ÅÜ„Å´„Å™„Çä„Åæ„Åó„Åü"" maybe performance? It's about real-time reflection. But the primary purpose seems to be performance improvements. The title ""Feature/fix handlers"" but body includes performance improvements. So classify as 'performance'.assistantfinalperformance"
2464579281,355,feat(messages): ÈùûÂêåÊúü„Åß„ÅÆËã±Ë™û„Åã„ÇâÊó•Êú¨Ë™û„Å∏„ÅÆÂ§âÊèõÂá¶ÁêÜ„ÇíÊîπÂñÑ„Åó„ÄÅÈü≥Â£∞ÂêàÊàê„ÅÆ„Éê„ÉÉ„Éï„Ç°Âá¶ÁêÜ„ÇíÊúÄÈÅ©Âåñ,tegnike,35606144,closed,2025-04-16T21:29:26Z,2025-04-16T22:20:15Z,2025-04-16T22:20:15Z,https://api.github.com/repos/tegnike/aituber-kit,https://github.com/tegnike/aituber-kit/pull/355,"

<!-- This is an auto-generated comment: release notes by coderabbit.ai -->
## Summary by CodeRabbit

- **„Éê„Ç∞‰øÆÊ≠£**
  - Èü≥Â£∞ÂêàÊàêÂá¶ÁêÜ„ÅÆÈ†ÜÂ∫è„Å®„Ç®„É©„Éº„Éè„É≥„Éâ„É™„É≥„Ç∞„ÅåÊîπÂñÑ„Åï„Çå„ÄÅÈùûÂêåÊúüÂá¶ÁêÜ„ÅÆÂÆâÂÆöÊÄß„ÅåÂêë‰∏ä„Åó„Åæ„Åó„Åü„ÄÇ
  - „É°„ÉÉ„Çª„Éº„Ç∏„ÇÑÈü≥Â£∞„Éá„Éº„Çø„ÅåÂ≠òÂú®„Åó„Å™„ÅÑÂ†¥Âêà„Åß„ÇÇ„ÄÅÂÆå‰∫Ü„Ç≥„Éº„É´„Éê„ÉÉ„ÇØ„ÅåÁ¢∫ÂÆü„Å´Âëº„Å∞„Çå„Çã„Çà„ÅÜ„Å´„Å™„Çä„Åæ„Åó„Åü„ÄÇ
<!-- end of auto-generated comment: release notes by coderabbit.ai -->",Human,"analysisWe need to classify as performance or non-performance. The PR title: ""feat(messages): ÈùûÂêåÊúü„Åß„ÅÆËã±Ë™û„Åã„ÇâÊó•Êú¨Ë™û„Å∏„ÅÆÂ§âÊèõÂá¶ÁêÜ„ÇíÊîπÂñÑ„Åó„ÄÅÈü≥Â£∞ÂêàÊàê„ÅÆ„Éê„ÉÉ„Éï„Ç°Âá¶ÁêÜ„ÇíÊúÄÈÅ©Âåñ"". That says ""improve async English to Japanese conversion process, optimize voice synthesis buffer processing"". The body mentions bug fixes, improved stability, ensuring callbacks. The keywords: ""ÊúÄÈÅ©Âåñ"" (optimization), ""„Éê„ÉÉ„Éï„Ç°Âá¶ÁêÜ„ÇíÊúÄÈÅ©Âåñ"" (buffer processing optimization). That is performance optimization. So classify as 'performance'.assistantfinalperformance"
2358222784,1366,Disable cache on CI on Windows because downloading the cache takes a super long time,whitphx,3135397,closed,2025-02-26T03:41:45Z,2025-02-27T17:52:52Z,,https://api.github.com/repos/whitphx/stlite,https://github.com/whitphx/stlite/pull/1366,,Human,"analysisWe need to classify whether the primary purpose of the PR is to improve performance. The PR title: ""Disable cache on CI on Windows because downloading the cache takes a super long time"". The body likely explains disabling cache on CI on Windows because downloading the cache takes a super long time. This is about performance: the cache download is slow, so disabling it improves performance. So it's a performance PR. The classification: 'performance'.assistantfinalperformance"
2404972295,4994,granian all the way,adhami3310,27952765,closed,2025-03-19T20:38:48Z,2025-03-20T02:52:41Z,2025-03-20T02:52:41Z,https://api.github.com/repos/reflex-dev/reflex,https://github.com/reflex-dev/reflex/pull/4994,,Human,"analysisWe need to classify the pull request. The input is ""granian all the way"". That is the title and body combined. It's a short phrase. Does it indicate performance? ""granian all the way"" maybe referencing ""Granian"" which is a Python ASGI server, known for performance. But the phrase alone is ambiguous. The guidelines: performance-related PRs may contain keywords like performance, optimization, etc. This phrase doesn't contain those. It might be a PR that is about using Granian for performance? But no explicit mention. The phrase ""granian all the way"" could be a commit message or PR title indicating a shift to Granian. But we don't have enough context. The instruction: determine whether the primary purpose is to improve performance. The phrase ""granian all the way"" suggests a shift to Granian, which is a performance improvement. But the guidelines: look for keywords. There's no explicit keyword. But ""Granian"" is a known performance server. But the guidelines say performance-related PRs may contain keywords such as... or any other terms referring to how faster and efficiently software runs. ""Granian"" is not in that list. But it's a known performance server. But the guidelines might allow other terms. But the phrase is ambiguous. I'd lean to ""non-performance"" because no explicit performance keyword. But maybe it's a PR to switch to Granian for performance. But the phrase ""all the way"" indicates a shift. But the guidelines: ""or any other terms referring to how faster and efficiently software runs."" ""Granian"" is a server known for performance, but not explicitly a performance term. But it's a known performance improvement. But the guidelines: ""Performance-related pull requests may contain keywords such as... or any other terms referring to how faster and efficiently software runs."" So ""Granian"" might be considered a term referring to performance. But it's not obviously a performance term. But it's a known performance server. I'd guess it's a performance PR. But the phrase is too short. I'd lean to ""performance"" because it's about using Granian, which is a performance improvement. But the guidelines: ""or any other terms referring to how faster and efficiently software runs."" ""Granian"" is a server that improves performance. So I'd classify as performance. But I'm not fully sure. The phrase ""granian all the way"" could be a PR to adopt Granian for performance. So I'd classify as performance.assistantfinalperformance"
2376869573,400,Bugfixes and query performance improvements,karthikscale3,105607645,closed,2025-03-06T17:31:51Z,2025-03-06T17:32:19Z,2025-03-06T17:32:19Z,https://api.github.com/repos/Scale3-Labs/langtrace,https://github.com/Scale3-Labs/langtrace/pull/400,,Human,"analysisWe need to classify as performance or non-performance. The title: ""Bugfixes and query performance improvements"". That clearly indicates performance improvements. So classification: performance.assistantfinalperformance"
2358007940,1967,Check git out - Checkpoints 2.0,canvrno,46584286,closed,2025-02-26T00:39:54Z,2025-02-26T00:51:39Z,2025-02-26T00:51:38Z,https://api.github.com/repos/cline/cline,https://github.com/cline/cline/pull/1967,"### Description

Implementing a branch-per-task architecture to improve checkpoint behavior and optimization. This change consolidates the checkpoint system by moving from per-task repositories to a workspace-level repository with task-specific branches. This enables more efficient storage, better isolation, and improved performance.

Key architectural improvements:
- Consolidated shadow repositories: One repository per workspace instead of per task
- Branch management: Each task gets a dedicated task-{id} branch
- Branch cleanup: Automatic branch deletion when tasks are removed
- Legacy support: Maintains backward compatibility with existing checkpoints
- Optimized commit creation and branch switching

Enhanced file filtering:
- Expanded exclusion patterns for build artifacts, media, cache files, etc.
- Integration with workspace's .gitattributes for LFS patterns
- #1837 - integrates for user configurability

Safeguards:
- Safe branch deletion with fallback mechanisms
- Disables checkpoints in workspaces with 50,000+ files 

Fixes #1267 
Fixes #1311 
Fixes #1313 
Fixes #1263 
Fixes #1873 

### Test Procedure

<Test documentation in Discord >

### Type of Change

<!-- Put an 'x' in all boxes that apply -->

-   [ ] üêõ Bug fix (non-breaking change which fixes an issue)
-   [X] ‚ú® New feature (non-breaking change which adds functionality)
-   [ ] üí• Breaking change (fix or feature that would cause existing functionality to not work as expected)
-   [ ] üìö Documentation update

### Pre-flight Checklist

<!-- Put an 'x' in all boxes that apply -->

-   [X] Changes are limited to a single feature, bugfix or chore (split larger changes into separate PRs)
-   [x] Tests are passing (`npm test`) and code is formatted and linted (`npm run format && npm run lint`)
-   [X] I have created a changeset using `npm run changeset` (required for user-facing changes)
-   [X] I have reviewed [contributor guidelines](https://github.com/cline/cline/blob/main/CONTRIBUTING.md)

### Screenshots


üßú‚Äç‚ôÄÔ∏èüßú‚Äç‚ôÄÔ∏èüßú‚Äç‚ôÄÔ∏èüßú‚Äç‚ôÄÔ∏èüßú‚Äç‚ôÄÔ∏èüßú‚Äç‚ôÄÔ∏è
![image](https://github.com/user-attachments/assets/306fad32-ea10-405e-b21a-3d9e67312ab4)

### Additional Notes

Tests have been renamed to exclude them, as they are numerous and slow to execute. Test parallelization to be explored later.

<!-- ELLIPSIS_HIDDEN -->


----

> [!IMPORTANT]
> Introduces a branch-per-task architecture for checkpoints, enhancing storage efficiency and performance while maintaining backward compatibility and adding extensive test coverage.
> 
>   - **Architecture**:
>     - Introduces branch-per-task architecture in `CheckpointTracker.ts`, consolidating to one repository per workspace with task-specific branches.
>     - Maintains backward compatibility with legacy checkpoints.
>     - Automatic branch deletion when tasks are removed.
>   - **File Filtering**:
>     - Enhanced exclusion patterns in `CheckpointExclusions.ts` for build artifacts, media, cache files, etc.
>     - Integrates with workspace's `.gitattributes` for LFS patterns.
>   - **Safeguards**:
>     - Safe branch deletion with fallback mechanisms in `CheckpointGitOperations.ts`.
>     - Disables checkpoints in workspaces with 50,000+ files.
>   - **Testing**:
>     - Adds extensive tests in `CheckpointCommit.test.ts.bak`, `CheckpointCreation.test.ts.bak`, `CheckpointDiff.test.ts.bak`, and others to cover new functionality.
>   - **Misc**:
>     - Updates `Cline.ts` to use new checkpoint architecture.
>     - Minor logging changes in `ClineProvider.ts`.
> 
> <sup>This description was created by </sup>[<img alt=""Ellipsis"" src=""https://img.shields.io/badge/Ellipsis-blue?color=175173"">](https://www.ellipsis.dev?ref=cline%2Fcline&utm_source=github&utm_medium=referral)<sup> for 3ff2786fb743cf41317d2ae7ab0dd0495fa9b3b8. It will automatically update as commits are pushed.</sup>


<!-- ELLIPSIS_HIDDEN -->",Human,"analysisWe need to classify as performance or non-performance. The PR description says ""improve checkpoint behavior and optimization"" and ""improved performance"". Keywords: performance, optimization, efficient storage, improved performance. So it's performance. So label 'performance'.assistantfinalperformance"
2503287360,89,perf(copool): remove lock in queue,noneback,46670806,closed,2025-05-07T05:26:42Z,2025-05-07T05:31:00Z,2025-05-07T05:31:00Z,https://api.github.com/repos/noneback/go-taskflow,https://github.com/noneback/go-taskflow/pull/89,,Human,"analysisWe need to classify as performance or non-performance. The title: ""perf(copool): remove lock in queue"". The body is not provided. The title indicates ""perf"" prefix, and ""remove lock in queue"" likely improves performance by removing a lock, reducing contention. So it's performance. So output ""performance"".assistantfinalperformance"
